---
title: "Interval Estimation"
format:
  html:
    toc: true
    toc-depth: 3
    toc-location: right
    self-contained: false     # must be false when using webr
urlcolor: blue
filters:
  - webr
execute:
  webR: true
editor: 
  markdown: 
    wrap: 72
---

```{r setup, echo = FALSE, message = FALSE}

library(dplyr, quietly = TRUE)
library(ggplot2, quietly = TRUE)
library(kableExtra, quietly = TRUE)
library(tidyquant, quietly = TRUE)

```

## Standard Error

The standard error (SE) of a statistic is the standard deviation of its
sampling distribution.

Assume that $x_1, x_2, \dots , x_n$ are i.i.d with mean $\mu_x$ and
variance $\sigma_x^2$.

We have determined that

$$var(\bar{x}) = \frac{\sigma_x^2}{n}$$

For an infinite population, the standard error of the mean $\bar{x}$ is:

$$se(\bar{x}) = \sigma_{\bar{x}} = \frac{\sigma_x}{\sqrt{n}}$$

Since the standard deviation of the population $\sigma_x$ is typically
unknown, we use the sample standard deviation $s$ or

$$\hat{\sigma_x} = s(\bar{x}) = \frac{s_x}{\sqrt{n}}$$

## Finite Standard Error

What if the population from which the samples are drawn is not infinite?

In this case, we must use a **finite population correction factor** for
the standard error of the mean.

The finite population correct factor is used when the fraction of the
population sampled is greater than 5%.

In finite populations, the standard error of the mean is:

$$\sigma(\bar{x}) = \sqrt{\frac{(N-n)}{(N-1)}} \bigg(\frac{\sigma}{\sqrt{n}} \bigg)$$

$$s(\bar{x}) = \sqrt{\frac{(N-n)}{(N-1)}} \bigg(\frac{s}{\sqrt{n}} \bigg)$$

## Z-Score Example

Assume that individuals are distributed in a population so that the ages
of the individuals are normally distributed with a mean age of 27.0 and
a standard deviation of 12.0. If we define $X$ as a random variable
representing age, then

$$
X \sim N(\mu, \sigma^2) \rightarrow X \sim N(27, 144)
$$

What is the probability that a single randomly selected individual is
less than 30 years old?

$$P(X \le 30) = P(Z \le \frac{30 - 27}{12}) = P(Z \le 0.25) = 0.5987$$

We can easily demonstrate this problem using the code chunk below.

```{webr-r}

rm(list = ls())

p_x <- tibble(x <- 30, 
              p_x <- pnorm(30, 27, 12))

kable(p_x,
      align = 'c',
      digits = 4,
      col.names = c('X', 'P(X <= 30)'))

```

## Standard Error Example

Assume that we now draw multiple samples from the population.

The sample size of each trial is 36 and we repeatedly sample the
population.

Given the population mean is 27, the population variance is 144, then:

$$
E[\bar{x}] = \mu = 27
$$

$$
var(\bar{x}) = \frac{\sigma^2}{n} = \frac{144}{36} = 4
$$

$$
sd(\bar{x}) = \frac{\sigma}{\sqrt{n}} = \frac{12}{6} = 2
$$

From the Central Limit Theorem, the sampling distribution of the sample
averages is:

$$\bar{x} \sim N(\mu, \frac{\sigma^2}{n})$$ $$\bar{x} \sim N(27, 4)$$

What is the probability that the average age of a single sample of
randomly selected individuals is less than 30 years given that the
sample size is 36 observations?

$$P(\bar{x} \le 30)  = P(Z \le \frac{30-27}{2}) = P(Z \le 1.5) = 0.9332$$

The code chunk below does the following:

-   Declares the variables of interest

-   Creates a tibble with the sample average equal to 30 and estimates
    the probability that the one observes a sample average of less than
    or equal to 30 with a mean of 27 and a standard error of 2

-   Produces a table of the tibble

-   Creates a tibble where the variable $x$ is defined on the interval
    $[20, 34]$ and creates 200 observations for $x$ along this interview
    and then estimates the associated probabilities for $x$ given a mean
    of 27 and a standard error of 2

-   Creates a tibble that only contains observations where $x$ is less
    than or equal to 30

-   Creates a density plot of the tibble to represent the distribution
    of sample averages

-   Shades the distribution to represent $P(\bar{x} \le 30)$

```{r, message = FALSE, warning = FALSE, results = 'hold'}

rm(list = ls())

mu <- 27         
sigma <- 12       
n <- 36          

mean_xbar <- mu
sd_xbar   <- sigma / sqrt(n)

p_xbar <- tibble(x_bar = 30,
                 p_x_bar = pnorm(30, mean_xbar, sd_xbar))

kable(p_xbar,
      align = 'c',
      digits = 4,
      col.names = c('X-Bar','P(X-Bar <= 30)'))

df_xbar <- tibble(x = seq(20, 34, length = 200),
                  y = dnorm(x, mean = mean_xbar, sd = sd_xbar))

shade_df <- df_xbar %>% filter(x <= 30)

ggplot(df_xbar, 
       aes(x = x, 
           y = y)) +
geom_line(color = "blue", linewidth = 1.2) +
geom_area(data = shade_df, 
          aes(x = x, 
              y = y), 
          fill = "red", 
          alpha = 0.5) +
geom_vline(xintercept = 30, 
           linetype = "dashed", 
           color = "darkblue") +
theme_minimal(base_size = 12) +
labs(title    = "Sampling Distribution of X-Bar",
     subtitle = "Mean = 27, Sigma = 12, N= 36",
     x        = "X-Bar",
     y        = "Density") 

```

## Standard Error Example

As part of your research, you have obtained a sample of 3,600
individuals. Using this data, you have estimated the average income of
the sample and the sample standard deviation of income.

$$
\bar{x} = 52,500
$$

$$
s = 2,750
$$

What is the standard error of the sample estimator in this example?

$$
se(\bar{x}) = \frac{s}{\sqrt{n}} = \frac{2,750}{\sqrt{3,600}} = 45.83
$$

What is the probability, in repeated samples of 3,600, of drawing an
individual with an income of 47,500 or less?

$$
P(X \le 47,500) = P(Z \le \frac{x - \mu}{\sigma}) = P(Z \le \frac{47500-52500}{2750}) = P(Z \le -1.818)
$$

$$
P(Z \le -1.818) = 0.0345
$$

Remember, the distribution of sample averages is more "tightly
clustered" around the population mean. We can observe this by noting the
sample standard deviation is 2,750 but the standard error of the sample
averages is 45.83. Our samples are relatively 'large' so the variance of
the sample averages is much lower than the variance of the random
variable.

For example, what is the probability, with a sample size of 3,600,
sample average of 52,500, and a sample standard error of 45.83 of
drawing a sample with a sample average less than or equal to 52,400?

$$
P(\bar{x} \le 52,400) = P(Z \le \frac{52400-52500}{45.83}) = 0.0145
$$

Let's say that we change the sample size from 3,600 to 600. What impact
does this have on the probability of observing a sample average of
52,400 or less?

Given the change in the sample size, we re-estimate the standard error
and obtain a new probability of observing a sample with a sample average
of 52,400.

$$
se(\bar{x}) = \frac{s}{\sqrt{n}} = \frac{2,750}{\sqrt{600}} = 112.27
$$

$$
P(\bar{x} \le 52,400) = P(Z \le \frac{52400-52500}{112.27}) = 0.1865
$$

In the code chunk below, we create simulated population of 500,000
observations and then take samples of 60, 600, and 6000 from the
population. We then estimate the probability of observing a sample
average of 52,400 for each of the sample sizes.

As one would expect, given the Central Limit Theorem, as you increase
the sample size, the standard error of the sample average estimator
declines, and the probability of observing a sample with an average
income of 52,400 declines.

```{r, warnings = FALSE, message = FALSE}

rm(list = ls())

set.seed(1234)

library(dplyr)
library(kableExtra)

population <- tibble(income = rnorm(500000, 52500, 2750))

sample_1 <- population %>% 
            slice_sample(n = 3600, replace = FALSE) %>%
            summarize(average = mean(income),
                      sd      = sd(income),
                      se      = sd(income)/sqrt(6000)) %>%
            mutate(p_x_bar = pnorm(52400, 52500, se),
                   n       = 6000) %>%
            select(n, everything())

sample_2 <- population %>% 
            slice_sample(n = 600, replace = FALSE) %>%
            summarize(average = mean(income),
                      sd      = sd(income),
                      se      = sd(income)/sqrt(600)) %>%
            mutate(p_x_bar = pnorm(52400, 52500, se),
                   n       = 600) %>%
            select(n, everything())

sample_3 <- population %>% 
            slice_sample(n = 60, replace = FALSE) %>%
            summarize(average = mean(income),
                      sd      = sd(income),
                      se      = sd(income)/sqrt(60)) %>%
            mutate(p_x_bar = pnorm(52400, 52500, se),
                   n       = 60) %>%
            select(n, everything())

output <- sample_1 %>%
          rbind(sample_2) %>%
          rbind(sample_3) %>%
          arrange(n)

kable(output,
      align = 'c',
      digits = 3,
      col.names = c('n', 'X-bar', 'SD(X)', 'SE(X-bar)','P(X-bar <= 52400)')) %>%
kable_classic()



```

## Interval Estimation

An estimator generates a point estimate of a population parameter of
interest.

However, the point estimate is unlikely to be exactly equal to the
population parameter of interest.

An **interval estimate** provides information on how "close" the point
estimate is to the population parameter.

The **margin of error** is often added and subtracted from the point
estimate to generate the interval estimate or:

$$\bar{x} \pm \, \text{m.o.e}$$

The sampling distribution of $\bar{x}$ is the basis for the interval
estimates.

## Statistical Significance

First, we define the level of statistical significance as equal to the
probability of incorrectly rejecting the null hypothesis of interest. In
other words, we define the level of statistical significance as the
likelihood of a **false positive**.

For example, assume that we are investigating whether a drug
statistically reduces the likelihood of cancer. The null hypothesis is
that the drug has not statistically discernible influence on the
likelihood of cancer (or increases the likelihood of having cancer).

The alternative hypothesis is that the drug reduces the likelihood of
having cancer.

A **false positive** would be determining the drug reduces the
likelihood of having cancer when it has no impact (or, even worse,
increases the likelihood of having cancer).

So, if we set the level statistical significance to 0.05, we are saying
there is a 5% change of incorrectly rejecting the null hypothesis.

***An increase in the level of statistical significance lowers the
probability of a Type 1 error.***

Let $\alpha$ be equal to the probability of making a Type 1 error. As we
lower $\alpha$ from 0.10 to 0.05 to 0.01, we are increasing statistical
significance from 10% to 5% to 1%.

## Interval Estimation

Assume that the population variance is known.

Let $\alpha$ be the level of statistical significance or the probability
of making a Type 1 error.

The confidence interval of the interval estimate is:

$$
(1 - \alpha)
$$

The interval estimate for the sample average when $\sigma^2$ is known
is:

$$\bar{x} \pm \, z_{\alpha/2} \frac{\sigma}{\sqrt{n}}$$

Let $\sigma = 20$ and $n = 100$ and $\alpha = 0.05$ so
$\alpha/2 = 0.025$

$$\sigma(\bar{x}) = \frac{\sigma_x}{\sqrt{n}} = \frac{20}{\sqrt{100}} = 2$$

Given that $1 - \alpha/2 = 0.975$ then $Z(0.975)=$

$$\bar{x} \pm \, z_{\alpha/2} \frac{\sigma}{\sqrt{n}} = \bar{x} \pm (1.96 \times 2)  = \bar{x} \pm 3.92$$

## Interval Estimation in R

We demonstrate how to estimate confidence intervals in R in the code
chunk below.

-   The sample size is set to 500

-   Three significance levels are used, 10%, 5%, and 1%

-   We use $\alpha/2$ and **qnorm** to find the appropriate **Z** value
    for each interval

-   We create a population of 100,000 observations with a known mean and
    standard deviation

-   We randomly sample the population and estimate descriptive
    statistics

-   We estimate the lower and upper bounds of each confidence internal

-   We produce a table of results

```{r, warnings = FALSE, message = FALSE}

rm(list = ls())

library(dplyr)
library(ggplot2)
library(kableExtra)

sample_size = 500

alpha_1 = 0.10
alpha_2 = 0.05
alpha_3 = 0.01

ci_1 <- 1 - alpha_1/2
ci_2 <- 1 - alpha_2/2
ci_3 <- 1 - alpha_3/3

z_ci_1 <- qnorm(ci_1, 0, 1)
z_ci_2 <- qnorm(ci_2, 0, 1)
z_ci_3 <- qnorm(ci_3, 0, 1)

population <- tibble(income = rnorm(100000, mean = 52500, sd = 2750))

sample <- population %>%
          slice_sample(n = sample_size) %>%
          summarize(samp_avg = mean(income),
                    samp_sd  = sd(income),
                    samp_se  = sd(income)/sqrt(sample_size))

moe <- sample %>%
       mutate(lower_1 = samp_avg - z_ci_1 * samp_se,
              upper_1 = samp_avg + z_ci_1 * samp_se,
              lower_2 = samp_avg - z_ci_2 * samp_se,
              upper_2 = samp_avg + z_ci_2 * samp_se,
              lower_3 = samp_avg - z_ci_3 * samp_se,
              upper_3 = samp_avg + z_ci_3 * samp_se)


kable(moe,
      align       = "c",
      digits      = 2,
      caption     = "Confidence Intervals",
      col.names   = c("Sample Average", 
                      "Sample SD",
                      "Sample Avg SE",
                      "Lower Bound (10%)",
                      "Upper Bound (10%)",
                      "Lower Bound (5%)",
                      "Upper Bound (5%)",
                      "Lower Bound (1%)",
                      "Upper Bound (1%)")) %>%
kable_styling(font_size = 12)


```

## Interval Estimation - Mean

While we would prefer to know the population variance, in all
likelihood, the true population variance is not know.

We, however, do have unbiased estimators of the population mean and
variance.

$$
E[\bar{x}] = \mu
$$

$$
E[var(\bar{x})] = E[\frac{s}{\sqrt{n}}] = \frac{\sigma}{\sqrt{n}} 
$$

Let $s^2$ be the unbiased sample variance.

Since $\sigma^2$ is unknown, then replacing $\sigma^2$ with its unbiased
estimate $s^2$ yields the t-statistic or:

$$t = \frac{\bar{x} - \mu}{s/\sqrt{n}}$$

## T-Distribution

The t-statistic is:

$$t = \frac{\bar{x} - \mu}{s/\sqrt{n}}$$

Rearrange the t-statistic to obtain:

$$T_{n-1} = \frac{\sqrt{n} (\bar{x} - \mu)}{s}$$ The numerator is a
standard normal random variable.

The denominator follows a chi-squared distribution with $(n - 1)$
degrees of freedom.

$$s^2 = \frac{1}{n-1} \, \sum_{i = 1}^{n} (x_i - \bar{x})^2$$

The numerator and denominator are independent.

## T Distribution Plot

```{r, warnings = FALSE, message = FALSE}

rm(list = ls())

library(dplyr)
library(ggplot2)
library(kableExtra)

ggplot(data.frame(x = c(-5, 5)), 
       aes(x = x)) +
stat_function(fun = dt, 
              args = list(df = 1),
              linewidth = 1.2,
              color = "dark blue") +
stat_function(fun = dnorm, 
              args = list(mean = 0, sd = 1),
              linewidth = 1.2,
              color = "red") +
theme_minimal() +
labs(x = "x",
     y = "P(x)",
     title = "Standard Normal and T Distribution",
     subtitle = "1 degree of freedom")
```

```{r, warnings = FALSE, message = FALSE}

rm(list = ls())

library(dplyr)
library(ggplot2)
library(kableExtra)

library(ggplot2)

ggplot(data.frame(x = c(-5, 5)), 
       aes(x = x)) +
stat_function(fun = dt, 
              args = list(df = 10),
              linewidth = 1.2,
              color = "dark blue") +
stat_function(fun = dnorm, 
              args = list(mean = 0, sd = 1),
              linewidth = 1.2,
              color = "red") +
theme_minimal() +
labs(x = "x",
     y = "P(x)",
     title = "Standard Normal and T-Distribution",
     subtitle = "10 degrees of freedom")

```

## Interval Estimation - Mean

Assume that $\sigma$ is unknown and $s$ is an unbiased estimator.

Let $(1-\alpha)$ be the confidence coefficient.

The interval estimate of the population mean is:

$$\bar{x} \pm \, t_{\alpha/2} \, \frac{s}{\sqrt{n}}$$

Let n = 70, the sample average is 9,312, sample standard deviation is
4,007, and $\alpha = 0.05$.

We can use the **qt** function or a **t-table** to find the appropriate
t-statistic.

The interval estimate is thus:

$$\bar{x} \pm \, t_{\alpha/2} \, \frac{s}{\sqrt{n}} = 9312 \pm 1.994945 \times \frac{4007}{\sqrt{70}} = 9312 \pm 955.4352$$

```{r}

rm(list = ls())

alpha = 0.05
n = 70
xbar = 9312
s = 4007

t_stat <- qt(alpha/2,n-1, lower.tail = FALSE)

moe <- tibble(xbar = xbar,
              se = s/sqrt(n),
              t_stat = t_stat,
              lower = xbar - t_stat*se,
              upper = xbar + t_stat*se)

kable(moe,
      align = 'c',
      digits = 3,
      col.names = c('X-Bar','SE(X-Bar)','T-Statistic',
                    'Lower Bound', 'Upper Bound')) %>%
kable_classic()


```

## R Example

Here we have a randomly generated population with $\mu = 50,000$ and
$\sigma = 9700$.

Assume we don't know $\mu$ or $\sigma$ but we can sample the population.

We can construct a 90% confidence interval using the sample average and
the t-statistic.

We can vary $\alpha$ and $n$ to observe how changing the confidence
interval and sample size affects the confidence interval.

```{r, warnings = FALSE, message = FALSE}

rm(list = ls())

library(dplyr)
library(ggplot2)
library(kableExtra)

pop = 100000
mu = 50000
sigma = 9700
nsample <- 500
alpha <- 0.10

ci <- 1 - (alpha/2)
t_ci <- qt(ci, nsample - 1)

population <- data.frame(x = rnorm(pop, mu, sigma))

samples <- population %>%
  slice_sample(n = nsample) %>%
  summarize(samp_avg = mean(x),
            samp_sd  = sd(x)) %>%
  mutate(std_err = samp_sd/sqrt(nsample),
         moe     = t_ci*std_err) %>%
  mutate(lower_ci = samp_avg - moe,
         upper_ci = samp_avg + moe)

kable(samples,
      align       = "cccccc", 
      caption     = "Confidence Interval",
      col.names   = c("Average", "Std Dev", 
                      "Std Error", "MOE",
                      "Lower Bound", "Upper Bound")) %>%
  kable_styling(font_size = 12)


```

## Wrapping Up

In this module, we have explored how samples are drawn from a population
and investigated how sampling variability occurs when estimating
population parameters. We have used data to explore the shape and
dispersion of sampling distributions, illustrating the relationship
between estimators and the population characteristics of interest.

We explored the **Law of Large Numbers** and **Central Limit Theorem**.
These play a central role in inferential statistics. We noted that,
regardless of the underlying population distribution, the sample
distribution of the sample averages approaches normality as sample size
increases. We estimated the standard error and applied these concepts in
a variety of examples.
