---
title: "Z-Scores"
format:
  html:
    toc: true
    toc-depth: 3
    toc-location: right
    self-contained: false     # must be false when using webr
urlcolor: blue
filters:
  - webr
execute:
  webR: true
---

```{r setup, echo = FALSE, message = FALSE}

library(dplyr, quietly = TRUE)
library(ggplot2, quietly = TRUE)
library(kableExtra, quietly = TRUE)
library(tidyquant, quietly = TRUE)

```

## Z-Scores

For $i$ observations in a population, the population mean deviations are equal to:

$$d_i = x_i - \mu$$ 

For $i$ observations in a sample, the sample mean deviations are equal to:

$$d_i = x_i - \bar{x}$$
The mean deviations frame our discussion of the data in terms of the **center** of the data.

We lack information, however, on whether the mean deviation is "large" or "small" relative to the dispersion of the data.

The **z-score** or **standard score** is a measure of the mean deviation of an observation relative to the standard deviation of the variable.

In the population, the z-score for an observation is:

$$Z = \frac{x_i - \mu}{\sigma}$$

In the sample, the Z-score for an observation is:

$$Z = \frac{x_i - \bar{x}}{s}$$

The Z-score can be placed on a **normal distribution** curve and provides a measure in **standard deviation units**.

If $Z_{x1} = 2$ then the observation $x_1$ is two standard deviations to the right of $\mu$.

If $Z_{x2} = -0.75$, then the observation $x_2$ is 0.75 observations to the left of $\mu$.

Z-scores allow you to compare results to a normally distributed population. If you know, for example, that someone's weight is 175 pounds, you might want to compare it to the *average* persons weight. The z-score compares the difference of the individual's weight with the average person's weight and adjusts it for the dispersion of the data.

## Calculating Z-scores

Assume that you have a standardized test where the mean is $\mu = 150$ and the standard deviation is $\sigma = 25$.

If we assume a normal distribution and an individual scores 190. 

$$Z_{190} = \frac{x_i - \mu}{\sigma} = \frac{190 - 150}{25} = 1.6$$
A Z-score of 1.6 means that the individual's score of 190 is 1.6 standard deviations from the population mean. Given the value of the Z-score is positive, this means the score of 190 lies 1.6 standard deviations to the right of the population mean.

What is an individual scored 100?

$$Z_{100} = \frac{x_i - \mu}{\sigma} = \frac{100 - 150}{25} = -2$$
A Z-score of -2 means that the individual's score is 2 standard deviations to the **left** of the population mean. The negative value of the Z-score means that the individual scored less than the population mean.

Later in the class we will use a Z-table and then R to determine the probabilities associated with Z-scores.

## Z-Scores in R

In the following example, we generate a simulated population of 10,000 observations with $\mu = -25$ and $\sigma = 2.9$.

We then generate the population mean and population standard deviation for the simulated population.

The next step is estimating the z-score of each observation. 

We then output the first five observations to the table. Again, you may find it useful to note how the data in the table changes each time your run the code.


```{webr-r}

rm(list = ls())

df_1 <- data.frame(x = rnorm(10000, mean = -25, sd = 2.9)) 

z_norm <- df_1 %>%
    mutate(mean_x  = mean(x),
           sd_x    = sd(x),
           z_score = (x - mean(x))/sd(x))

kable(z_norm[1:5,],
        col.names = c('Observation',
                      "Mean X", "Standard Deviation X",
                      "Z-Score"),
        align     = 'c',
        caption   = 'Z-Scores') 

```


## Module Summary

We use descriptive statistics to develop an understanding of the properties of the data.

We have discussed two broad properties of the data:

-   The center of the data

-   The spread of the data

We will work on the shape of the data in coming modules.

Now, having worked with measures of the center and spread, we can turn our attention to the question of co-movement.

Do two variables "move" together in some systematic fashion?

What measures might suggest a potential relationship between two variables of interest?

In our next module, we will focus on **covariance** and **correlation**.
