[
  {
    "objectID": "mod-4-0-overview.html",
    "href": "mod-4-0-overview.html",
    "title": "19  Module 4 Overview",
    "section": "",
    "text": "19.1 Introduction\nWelcome to Module 4 of ECON 700.\nThis module introduces the foundational concepts of probability as a measure of uncertainty and likelihood. Students learn how random experiments generate outcomes that form a sample space, and how probability quantifies the chance that specific outcomes occur. The module progresses through the building blocks of probability theory, including combinations, permutations, complements, unions, and intersections of events. Through these principles, students develop an understanding of mutually exclusive and independent events and how they shape probability calculations.\nThe second half of the module applies probability rules to real-world decision-making and data analysis. Learners explore conditional probability, the multiplication and addition laws, and the distinction between joint and marginal probabilities. The module culminates in an introduction to Bayes’ Theorem, illustrating how prior beliefs are updated with new evidence—first conceptually, then through applications like medical testing and the Monty Hall problem. This approach emphasizes the interpretive and practical power of probability in economic reasoning, uncertainty, and risk assessment.",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Module 4 Overview</span>"
    ]
  },
  {
    "objectID": "mod-4-0-overview.html#learning-objectives",
    "href": "mod-4-0-overview.html#learning-objectives",
    "title": "19  Module 4 Overview",
    "section": "19.2 Learning Objectives",
    "text": "19.2 Learning Objectives\nBy the end of this module, you should be able to:\n\nDefine key probability concepts, including random experiments, sample spaces, and events.\nCompute probabilities using basic rules (addition, multiplication, complement) for mutually exclusive and independent events.\nDifferentiate between permutations and combinations in counting possible outcomes.\nInterpret conditional probabilities and construct joint and marginal probability tables.\nApply Bayes’ Theorem to update probabilities based on new information.\nAnalyze real-world scenarios, such as the Monty Hall problem or medical testing, to evaluate decision-making under uncertainty.",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Module 4 Overview</span>"
    ]
  },
  {
    "objectID": "mod-4-0-overview.html#readings-resources",
    "href": "mod-4-0-overview.html#readings-resources",
    "title": "19  Module 4 Overview",
    "section": "19.3 Readings & Resources",
    "text": "19.3 Readings & Resources\n\nBarbara Illowsky and Susan Dean. Introductory Statistics 2e. OpenStax. (Chapter 3). Available for free online at https://openstax.org/books/introductory-statistics-2e/pages/2-introduction\nIrizarry, R. (2025). Introduction to Data Science: Statistics and Prediction Algorithms Through Case Studies (Chapters 1-3). Available for free at: https://rafalab.dfci.harvard.edu/dsbook-part-2/\nWickham, H., & Grolemund, G. (2017). R for Data Science. (Chapters 1–3).\nOnline R documentation: https://cran.r-project.org/manuals.html\nRStudio Cheat Sheets",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Module 4 Overview</span>"
    ]
  },
  {
    "objectID": "mod-4-0-overview.html#activities",
    "href": "mod-4-0-overview.html#activities",
    "title": "19  Module 4 Overview",
    "section": "19.4 Activities",
    "text": "19.4 Activities\n\nComplete the hands-on coding exercises embedded in each lesson.\n\nComplete the weekly class assignment.\n\nParticipate in the discussion form.\n\nTake the weekly knowledge quiz.",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Module 4 Overview</span>"
    ]
  },
  {
    "objectID": "mod-4-0-overview.html#lessons-in-this-module",
    "href": "mod-4-0-overview.html#lessons-in-this-module",
    "title": "19  Module 4 Overview",
    "section": "19.5 Lessons in this Module",
    "text": "19.5 Lessons in this Module\n\n4.1 – Introduction to Probability\n4.2 – Probability Rules\n4.3 – Conditional Probability\n\n\nNext: Start with Introduction to Probability.",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Module 4 Overview</span>"
    ]
  },
  {
    "objectID": "mod-4-1-probability.html",
    "href": "mod-4-1-probability.html",
    "title": "20  Introduction to Probability",
    "section": "",
    "text": "20.1 Defining Probablity\nProbability is a measure of the likelihood that an outcome will occur.\nIf a fair coin has tails on one side and heads on the other side, we understand, over thousands of flips (trials), that we will observe heads 50% of the time and tails 50% of the time. Assuming that no flips end up on the edge, the likelihood of heads for a fair coin is 0.50 while the likelihood of tails is 0.50.\nProbability is a numerical measure of the likelihood of an outcome and is bounded between 0 and 1, including 0 and 1. If the probability of an outcome is ‘close’ to zero, then it is likely to occur than an outcome that is ‘closer’ to 1.\nIn other words, if the outcome of the outcome is beneficial (winning a prize), we would prefer that the probability of the outcome be higher (closer to 1) than lower. Likewise, if the outcome of the outcome if not beneficial (incurring a costly bill), we would prefer the probability of the outcome be lower (closer to 0) than higher.",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "mod-4-1-probability.html#experiments-and-randomness",
    "href": "mod-4-1-probability.html#experiments-and-randomness",
    "title": "20  Introduction to Probability",
    "section": "20.2 Experiments and Randomness",
    "text": "20.2 Experiments and Randomness\nA random experiment is a process that generates outcomes.\nIn our coin flipping example, the process of flipping the coin generates two possible outcomes, heads or tails. The outcomes are well defined prior to the start of the experiment and the outcomes are mutually exclusive. In other words, an outcome must be either heads or tails and there is not a possible outcome where heads and tails occur simultaneously.\nEach repetition of the experiment is a trial.\nAn experiment may consist of one trial, several trials, or many trials. However many trials there are in an experiment, the outcome of each trial is a random event, that is, the outcome is not pre-determined and occurs by chance.\nConsider an unfair six-sided die that is weighted so that it lands on six on every throw. Throwing the die is not a random experiment because the outcome of every trial is known prior to the experiment. Regardless of whether you throw the die one time or fifty times, the outcome is always six. Now compare this to a fair six sided die. On any given trial, the die could land on 1, 2, 3, 4, 5, or 6. The outcome of any specific trial is unknown prior to its occurrence but, over repeated trials, we know the probability of each outcome.\nWe have now discussed two random experiments, the flipping of a fair coin and the toss of a fair six sided tie. In the case of the fair coin, we have identified two possible outcomes: heads and tails. In the case of the fair six sided die, we have identified six possible outcomes: 1, 2, 3, 4, 5, and 6.\nThe set of possible outcomes for each random experiment is known as the sample space.\nThe sample space consists of the mutually exclusive and collectively exhaustive outcomes of a random experiment.\nMutually exclusive implies that a random experiment cannot have the same outcome simultaneously.\nCollectively exhaustive implies that all the outcomes of the random experiment are contained in the sample space.",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "mod-4-1-probability.html#multistep-counting-rules",
    "href": "mod-4-1-probability.html#multistep-counting-rules",
    "title": "20  Introduction to Probability",
    "section": "20.3 Multistep Counting Rules",
    "text": "20.3 Multistep Counting Rules\nConsider a fair 10-sided tie.\nThe potential outcomes of any trial are\n\\[s = \\{1, 2, 3, 4, 5, 6, 7, 8 , 9, 10\\}\\] What happens if we conduct two trails?\nThere are ten possible outcomes of the first trial and ten possible outcomes in the second trial. But what are number of outcomes for the multistep experiment?\nAssume that you roll a 1 on the first trial. The set of possible outcomes of the first trial \\(o_1\\) are:\n\\[o_1 = \\{(1,1), (1,2), (1,3), (1,4), (1,5), (1,6), (1,7), (1,8), (1,9), (1,10) \\}\\]\nAnd if you roll a 2 on the first trial, then the set of possible outcomes are:\n\\[o_2 = \\{(2,1), (2,2), (2,3), (2,4), (2,5), (2,6), (2,7), (2,8), (2,9), (2,10) \\}\\]\nIf the first roll produces a 1, there are 10 possible outcomes. If the first roll produces a 2, there are 10 different outcomes, and so on.\nExtending this logic yields 100 possible outcomes for the two trial experiment with a fair ten-sided tie.\nGiven a random experiment with a sequence of \\(k\\)-steps and \\(n_1, n_2, ..., n_k\\) outcomes for each step, then the number of outcomes in the sample space is equal to:\n\\[n_1 \\times n_2 \\times ... \\times n_k\\] The random experiment of rolling a 10-sided die two times has \\(k = 2\\) steps and the number of outcomes of each step is equal to \\(n_1 = 10, n_2 = 10\\). The number of outcomes is equal to\n\\[n_1 \\times n_2 = 10 \\times 10 = 100\\]",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "mod-4-1-probability.html#multistep-examples",
    "href": "mod-4-1-probability.html#multistep-examples",
    "title": "20  Introduction to Probability",
    "section": "20.4 Multistep Examples",
    "text": "20.4 Multistep Examples\nIn our first example, assume we are flipping a fair two-sided coin five times.\nGiven that \\(k = 5\\) and \\(n_1 = 5, n_2 = 5, n_3 = 5, n_4 = 5, n_5 = 5\\), then number outcomes in the sample space of the random experiment is equal to:\n\\[s = n_1 \\times n_2 \\times n_3 \\times n_4 \\times n_5 = 2 \\times 2 \\times 2 \\times 2 \\times 2 = 2^5 = 32\\]\nIn our second example, assume that we are throwing a fair six-sided die followed by a fair two-sided coin followed by a fair twenty-sided die.\nGiven that \\(k =3\\) and \\(n_1 = 6, n_2 = 2, n_3 = 20\\), then the number of outcomes in the sample space is equal to:\n\\[n_1 \\times n_2 \\times n_3 = 6 \\times 2 \\times20 = 240\\]",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "mod-4-1-probability.html#factorials",
    "href": "mod-4-1-probability.html#factorials",
    "title": "20  Introduction to Probability",
    "section": "20.5 Factorials",
    "text": "20.5 Factorials\nA factorial is merely a series of products and are indicated by an exclamation mark.\nThe factorial function of a positive integer \\(n\\) is defined as the product of all the positive integers not greater than \\(n\\).\n\\[\nn! = 1 \\times 2 \\times 3\\ \\times 4 \\dots (n-2) \\times (n-1) \\times n\n\\] For example, 5 factorial can be represented as\n\\[\n5! = 5 \\times 4 \\times 3 \\times 2 \\times 1 = 120\n\\]\nIn R, the function factorial allows one to estimate the factorial of a positive integer\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "mod-4-1-probability.html#combinations",
    "href": "mod-4-1-probability.html#combinations",
    "title": "20  Introduction to Probability",
    "section": "20.6 Combinations",
    "text": "20.6 Combinations\nIn our previous examples, the number of steps and outcomes for each step was relatively small. Determining the number of outcomes in a sample space can quickly become tedious, or, as complexity increases, hard. We can, however, rely on two counting rules to assist us in determining the number of outcomes in a sample space. The difference between the two counting rules is whether the order of the outcomes matters.\nA combination examines the number of ways the objects can be selected, without regard to the order in which they are selected.\nA bowl of vegetable soup contains a different vegetables and the number of vegetables may differ by type. We don’t care about ordering since all the vegetables together make soup.\nAssume we have four objects, and we select one object at a time without replacement.\nHow many ways can the four objects be drawn given we draw one object at a time without replacement?\nLet’s list the possible combinations:\n\nABC\nABD\nACD\nBCD\n\nWhy are there only four combinations? Remember, the order does not matter. In other words, ABC is the same as BCA which is the same as CAB which is the same as BAC and so on.\nAssume now that we have the four objects and we select two objects at a time without replacement.\nAssume the objects are labeled A, B, C, D and the order of drawing is not important (AC is the same as drawing CA), then the number of possible combinations:\n\nAB\nAC\nAD\nBC\nBD\nCD\n\nIn other words, given four objects and two objects are selected at a time, there are six possible combinations.\nLastly, consider the same number of objects (4) and three objects are selected at a time without replacement.\nAssume the objects are labeled A, B, C, D and the order of drawing is not important (AC is the same as drawing CA), then the number of possible combinations:\n\nABC\nABD\nACD\nBCD\n\nThere are four possible combinations.\nObviously, if the number of objects becomes ‘large,’ it would be quite difficult to list all the possible combinations.",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "mod-4-1-probability.html#combination-formula",
    "href": "mod-4-1-probability.html#combination-formula",
    "title": "20  Introduction to Probability",
    "section": "20.7 Combination Formula",
    "text": "20.7 Combination Formula\nThe number of combinations of \\(N\\) objects taken \\(n\\) at a time is equal:\n\\[C_n^N =  \\binom{N}{n} = \\frac{N!}{n!(N-n)!}\\] Returning to the example where there were four objects and one object is selected at a time without replacement, note that \\(N = 4\\) and \\(n = 1\\), we can calculate that there are 4 possible combinations that can be drawn.\n\\[C_1^4 =  \\binom{4}{1} = \\frac{4!}{1!(4-1)!} = \\frac{24}{6} = 4\\]\nReturning to the example where there were four objects and two objects were selected at a time without replacement, note that \\(N = 4\\) and \\(n = 2\\), we can calculate that there are 6 possible combinations that can be drawn.\n\\[C_2^4 =  \\binom{4}{2} = \\frac{4!}{2!(4-2)!} = \\frac{24}{4} = 6\\] Returning to the example where there were four objects and two objects were selected at a time without replacement, note that \\(N = 4\\) and \\(n = 3\\), we can calculate that there are 4 possible combinations that can be drawn.\n\\[C_3^4 =  \\binom{4}{3} = \\frac{4!}{3!(4-3)!} = \\frac{24}{6} = 4\\]",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "mod-4-1-probability.html#combinations-in-r",
    "href": "mod-4-1-probability.html#combinations-in-r",
    "title": "20  Introduction to Probability",
    "section": "20.8 Combinations in R",
    "text": "20.8 Combinations in R\nIn the following examples, we show how to calculate the factorial of a value in R, the manual calculation of combination without replacement, and using the choose} function to calculate the combination without replacement.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "mod-4-1-probability.html#lottery-example",
    "href": "mod-4-1-probability.html#lottery-example",
    "title": "20  Introduction to Probability",
    "section": "20.9 Lottery Example",
    "text": "20.9 Lottery Example\nA good example is a lottery. Assume that the lottery requires you to pick six numbers from 1 to 70. The numbers are picked without replacement. In other words, there are 70 objects (\\(N\\)) and 6 objects (\\(n\\)) are selected without replacement?\nHow many possible outcomes are there in the sample space for this lottery? The example code below demonstrates how to calculate the number of outcomes in the sample space when there are 70 objects and 6 objects are selected without replacement. There are 131,115,985 possible combinations or outcomes in the sample space for this experiment.\nHowever, lotteries found that larger jackpots attracted more customers. How do you boost the jackpot? One method is to have players pick a number of balls from one set of numbers without replacement and then another ball from another set of numbers. This “megaball” or “powerball” increases the number of possible combinations.\nIn the Virginia lottery Megamillions game, you pick 5 balls from 70 balls and then one ball from a separate 25 balls. The number of possible outcomes is equal to the combination of 70 balls picking 5 without replacement and then 1 ball from 25 possible balls. As you can see in the example below, the number of potential combinations increases to 302,575,350.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "mod-4-1-probability.html#permutations",
    "href": "mod-4-1-probability.html#permutations",
    "title": "20  Introduction to Probability",
    "section": "20.10 Permutations",
    "text": "20.10 Permutations\nWhile a combination does not require the objects to be drawn in a specific order, a permutation requires a specific ordering of objects.\nOne way to think about permutations is “this, then that.” In other words, one object must be drawn before another object for each possible outcome. Think about throwing a fair six-sided die two times. A roll of 1 on the first throw and 2 on the second throw is different than a roll of 2 on the first roll and 1 on the second roll. The order of selection is important unlike combinations.\nLet \\(N\\) be the number of objects and \\(n\\) are the number of objects that are selected without replacement. The number of permutations of \\(N\\) objects taken \\(n\\) at a time is defined as:\n\\[\nP_n^N = n! \\binom{N}{n} = \\frac{N!}{(N-n)!}\n\\]\nBecause the ordering matters, the number of outcomes from an ordered experiment is higher than the number of outcomes in an unordered experiment. In other words \\(P_n^N \\ge C_n^N\\).\nTo see this, assume that there are four possible objects:\n\nBlack (B)\nGreen (G)\nRed (R)\nWhite (W)\n\nGiven four objects, assume that two objects are selected without replacement.\nCombinations (6): BG, BR, BW, GR, GW, RW\nPermutations (12): BG, BR, BW, GB, GR, GW, RB, RG, RW, WB, WG, WR\nIn the example code below, the number of combinations and permutations are calculated manually.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "mod-4-1-probability.html#permutation-example",
    "href": "mod-4-1-probability.html#permutation-example",
    "title": "20  Introduction to Probability",
    "section": "20.11 Permutation Example",
    "text": "20.11 Permutation Example\nIn the example code below, the number of combinations and permutations are calculated using the choose and permutations functions.\nThe combination and permutations functions will create a matrix of outcomes for the random experiment.\nThese matrices grow large quickly and it may be more efficient to manually code the combinations and permutations.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "mod-5-0-overview.html",
    "href": "mod-5-0-overview.html",
    "title": "23  Module 5 Overview",
    "section": "",
    "text": "23.1 Introduction\nWelcome to Module 5 of ECON 700.\nThis module introduces students to the foundational concepts of probability and their application in data-driven analysis. Students begin by exploring the difference between discrete and continuous random variables, followed by the construction and interpretation of probability distributions. Using examples such as coin tosses, dice rolls, and manufacturing outcomes, the module develops an understanding of how randomness, expected value, and variance describe uncertainty and predict long-run behavior. Students will also examine empirical probability distributions using simulated and real-world data in R.\nBuilding on these principles, the module introduces the binomial and Poisson distributions—two key models for analyzing the likelihood of events within defined conditions. Learners use R to calculate and visualize probability mass functions, cumulative distribution functions, and inverse functions, as well as to simulate random variables. By the end of the module, students will understand how probability distributions underpin inferential statistics and decision-making in economics and data analysis.",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Module 5 Overview</span>"
    ]
  },
  {
    "objectID": "mod-5-0-overview.html#learning-objectives",
    "href": "mod-5-0-overview.html#learning-objectives",
    "title": "23  Module 5 Overview",
    "section": "23.2 Learning Objectives",
    "text": "23.2 Learning Objectives\nBy the end of this module, you should be able to:\n\nDifferentiate between discrete and continuous random variables and explain their roles in probability analysis.\nConstruct and interpret discrete probability distributions using empirical and theoretical data.\nCalculate expected value, variance, and standard deviation for discrete random variables.\nApply the binomial and Poisson probability functions to model real-world random processes.\nUse R to compute and visualize probability mass, cumulative, and inverse distribution functions.\nEvaluate how probability models support statistical reasoning and decision-making in economics.",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Module 5 Overview</span>"
    ]
  },
  {
    "objectID": "mod-5-0-overview.html#readings-resources",
    "href": "mod-5-0-overview.html#readings-resources",
    "title": "23  Module 5 Overview",
    "section": "23.3 Readings & Resources",
    "text": "23.3 Readings & Resources\n\nBarbara Illowsky and Susan Dean. Introductory Statistics 2e. OpenStax. (Chapter 4). Available for free online at https://openstax.org/books/introductory-statistics-2e/pages/2-introduction\nIrizarry, R. (2025). Introduction to Data Science: Statistics and Prediction Algorithms Through Case Studies (Chapters 1-3). Available for free at: https://rafalab.dfci.harvard.edu/dsbook-part-2/\nWickham, H., & Grolemund, G. (2017). R for Data Science. (Chapters 1–3).\nOnline R documentation: https://cran.r-project.org/manuals.html\nRStudio Cheat Sheets",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Module 5 Overview</span>"
    ]
  },
  {
    "objectID": "mod-5-0-overview.html#activities",
    "href": "mod-5-0-overview.html#activities",
    "title": "23  Module 5 Overview",
    "section": "23.4 Activities",
    "text": "23.4 Activities\n\nComplete the hands-on coding exercises embedded in each lesson.\n\nComplete the weekly class assignment.\n\nParticipate in the discussion form.\n\nTake the weekly knowledge quiz.",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Module 5 Overview</span>"
    ]
  },
  {
    "objectID": "mod-5-0-overview.html#lessons-in-this-module",
    "href": "mod-5-0-overview.html#lessons-in-this-module",
    "title": "23  Module 5 Overview",
    "section": "23.5 Lessons in this Module",
    "text": "23.5 Lessons in this Module\n\n5.1 – Discrete Random Variables\n\n\nNext: Start with **Discrete Random Variables*.",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Module 5 Overview</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ECON 700",
    "section": "",
    "text": "0.1 Course Introduction\nNote: The course is under construction. Some links will not work.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>ECON 700</span>"
    ]
  },
  {
    "objectID": "index.html#course-introduction",
    "href": "index.html#course-introduction",
    "title": "ECON 700",
    "section": "",
    "text": "Course Objectives and Materials\n\n\nMeet Your Instructor\n\n\nSyllabus",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>ECON 700</span>"
    ]
  },
  {
    "objectID": "index.html#module-01-introduction-to-r-and-descriptive-statistics",
    "href": "index.html#module-01-introduction-to-r-and-descriptive-statistics",
    "title": "ECON 700",
    "section": "0.2 Module 01 – Introduction to R and Descriptive Statistics",
    "text": "0.2 Module 01 – Introduction to R and Descriptive Statistics\n\n\nModule 1.0 - Module 1 Overview\n\n\nModule 1.1 - Data and the Challenge of Economic Analysis\n\n\nModule 1.2 – Introduction to R\n\n\nModule 1.3 – Variables and Assignments in R\n\n\nModule 1.4 – Data frames in R\n\n\nModule 1.5 – Using Quarto",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>ECON 700</span>"
    ]
  },
  {
    "objectID": "index.html#module-02---descriptive-statistics",
    "href": "index.html#module-02---descriptive-statistics",
    "title": "ECON 700",
    "section": "0.3 Module 02 - Descriptive Statistics",
    "text": "0.3 Module 02 - Descriptive Statistics\n\n\nModule 2.0 - Module 2 Overview\n\n\nModule 2.1 – Frequency Distributions\n\n\nModule 2.2 – Measures of Central Tendency\n\n\nModule 2.3 – Measures of Dispersion\n\n\nModule 2.4 – Z-Scores",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>ECON 700</span>"
    ]
  },
  {
    "objectID": "index.html#module-03-covariance-and-correlation",
    "href": "index.html#module-03-covariance-and-correlation",
    "title": "ECON 700",
    "section": "0.4 Module 03 – Covariance and Correlation",
    "text": "0.4 Module 03 – Covariance and Correlation\n\n\nModule 3.0 - Module 3 Overview\n\n\nModule 3.1 - Skewness\n\n\nModule 3.2 - Chebyshev’s Theorem\n\n\nModule 3.3 - Covariance and Correlation",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>ECON 700</span>"
    ]
  },
  {
    "objectID": "index.html#module-04---introduction-to-probability",
    "href": "index.html#module-04---introduction-to-probability",
    "title": "ECON 700",
    "section": "0.5 Module 04 - Introduction to Probability",
    "text": "0.5 Module 04 - Introduction to Probability\n\n\nModule 4.0 - Module 4 Overview\n\n\nModule 4.1 - Introduction to Probability\n\n\nModule 4.2 - Probability Rules\n\n\nModule 4.3 - Conditional Probability",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>ECON 700</span>"
    ]
  },
  {
    "objectID": "index.html#module-05---discrete-probability-distributions",
    "href": "index.html#module-05---discrete-probability-distributions",
    "title": "ECON 700",
    "section": "0.6 Module 05 - Discrete Probability Distributions",
    "text": "0.6 Module 05 - Discrete Probability Distributions\n\n\nModule 5.0 - Module 5 Overview\n\n\nModule 5.1 - Discrete Random Variables\n\n\n\n\nThis site provides interactive and static examples for economic data analysis using R and WebR. Click on the links above to explore each section.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>ECON 700</span>"
    ]
  },
  {
    "objectID": "mod-5-1-discrete.html",
    "href": "mod-5-1-discrete.html",
    "title": "24  Discrete Random Variables",
    "section": "",
    "text": "24.1 Discrete Random Variables\nA random variable is a numerical description of an outcome of an example.\nA discrete random variable may assume a finite or infinite number of values.\nA fair coin flip or roll of a six-sided die is a random experiment with a finite number of outcomes.\nFor the coin flip, the random variable is the face of the coin, the values of which are: heads (1) and tails (0).\nFor the six-sided die, the random variable the number of dots showing on the die, the values of which are: 1,2,3,4,5,6.\nIf the random experiment is operating a website for a 24-hour period, the random variable is the number of unique visitors to the website.\nThe values of the random variable are: 0, 1, 2, 3 ……",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "mod-5-1-discrete.html#continuous-random-variables",
    "href": "mod-5-1-discrete.html#continuous-random-variables",
    "title": "24  Discrete Random Variables",
    "section": "24.2 Continuous Random Variables",
    "text": "24.2 Continuous Random Variables\nA continuous random variable may assume any value in an interval or collection of intervals.\nIf we select 1,000 students at random who come to ODU today, the driving distance is bounded below at 0.\nThe driving distance is practically bounded above (maybe 200 miles?).\nDriving distance is a continuous random variable as it can take on any value from 0 to infinity (and beyond!).\nIf the random experiment is a visit to a webpage by a customer, the random variable is the time spent on the page, with values of the random variable being \\(x \\ge 0\\).\nIf the random experiment is to fill a kiloliter container, the random variable is the amount of liquid in the container, with values of the random variable being \\(0 \\le x \\le 1\\, kl\\).",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "mod-5-1-discrete.html#discrete-probability-distributions",
    "href": "mod-5-1-discrete.html#discrete-probability-distributions",
    "title": "24  Discrete Random Variables",
    "section": "24.3 Discrete Probability Distributions",
    "text": "24.3 Discrete Probability Distributions\nThe probability distribution for a random variable describes how probabilities are distributed over the values of the random variable.\nGiven a discrete random variable, \\(x\\), a probability function, \\(f(x)\\), provides the probability for each value of the random variable.\nThe classical method of assigning probabilities of the values of a random variable is used when the experimental outcomes generate values of the random variable that are equally likely.\nConsider the experiment of rolling a six-sided die.\n\nWe know the sample space is \\(s = \\{1, 2, 3, 4, 5, 6\\}\\)\nLet \\(x\\) be the number obtained on one throw of the die\nLet \\(f(x)\\) be the probability of \\(x\\)\nWe can represent the probability of \\(x\\) with a frequency table or graph.\n\nIn the code below, we run a simulated six-sided die 10,000 times.\n\nThe sample function takes a sample of the sequence of integers from 1 to 6\nThe size option sets the number of samples to be taken\nThe replace = TRUE replaces the number sampled\nThe group_by identifies the bins\nThe summarize command estimates the frequencies and relative frequencies\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "mod-5-1-discrete.html#frequency-tables",
    "href": "mod-5-1-discrete.html#frequency-tables",
    "title": "24  Discrete Random Variables",
    "section": "24.4 Frequency Tables",
    "text": "24.4 Frequency Tables\nThe relative frequency method is applicable when the data are “reasonably” large.\nThe discrete probability distribution that is developed using the relative frequency method is known as the empirical discrete distribution.\nTwo conditions are required for a discrete probability distribution.\n\\[f(x) \\ge 0\\]\n\\[\\sum f(x) = 1\\] In the code below, we replicate the experiment of throwing a six-sided die 10,000 times. We estimate the relative frequency for each of the outcomes of the experiment and create a relative frequency table that missors the graph.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "mod-5-1-discrete.html#discrete-probability-distribution",
    "href": "mod-5-1-discrete.html#discrete-probability-distribution",
    "title": "24  Discrete Random Variables",
    "section": "24.5 Discrete Probability Distribution",
    "text": "24.5 Discrete Probability Distribution\nA formula that gives the probability function, \\(f(x)\\) for every value of \\(x\\) is known as a discrete probability distribution.\nUsing data, we developed the empirical discrete probability function for a fair six-sided die.\nLet \\(n\\) be equal to the number of values of that the random variable can take.\nThe discrete uniform probability distribution is then defined as:\n\\[f(x) = \\frac{1}{n}\\]\nNote that we can apply the discrete uniform probability distribution function to several of our previous examples.\nFlipping a fair coin: \\[f(x) = \\frac{1}{2}\\]\nRolling a fair six-sided die: \\[f(x) = \\frac{1}{6}\\]",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "mod-5-1-discrete.html#expected-value",
    "href": "mod-5-1-discrete.html#expected-value",
    "title": "24  Discrete Random Variables",
    "section": "24.6 Expected Value",
    "text": "24.6 Expected Value\nThe expected value, or mean, of a random variable is a measure of the central location of the random variable.\nThe expected value of \\(x\\) is defined as\n\\[E(x) = \\mu = \\sum x \\, f(x)\\]\nFor the six-side die experiment, the expected value will approach 3.5 as the number of rolls increases.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n]]",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Discrete Random Variables</span>"
    ]
  }
]