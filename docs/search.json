[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ECON 700",
    "section": "",
    "text": "0.1 Course Introduction",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>ECON 700</span>"
    ]
  },
  {
    "objectID": "index.html#course-introduction",
    "href": "index.html#course-introduction",
    "title": "ECON 700",
    "section": "",
    "text": "Course Objectives and Materials\n\n\nMeet Your Instructor\n\n\nSyllabus",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>ECON 700</span>"
    ]
  },
  {
    "objectID": "index.html#module-01-introduction-to-r-and-descriptive-statistics",
    "href": "index.html#module-01-introduction-to-r-and-descriptive-statistics",
    "title": "ECON 700",
    "section": "0.2 Module 01 – Introduction to R and Descriptive Statistics",
    "text": "0.2 Module 01 – Introduction to R and Descriptive Statistics\n\n\nModule 1.0 - Module 1 Overview\n\n\nModule 1.1 - Data and the Challenge of Economic Analysis\n\n\nModule 1.2 – Introduction to R\n\n\nModule 1.3 – Variables and Assignments in R\n\n\nModule 1.4 – Data frames in R\n\n\nModule 1.5 – Using Quarto",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>ECON 700</span>"
    ]
  },
  {
    "objectID": "index.html#module-02---descriptive-statistics",
    "href": "index.html#module-02---descriptive-statistics",
    "title": "ECON 700",
    "section": "0.3 Module 02 - Descriptive Statistics",
    "text": "0.3 Module 02 - Descriptive Statistics\n\n\nModule 2.0 - Module 2 Overview\n\n\nModule 2.1 – Frequency Distributions\n\n\nModule 2.2 – Measures of Central Tendency\n\n\nModule 2.3 – Measures of Dispersion\n\n\nModule 2.4 – Z-Scores",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>ECON 700</span>"
    ]
  },
  {
    "objectID": "index.html#module-03-covariance-and-correlation",
    "href": "index.html#module-03-covariance-and-correlation",
    "title": "ECON 700",
    "section": "0.4 Module 03 – Covariance and Correlation",
    "text": "0.4 Module 03 – Covariance and Correlation\n\n\nModule 3.0 - Module 3 Overview\n\n\nModule 3.1 - Skewness\n\n\nModule 3.2 - Chebyshev’s Theorem\n\n\nModule 3.3 - Covariance and Correlation",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>ECON 700</span>"
    ]
  },
  {
    "objectID": "index.html#module-04---introduction-to-probability",
    "href": "index.html#module-04---introduction-to-probability",
    "title": "ECON 700",
    "section": "0.5 Module 04 - Introduction to Probability",
    "text": "0.5 Module 04 - Introduction to Probability\n\n\nModule 4.0 - Module 4 Overview\n\n\nModule 4.1 - Introduction to Probability\n\n\nModule 4.2 - Probability Rules\n\n\nModule 4.3 - Conditional Probability",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>ECON 700</span>"
    ]
  },
  {
    "objectID": "index.html#module-05---discrete-probability-distributions",
    "href": "index.html#module-05---discrete-probability-distributions",
    "title": "ECON 700",
    "section": "0.6 Module 05 - Discrete Probability Distributions",
    "text": "0.6 Module 05 - Discrete Probability Distributions\n\n\nModule 5.0 - Module 5 Overview\n\n\nModule 5.1 - Discrete Random Variables\n\n\nModule 5.2 - Binomial Probability Distribution\n\n\nModule 5.3 - Poisson Probability Distribution",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>ECON 700</span>"
    ]
  },
  {
    "objectID": "index.html#module-06---continuous-probability-distributions",
    "href": "index.html#module-06---continuous-probability-distributions",
    "title": "ECON 700",
    "section": "0.7 Module 06 - Continuous Probability Distributions",
    "text": "0.7 Module 06 - Continuous Probability Distributions\n\n\nModule 6.0 - Module 6 Overview\n\n\nModule 6.1 - Uniform Probability Distributions\n\n\nModule 6.2 - Normal Probability Distributions\n\n\nModule 6.3 - Exponential Probability Distributions",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>ECON 700</span>"
    ]
  },
  {
    "objectID": "index.html#module-07---sampling-distributions",
    "href": "index.html#module-07---sampling-distributions",
    "title": "ECON 700",
    "section": "0.8 Module 07 - Sampling Distributions",
    "text": "0.8 Module 07 - Sampling Distributions\n\n\nModule 7.0 - Module 7 Overview\n\n\nModule 7.1 - Sampling\n\n\nModule 7.2 - Central Limit Theorem\n\n\nModule 7.3 - Interval Estimation",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>ECON 700</span>"
    ]
  },
  {
    "objectID": "index.html#module-08---inference",
    "href": "index.html#module-08---inference",
    "title": "ECON 700",
    "section": "0.9 Module 08 - Inference",
    "text": "0.9 Module 08 - Inference\n\n\nModule 8.0 - Module 8 Overview\n\n\nModule 8.1 - Hypothesis Testing\n\n\nModule 8.2 - Hypothesis Testing: Means\n\n\nModule 8.3 - Hypothesis Testing: Variance\n\n\n\n\nThis site provides interactive and static examples for economic data analysis using R and WebR. Click on the links above to explore each module.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>ECON 700</span>"
    ]
  },
  {
    "objectID": "mcnab.html",
    "href": "mcnab.html",
    "title": "2  Meet Your Instructor",
    "section": "",
    "text": "2.1 About\nChair, Department of Economics | rmcnab@odu.edu | 757-683-3153\nProfessor Robert M. McNab currently serves as the Chair of the Department of Economics in the Strome College of Business at Old Dominion University. Dr. McNab is also the Director of the Dragas Center for Economic Analysis and Policy at Old Dominion University. He is a member of the Federal Reserve Bank of Philadelphia’s Survey of Professional Forecasters and, from 2018 to 2022, he was a member of the Joint Advisory Board of Economists for the Commonwealth of Virginia. Professor McNab has published in Applied Economics, Cornell Hospitality Quarterly, Defense and Peace Economics, National Tax Journal, Public Budgeting and Finance, and World Development, among others. He edits the annual State of the Region: Hampton Roads and State of the Commonwealth reports Professor McNab has appeared in the Associated Press, China Global Television Network, CNN, LiveNow from Fox News, Forbes, Newsweek, Wall Street Journal, Washington Post, Welt am Sonntag, Yahoo News, Yahoo Finance, Richmond Times-Dispatch, Virginian Pilot, Virginia Public Media, and Daily Press, among others. Dr. McNab joined the faculty of the Department of Economics in the Strome College of Business of Old Dominion University in July 2016 and previously was a member of the faculty of the Naval Postgraduate School in Monterey, California from 2000 to 2016.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Meet Your Instructor</span>"
    ]
  },
  {
    "objectID": "mcnab.html#contact-policy",
    "href": "mcnab.html#contact-policy",
    "title": "2  Meet Your Instructor",
    "section": "2.2 Contact Policy",
    "text": "2.2 Contact Policy\nStudents should feel welcome to contact me via email at rmcnab@odu.edu or drop by Zoom office hours. I strongly encourage students to communicate with me. I will try to answer emails within 48 business hours (often much sooner) for course related topics.\nStudents should take the time to craft complete, professional emails. The more information that you can provide about a question or problem, the more likely that my response will be helpful. Avoid non-professional language and practice communicating in the corporate workplace. Emails that are unprofessional will be returned with no action.\nStudents should proactively address issues with the class and scheduling rather than waiting until an assignment or case study discussion is overdue. In many cases, accommodations can be made for ‘life events,’ however, clear and prompt communication is necessary.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Meet Your Instructor</span>"
    ]
  },
  {
    "objectID": "mcnab.html#virtual-office-hours",
    "href": "mcnab.html#virtual-office-hours",
    "title": "2  Meet Your Instructor",
    "section": "2.3 Virtual Office Hours",
    "text": "2.3 Virtual Office Hours\nOffice hours will be held on a rotating schedule and are available by appointment. Check the class announcements for the schedule of office hours, which vary to accommodate students with different schedules.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Meet Your Instructor</span>"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "3  Course Objectives",
    "section": "",
    "text": "3.1 Course Learning Objectives (CLOs)",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Course Objectives</span>"
    ]
  },
  {
    "objectID": "about.html#course-learning-objectives-clos",
    "href": "about.html#course-learning-objectives-clos",
    "title": "3  Course Objectives",
    "section": "",
    "text": "CLO 1: Formulate economic research questions and testable hypotheses that can be examined using publicly available data\nCLO 2: Acquire, manage, and prepare economic data by applying programming techniques in R, including data extraction from APIs and other official sources.\nCLO 3: Apply descriptive statistics, probability concepts, and sampling methods to summarize, visualize, and interpret economic data.\nCLO 4: Conduct hypothesis testing and interval estimation to draw valid inferences from economic and financial data.\nCLO 5: Utilize R and RStudio to implement statistical techniques and effectively communicate results through reproducible code, visualizations, and written analysis.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Course Objectives</span>"
    ]
  },
  {
    "objectID": "about.html#textbooks-and-materials",
    "href": "about.html#textbooks-and-materials",
    "title": "3  Course Objectives",
    "section": "3.2 Textbooks and Materials",
    "text": "3.2 Textbooks and Materials\nThe required textbooks are an integral part of the class. Students should expect to read the textbooks prior to class and to be prepared to answer discussion questions from the textbook. These textbooks provide a foundation upon which the course is built and students will succeed if they take the time to read and review the material in the textbooks.\nThese textbooks are free.\nAs this course focuses on the learning and application of statistical techniques, a computer with R and R Studio installed is necessary. You will need access to a laptop, tablet, or other computing device to complete the assignments and exams.\nYou may also use R Studio Cloud instead of a locally installed version of R.\nYou can create a free R-Studio cloud if you do not want to use a locally installed version of R and R Studio.\nAll other course materials are on Canvas or can be accessed through Canvas. You are required to know how to use Canvas to access and submit assignments, quizzes, and examinations. You should also be comfortable using ODU email to communicate.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Course Objectives</span>"
    ]
  },
  {
    "objectID": "about.html#required-texts",
    "href": "about.html#required-texts",
    "title": "3  Course Objectives",
    "section": "3.3 Required Texts",
    "text": "3.3 Required Texts\nThe following textbooks are available for free.\nPlease select the links provided for each to access the textbooks.   \nBarbara Illowsky and Susan Dean. Introductory Statistics 2e. OpenStax.\nChristoph Hanck, Martin Arnold, Alexander Gerber,and Martin Schmelzer. (2025). Introduction to Econometrics with R.\nRafael Irizarry. (2025). Introduction to Data Science: Data Wrangling and Visualization with R.\nHadley Wickham, Mine Cetinkaya-Rundel, and Garrett Grolemund. (2025) R for Data Science.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Course Objectives</span>"
    ]
  },
  {
    "objectID": "about.html#optional-materials",
    "href": "about.html#optional-materials",
    "title": "3  Course Objectives",
    "section": "3.4 Optional Materials",
    "text": "3.4 Optional Materials\nDavid R. Anderson, Dennis J. Sweeney, Thomas A. Williams, Jeffrey D. Camm, and James J. Cochran. Statistics for Business and Economics 14th Edition. Earlier editions will suffice.\nAngrist, J.D. & Pischke, Jorn-Steffen. (2013). Mostly Harmless Econometrics: An Empiricist’s Companion. Content Technologies Inc.\nBaruffa, O. (2025). The Big Book of R\nKennedy, P. (2003). A Guide to Econometrics. MIT Press.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Course Objectives</span>"
    ]
  },
  {
    "objectID": "mod-1-0-overview.html",
    "href": "mod-1-0-overview.html",
    "title": "4  Module 1 Overview",
    "section": "",
    "text": "4.1 Introduction\nWelcome to Module 1 of ECON 700.\nIn this module, you will be introduced to the R programming environment and its integrated interface, RStudio. You will learn how to install and navigate both tools while becoming familiar with R’s basic syntax and data types. You will also learn how to import, view, and explore datasets to build a foundational understanding of data handling in R. Finally, you will apply descriptive statistical techniques to perform initial data exploration and summarize key insights from the datasets.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Module 1 Overview</span>"
    ]
  },
  {
    "objectID": "mod-1-0-overview.html#learning-objectives",
    "href": "mod-1-0-overview.html#learning-objectives",
    "title": "4  Module 1 Overview",
    "section": "4.2 Learning Objectives",
    "text": "4.2 Learning Objectives\nBy the end of this module, you should be able to:\n\nMLO 1: Install and navigate R and RStudio. (CLO 5)\n\nMLO 2: Identify R’s basic syntax and data types. (CLO 5)\n\nMLO 3: Work with variables and simple operations in R.(CLO 5)\n\nMLO 4: Explore data frames and tibbles in R. (CLO 5)\n\nMLO 5: Apply descriptive statistics for initial data exploration. (CLO 2, 5)",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Module 1 Overview</span>"
    ]
  },
  {
    "objectID": "mod-1-0-overview.html#required-texts",
    "href": "mod-1-0-overview.html#required-texts",
    "title": "4  Module 1 Overview",
    "section": "4.3 Required Texts",
    "text": "4.3 Required Texts\nThe following textbooks are available for free. Please select the links provided to access.\n\nBarbara Illowsky and Susan Dean. Introductory Statistics 2e. OpenStax. Chapter 1.\nRafael Irizarry. (2025). Introduction to Data Science: Data Wrangling and Visualization with R. Chapters 1-2.\nHadley Wickham, Mine Cetinkaya-Rundel, and Garrett Grolemund. (2025) R for Data Science. Sections 1-19.\nOnline R documentation\nRStudio Cheat Sheets",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Module 1 Overview</span>"
    ]
  },
  {
    "objectID": "mod-1-0-overview.html#module-to-do-list",
    "href": "mod-1-0-overview.html#module-to-do-list",
    "title": "4  Module 1 Overview",
    "section": "4.4 Module To Do List",
    "text": "4.4 Module To Do List\n\nComplete the hands-on coding exercises embedded in each lesson (MLO 3,4)\n\nUse WebR to experiment directly in your browser (MLO 2, 3, 4)\n\nComplete the weekly class assignment (MLO 1-5)\n\nTake the weekly knowledge quiz (MLO 2-5)",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Module 1 Overview</span>"
    ]
  },
  {
    "objectID": "mod-1-0-overview.html#lessons-in-this-module",
    "href": "mod-1-0-overview.html#lessons-in-this-module",
    "title": "4  Module 1 Overview",
    "section": "4.5 Lessons in this Module",
    "text": "4.5 Lessons in this Module\n\n1.1 – Introduction to Economic Analysis\n\n1.2 – Introduction to R\n\n1.3 – Variables in R\n\n1.4 - Data frames in R\n1.5 - Using Quarto in R\n\n\nNext: Start with Economic Analysis.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Module 1 Overview</span>"
    ]
  },
  {
    "objectID": "mod-1-1-intro-to-data.html",
    "href": "mod-1-1-intro-to-data.html",
    "title": "5  Economic Analysis",
    "section": "",
    "text": "5.1 Data and the Challenge of Economic Analysis\nWe are awash in data. The challenge is to find the right data at the appropriate time to investigate an empirical question of interest. In some cases, the data are well defined, consistently formatted, readily available, and structured to facilitate analysis. Then there are data that could help answer a question, but are not in an easily accessible format. But, before tackling these problems, we need to ask the simple question:\nWhat are data?\nThe Merriam-Webster dictionary defines data as: “…factual information (such as measurements or statistics) used a basis for reasoning, discussion, or calculation.” Broadly speaking, data can consist of measurements regarding the health of individuals, performance of firms, the income of regions, the inflation rate of a national economy, among other things. Data can also consist of statistics that describe the properties of an underlying set of measurements.\nThe Bureau of Labor Statistics (BLS) defines the civilian labor force as “The labor force includes all people age 16 and older who are classified as either employed and unemployed. Conceptually, the labor force level is the number of people who are either working or actively looking for work.”\nHow many people are in the civilian labor force in the United States? How many in Virginia? Is the labor force larger today than it was at the same time last year? How does the growth in the labor force in Virginia compare to other states? All these questions require obtaining data and, for some, manipulating data to create statistics about the properties of the data.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Economic Analysis</span>"
    ]
  },
  {
    "objectID": "mod-1-1-intro-to-data.html#headline-unemployment-rate",
    "href": "mod-1-1-intro-to-data.html#headline-unemployment-rate",
    "title": "5  Economic Analysis",
    "section": "5.2 Headline Unemployment Rate",
    "text": "5.2 Headline Unemployment Rate\nThe headline unemployment rate is an example of a measure that has different values across time. Collectively, these measures are data on the headline unemployment rate. This rate is equal to the ratio of the number of unemployed persons in the civilian labor force to the number of individuals in the civilian labor force.\nIn the following figure, we plot the evolution of the unemployment rate over time.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Economic Analysis</span>"
    ]
  },
  {
    "objectID": "mod-1-1-intro-to-data.html#unemployment-by-race",
    "href": "mod-1-1-intro-to-data.html#unemployment-by-race",
    "title": "5  Economic Analysis",
    "section": "5.3 Unemployment by Race",
    "text": "5.3 Unemployment by Race\nData can consist of measurements of a variable across groups of individuals and time. In the following figure, we present the headline unemployment rate by selected racial groups in the United States. Unlike the national unemployment rate which encompasses all individuals in the civilian labor force, each group represents all individuals by race relative to the civilian labor force by race. We can observe how the responsiveness of the unemployment rate differed by race.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Economic Analysis</span>"
    ]
  },
  {
    "objectID": "mod-1-1-intro-to-data.html#unemployment-rate-for-selected-states",
    "href": "mod-1-1-intro-to-data.html#unemployment-rate-for-selected-states",
    "title": "5  Economic Analysis",
    "section": "5.4 Unemployment Rate for Selected States",
    "text": "5.4 Unemployment Rate for Selected States\nIn the table below, we obtain data on unemployment rates for Virginia, Maryland, North Carolina, West Virginia, and the United States from the Federal Reserve Economic Data (FRED) website.\nIn the table, each row corresponds to a geography (state or nation) while each column represents a variable for that geography. Note that only the last period available is represented in the table.\n\n\nUnemployment Rate for Selected Geographies\n\n\nFRED Symbol\nDate of Observation\nValue\nValue in Decimal Form\nGeography\n\n\n\n\nUNRATE\n2025-11-01\n4.6\n0.046\nUnited States",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Economic Analysis</span>"
    ]
  },
  {
    "objectID": "mod-1-1-intro-to-data.html#statistics-and-data",
    "href": "mod-1-1-intro-to-data.html#statistics-and-data",
    "title": "5  Economic Analysis",
    "section": "5.5 Statistics and Data",
    "text": "5.5 Statistics and Data\nThe term statistics can refer to numerical facts such as minimums, maximums, averages, medians, variances, and standard deviations (among others) that describe the properties of data.\nStatistics can also refer to the art and science of collecting, analyzing, interpreting, and presenting data.\nData are the facts collected, analyzed, and summarized for interpretation and presentation. A data set refers to all the data collected for a specific study.\nData are plural, while a data set is singular.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Economic Analysis</span>"
    ]
  },
  {
    "objectID": "mod-1-1-intro-to-data.html#elements-variables-observations",
    "href": "mod-1-1-intro-to-data.html#elements-variables-observations",
    "title": "5  Economic Analysis",
    "section": "5.6 Elements, Variables, Observations",
    "text": "5.6 Elements, Variables, Observations\n\nAn element is the entity or entities on which data are collected.\nA variable is a characteristic of interest for the elements.\nAn observation is the set of measurements for an element.\n\nA data set consists of 1,200 individuals with data concerning individual income and job tenure.\n\n1,200 observations (elements)\n2 variables (income and tenure)\n2,400 data values\n\n\n\n\nExample Data Set\n\n\nIndividual\nIncome\nJob Tenure\n\n\n\n\n1\n23420\n8\n\n\n2\n33239\n10\n\n\n3\n44849\n14\n\n\n4\n54829\n17\n\n\n5\n75793\n39",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Economic Analysis</span>"
    ]
  },
  {
    "objectID": "mod-1-1-intro-to-data.html#measurement",
    "href": "mod-1-1-intro-to-data.html#measurement",
    "title": "5  Economic Analysis",
    "section": "5.7 Measurement",
    "text": "5.7 Measurement\nReferring to the unemployment rate table, there are two rows with observations for the selected geographical areas. In other words, the elements are the states on which data are collected and presented in the table. There are five variables in the table: the symbol of the variable from FRED, the date of the observation, the value of the observation, the value of the observation in decimal form, and the geography name of the observation.\nThe table contains different scales of measurement. The date variable contains information on time while the state variable is measured on the nominal scale. The nominal scale is used to identify the observational data, that is, the data in the table are organized by the geographical area of the observation. The data pertaining to the unemployment rate is ratio data in that the measurements contain numeric information.\nBroadly speaking, data can be classified as categorical or quantitative. In the table, the variable containing the geographical names is categorical. We cannot mathematically manipulate the categorical data (subtracting 1 from Virginia does not produce a meaningful result). We could, however, create a new variable that contains numerical values to represent the categorical data, that is, the new quantitative variable would be a numerical representation of the categorical data.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Economic Analysis</span>"
    ]
  },
  {
    "objectID": "mod-1-1-intro-to-data.html#nominal-scale-of-measurement",
    "href": "mod-1-1-intro-to-data.html#nominal-scale-of-measurement",
    "title": "5  Economic Analysis",
    "section": "5.8 Nominal Scale of Measurement",
    "text": "5.8 Nominal Scale of Measurement\nThe scale of measurement determines the amount of information contained in the data. Data are labels or names used to identify an attribute of an element.\nData may be numeric, non-numeric, or both. Ranking is not implied by the numeric values.\nThe individual’s grade can be represented by a character label (“Freshman”) or a numeric value (1).\n\n\n\nNominal Example\n\n\nObservation\nName\nGrade\nNumeric Grade\n\n\n\n\n1\nBob\nFreshman\n4\n\n\n2\nSally\nSophmore\n3\n\n\n3\nJim\nJunior\n2\n\n\n4\nTony\nFreshman\n4\n\n\n5\nSarah\nSenior\n1",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Economic Analysis</span>"
    ]
  },
  {
    "objectID": "mod-1-1-intro-to-data.html#ordinal-scale-of-measurement",
    "href": "mod-1-1-intro-to-data.html#ordinal-scale-of-measurement",
    "title": "5  Economic Analysis",
    "section": "5.9 Ordinal Scale of Measurement",
    "text": "5.9 Ordinal Scale of Measurement\n\nOrdinal scales of measurement have the same properties as nominal scales of measure.\nThe ordering of the ordinal scale is meaningful.\nData may be numeric, non-numeric, or both.\nThe class rank variable is numeric and the values represent a ranking, that is, the lower the number, the higher the rank in the class of students.\n\n\n\n\nOrdinal Example\n\n\nObs\nName\nGrade\nClass Rank\n\n\n\n\n1\nBob\nFreshman\n3\n\n\n2\nSally\nFreshman\n5\n\n\n3\nJim\nFreshman\n29\n\n\n4\nTony\nFreshman\n55\n\n\n5\nSarah\nFreshman\n144",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Economic Analysis</span>"
    ]
  },
  {
    "objectID": "mod-1-1-intro-to-data.html#interval-scale-of-measurement",
    "href": "mod-1-1-intro-to-data.html#interval-scale-of-measurement",
    "title": "5  Economic Analysis",
    "section": "5.10 Interval Scale of Measurement",
    "text": "5.10 Interval Scale of Measurement\n\nInterval scales of measurement have the same properties as ordinal scales of measurement.\nThe ranking of the scale contains information.\nThe intervals between observations contain information.\nInterval data are always numeric.\nThe SAT score is ordinal while the difference between the highest scoring student and another student is an interval measure.\n\n\n\n\nInterval Example\n\n\nObs\nName\nSAT Score (Ordinal)\nInterval to Bob\n\n\n\n\n1\nBob\n1580\n0\n\n\n2\nSally\n1320\n260\n\n\n3\nJim\n1110\n470\n\n\n4\nTony\n1052\n528\n\n\n5\nSarah\n952\n628",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Economic Analysis</span>"
    ]
  },
  {
    "objectID": "mod-1-1-intro-to-data.html#ratio-scale-of-measurement",
    "href": "mod-1-1-intro-to-data.html#ratio-scale-of-measurement",
    "title": "5  Economic Analysis",
    "section": "5.11 Ratio Scale of Measurement",
    "text": "5.11 Ratio Scale of Measurement\n\nA ratio scale of measurement has the properties of the interval scale and the ratio of the two values is useful.\nThe ratio is numeric and zero is a possible value.\nCare must be taken to define and interpret the ratio data.\nStudents take a physical fitness test across three events with a maximum possible score of 300 points.\nThe ratio represents the percentage score of each student relative to the maximum score on the test.\n\n\n\n\nRatio Example\n\n\nObs\nName\nRaw Score (300 Max)\nRatio\n\n\n\n\n1\nBob\n248\n0.83\n\n\n2\nSally\n232\n0.77\n\n\n3\nJim\n178\n0.59\n\n\n4\nTony\n157\n0.52\n\n\n5\nSarah\n109\n0.36",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Economic Analysis</span>"
    ]
  },
  {
    "objectID": "mod-1-1-intro-to-data.html#categorical-measures",
    "href": "mod-1-1-intro-to-data.html#categorical-measures",
    "title": "5  Economic Analysis",
    "section": "5.12 Categorical Measures",
    "text": "5.12 Categorical Measures\n\nCategorical variables represent types of data which may be divided into groups.\nLabels or names identify an attribute of each observation.\nCategorical and qualitative data may be used synonymous.\nCan be numeric or non-numeric.\nCan be nominal or ordinal scale of measurement.\n\n\n\n\nCategorical Example\n\n\nObs\nName\nRace\nOpinion\n\n\n\n\n1\nBob\nBlack\nAgree\n\n\n2\nSally\nWhite\nDisagree\n\n\n3\nJim\nAsian\nNone\n\n\n4\nTony\nWhite\nAgree\n\n\n5\nSarah\nBlack\nStrongly Agree",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Economic Analysis</span>"
    ]
  },
  {
    "objectID": "mod-1-1-intro-to-data.html#quantitative-versus-qualitative-data",
    "href": "mod-1-1-intro-to-data.html#quantitative-versus-qualitative-data",
    "title": "5  Economic Analysis",
    "section": "5.13 Quantitative versus Qualitative Data",
    "text": "5.13 Quantitative versus Qualitative Data\n\nQuantitative data capture how many or how much.\nQuantitative data are always numeric.\nQuantitative data may be ordinal, ranked, interval, ratio or a combination of all types of scales.\nQuantitative data may capture how many or how much but may also capture non-quantitative information.\nOpinion surveys, for example, may ask respondents to provide a numeric scale from strongly agree (1) to strongly disagree (5).\nThe numeric measure is a proxy for the strength of agreement or disagreement.\nIn many instances, qualitative responses are transformed into numerical representations to allow for quantitative analysis.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Economic Analysis</span>"
    ]
  },
  {
    "objectID": "mod-1-1-intro-to-data.html#cross-sectional-time-series-and-panel-data",
    "href": "mod-1-1-intro-to-data.html#cross-sectional-time-series-and-panel-data",
    "title": "5  Economic Analysis",
    "section": "5.14 Cross-Sectional, Time Series, and Panel Data",
    "text": "5.14 Cross-Sectional, Time Series, and Panel Data\nThere are three broad types of data. Let’s establish some terminology. When we discuss data, we often refer to individuals and time. In this context, individuals can represent people, firms, airlines, cars, or some other characteristic that defines the unique observations.\nFor example, if we had data on the average fuel mileage of passenger vehicles in 2019, then individuals would refer to each type of passenger vehicle. If we had randomized data on 100,000 taxpayers in Virginia for 2018, then individuals would refer to unique taxpayers. Lastly, if we collected data on airplane arrivals at Dulles International Airport for each day in 2019, then we would have individual observations across time. We could organize this data by plane registration, airline, or country of origin, or some other category of interest.\nCross-sectional data are data collected across individuals at the same point of time. There is no depth to cross-sectional data, that is, the data represent a snapshot at a specific point in time.\nTime series data are data collected over several time periods. Data for the headline unemployment graph for the United States are time series data. In this context, time series data refers to one variable across time. One can have a collection of time series variables (inflation rate, unemployment rate, and so on). Time series data has depth but not breadth.\nPanel data are data collected across individuals and time. If we collected data for the unemployment rate across states from 2013 to 2025, then we would have individuals (states) and time (months) as defining characteristics of our data. Panel data has breadth (across individuals) and depth (across time).",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Economic Analysis</span>"
    ]
  },
  {
    "objectID": "mod-1-1-intro-to-data.html#types-of-studies",
    "href": "mod-1-1-intro-to-data.html#types-of-studies",
    "title": "5  Economic Analysis",
    "section": "5.15 Types of Studies",
    "text": "5.15 Types of Studies\n\nObservational\n\nObservational studies are non-experimental studies.\nNo attempt is made (or can be made) to control or influence the variables of interest.\nIf we study the impact of smoking on birth weight, we do not control who smokes and who does not smoke during pregnancy.\nIf we examine how exports influence economic growth across countries, we do not control export policy.\n\nConvenience\n\nA convenience study is the most common method of non-probability sampling.\nObservations are obtained because individuals are available to the researcher.\nThis type of study is cheaper and easier than other types, hence it’s popularity.\nThere is no attempt to control for sampling or to randomly sample the population of interest.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Economic Analysis</span>"
    ]
  },
  {
    "objectID": "mod-1-1-intro-to-data.html#population-versus-sample-data",
    "href": "mod-1-1-intro-to-data.html#population-versus-sample-data",
    "title": "5  Economic Analysis",
    "section": "5.16 Population versus Sample Data",
    "text": "5.16 Population versus Sample Data\nWhen working with data, a question may arise whether it is appropriate to work with the population or a sample.\nThe population represents all the individuals for a particular study while a sample is a subset of the population.\nLet’s say we wanted to calculate the average age of students at Old Dominion University for the Fall 2025 semester. If we had access to student records, this might be a relatively easy exercise. In other words, if we had access to data for the population of students, we could calculate the average age of the student population.\nNow, let’s assume we wanted to calculate the average age for all college students in the United States. The problem has become more difficult. We could try and obtain all the records from all the colleges and universities in the United States but this effort might be costly and complex.\nWe could instead sample different colleges and universities and construct an estimate of the average age of college students. The “closer” our sample is to the population, the better our estimate of the average age of all the college students in the United States. If we only selected colleges and universities with undergraduate programs, our sample would likely understate the average age of college students. If we only selected graduate programs at colleges and universities, our sample would likely overstate the average age of college students.\nThe process by which we use a sample to estimate the properties of the population is statistical inference.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Economic Analysis</span>"
    ]
  },
  {
    "objectID": "mod-1-1-intro-to-data.html#descriptive-statistics",
    "href": "mod-1-1-intro-to-data.html#descriptive-statistics",
    "title": "5  Economic Analysis",
    "section": "5.17 Descriptive Statistics",
    "text": "5.17 Descriptive Statistics\n\nDescriptive statistics are a set of tools used to describe the properties of a set of data.\nDescriptive statistics can be numerical, tabular, or graphical.\nDescriptive statistics typically summarize the properties of the individual observations.\nWe can create a population for 10,000 observations that have a mean of 50.5 and a standard deviation of 10.\nWe can then estimate sample averages to describe the mean of the data.\n\nGathering a sample that is representative of the population is the first step. We then must understand the properties of the sample and how this reflects the properties of the population.\n\n# Create a data frame with 10,000 observations\n# mean of the observations is 50.5\n\ndata_1 &lt;- tibble(x1 = rnorm(10000, mean = 50.5, sd = 10))\n\ndata_2 &lt;- slice_sample(data_1, n = 10) %&gt;% \n          summarize(mean_10 = mean(x1))\n\ndata_3 &lt;- slice_sample(data_1, n = 100) %&gt;% \n          summarize(mean_100 = mean(x1))\n\ndata_4 &lt;- slice_sample (data_1, n = 1000) %&gt;% \n          summarize(mean_1000 = mean(x1))\n\ndata_5 &lt;- cbind(data_2, data_3, data_4)\n\nkable(data_5,\n      col.names = c(\"Sample = 10\",\n                    \"Sample = 100\",\n                    \"Sample = 1000\"),\n      align     = c('c', 'c', 'c'),\n      digits    = 2,\n      caption   = 'Sample Averages') %&gt;% \n  column_spec(2:3, width = \"3cm\") %&gt;%\n  kable_styling(full_width = TRUE,\n                font_size  = 10)\n\n\nSample Averages\n\n\nSample = 10\nSample = 100\nSample = 1000\n\n\n\n\n49.78\n49.92\n50.44",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Economic Analysis</span>"
    ]
  },
  {
    "objectID": "mod-1-1-intro-to-data.html#statistical-inference",
    "href": "mod-1-1-intro-to-data.html#statistical-inference",
    "title": "5  Economic Analysis",
    "section": "5.18 Statistical Inference",
    "text": "5.18 Statistical Inference\n\nObtaining data for the population of interest may be difficult, complex, and costly.\nFor cost, complexity, and time considerations, we may use a subset or sample of the population.\nThe decennial Census of the population attempts to count every person in the United States.\nEvery non-Census year, a sample of 1-2 million individuals is taken to estimate the properties of the population.\nStatistical inference is the process of using sample data to make estimates and test hypotheses about the characteristics of the population.\nSince we are using sample data, our estimates are not precise, but come with a measure of error.\nThis course is, at its core, about using sample data to make inferences about the population.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Economic Analysis</span>"
    ]
  },
  {
    "objectID": "mod-1-2-intro-to-r.html",
    "href": "mod-1-2-intro-to-r.html",
    "title": "6  Introduction to R",
    "section": "",
    "text": "6.1 Why use R and RStudio?\nWe will use R and RStudio extensively in this class. You will also use these programs extensively in the following classes on the econometrics sequence.\nA frequently asked question is “why can’t we use Excel?”\nThe simple answer is that Excel is good for some things but R is better for the things we want to do in this and other classes.\nWriting code (programming) allows for:\nOther programs have lower fixed costs than R. These programs, whether Excel, SAS, SPSS, or TSP, have ‘built in’ functions and interfaces that allow ‘easier entry’ than R. In other words, you will invest less time to learn how to start working in other programs than R.\nSo, why then use R? First, coding is like learning a second language. It takes a while to learn the logic and ‘flow’ of the programming language. Second, unlike many other programs, you have a very good idea of what R is doing. If you write code, you learn where data come from, how they are formatted, the properties of data, and you gain a much deeper understanding of the process by which you arrive at an answer.\nAnother strength of this approach is that once you learn to code in R and use RStudio, your marginal cost of learning Python, C++, or SQL is lower because the logic behind each of these languages is similar.\nSimply put, coding is as much about the journey as the destination.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "mod-1-2-intro-to-r.html#why-use-r-and-rstudio",
    "href": "mod-1-2-intro-to-r.html#why-use-r-and-rstudio",
    "title": "6  Introduction to R",
    "section": "",
    "text": "Reproducibility (code provides transparency to processes)\nCustomization (many solutions to a problem)\nAutomation (code once, run many times)\nAccountability (the programmer is responsible for what the code does)",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "mod-1-2-intro-to-r.html#r-and-rstudio-an-introduction",
    "href": "mod-1-2-intro-to-r.html#r-and-rstudio-an-introduction",
    "title": "6  Introduction to R",
    "section": "6.2 R and RStudio: An Introduction",
    "text": "6.2 R and RStudio: An Introduction\nYou will need to install R and RStudio.\nEach is a piece of software. R is the ‘engine’ and RStudio is the interface.\nYou can work with R directly, but RStudio makes the process less painful.\nR is the statistical software package that we will use throughout the course.\nRStudio is an interface that makes using R much easier.\nYou should first install R and then install RStudio.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "mod-1-2-intro-to-r.html#downloading-r",
    "href": "mod-1-2-intro-to-r.html#downloading-r",
    "title": "6  Introduction to R",
    "section": "6.3 Downloading R",
    "text": "6.3 Downloading R\n\nSelect the link to access: The R Project for Statistical Computing\nSelect the correct operating system to download and install R.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "mod-1-2-intro-to-r.html#downloading-rstudio",
    "href": "mod-1-2-intro-to-r.html#downloading-rstudio",
    "title": "6  Introduction to R",
    "section": "6.4 Downloading RStudio",
    "text": "6.4 Downloading RStudio\n\nSelect the link to access and download: RStudio\nNote how Posit tells you to install R and then RStudio.\nYou can also use Posit Cloud from Posit.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "mod-1-2-intro-to-r.html#tutorials-in-r",
    "href": "mod-1-2-intro-to-r.html#tutorials-in-r",
    "title": "6  Introduction to R",
    "section": "6.5 Tutorials in R",
    "text": "6.5 Tutorials in R\nThere are numerous tutorials online on how to use R.\nSome overviews that might be helpful:\n\nThe Big Book of R\nGetting Started with R\nR Programming for Beginners\nR Programming Tutorial",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "mod-1-2-intro-to-r.html#how-to-get-better-at-coding",
    "href": "mod-1-2-intro-to-r.html#how-to-get-better-at-coding",
    "title": "6  Introduction to R",
    "section": "6.6 How to Get Better at Coding",
    "text": "6.6 How to Get Better at Coding\nNo coder works alone. Everyone (and yes, I mean everyone) relies on the expertise of others to code.\nWhat does this mean?\nWhen you are working on code, try it yourself first. If something doesn’t work, look for small typos.\nYou can then start asking other resources for help.\nArtificial Intelligence (AI) is helpful to debug code. It can even write code for you, if you know what you are doing.\nRemember, however, that AI is only as good as its algorithm and your prompt. It can (and will) give you answers that are incorrect, answers that appear correct but are wrong, or lead you through solution paths that are inefficient.\nCoding is a dance between what you know and what you can find. The more that you do on your own, the better you will be able to use AI to help you improve (not write) your code.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "mod-1-2-intro-to-r.html#opening-rstudio",
    "href": "mod-1-2-intro-to-r.html#opening-rstudio",
    "title": "6  Introduction to R",
    "section": "6.7 Opening RStudio",
    "text": "6.7 Opening RStudio\n\n\n\nScreenshot of RStudio in Windows\n\n\nWhen you open RStudio for the first time, you will see four panels like in the above image. It is likely that your version of RStudio has a white background with blue or black text.\nIf you would like to change this, go to “Tools &gt; Global Options… &gt; Appearance &gt; Editor theme”.\nYou can choose a darker theme, or leave it a lighter theme.\nThe four panels are as follows:\n\nTop Left: Source – This is where you will write the R code you want to save. In other words, this is where you write and save your work, usually called R scripts (.R files).\nBottom Left: Console – When you execute (or run) code, you will usually see output here. This is also a place you can write code you do not want to be part of your final script. If you were a painter, the Source panel would be your canvas and the Console would be your palette.\nTop Right: Environment – Here is where we will be able to see all the objects (data, etc.) that we are working with in the moment. To clear your environment, use the code rm(list = ls()).\nBottom Right: Output – This is mostly where you will see plots you have generated, but can also see files on your computer, packages you have installed, and “Help” for certain functions.\n\n\n\n\nScreenshot of Rstudio with panel labels.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "mod-1-2-intro-to-r.html#packages-in-r",
    "href": "mod-1-2-intro-to-r.html#packages-in-r",
    "title": "6  Introduction to R",
    "section": "6.8 Packages in R",
    "text": "6.8 Packages in R\nOne of the most powerful features of R is that there are numerous packages or add-ons to base R that provide additional functionality. We need to install these packages and then include them so they are accessible to R as we work with data.\nIf you need to install a package, you can always use the install.packages command. For example, you could use install.packages(“dplyr”) to install the dplyr package.\nOnce a package is installed, you typically will not need to re-install it. You may need to update your packages, however, and this can be easily done. You can either use the function update.packages or update packages through the packages menu in RStudio.\nIn the following code, we clear the workspace (essentially erasing working memory) and load three packages.\nFirst, we clear memory with the command rm(list = ls()).\nSecond, we load the packages into memory by using the library statement.\nFor example, library(dplyr) loads the dplyr package so we can use its functions later in our code. Note, that if you have not installed the dplyr package, the code will not run.\nTo learn more about each package, see the following links.\n\ndplyr\nggplot2\nlubridate\ntidyverse\nkable\n\n\n#Clear the Workspace\n\nrm(list = ls())\n\n#Load packages\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(lubridate)\nlibrary(tidyverse)\nlibrary(kableExtra)\n\nTo do this in RStudio, first install the above packages and then use the following steps\n\nOpen RStudio\nSelect File | New File | R Script\nCopy the code above and paste it into your script file\nClick on “Run” or Use CTRL+SHIFT+ENTER\n\n\n\n\nScreenshot of Rstudio with package code",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "mod-1-2-intro-to-r.html#webr",
    "href": "mod-1-2-intro-to-r.html#webr",
    "title": "6  Introduction to R",
    "section": "6.9 WebR",
    "text": "6.9 WebR\nWe can use WebR to provide dynamic examples of code. The WebR code chunks appear different than the static code chucks.\nAs you can see below, you can run the WebR code in your browser. You can add, subtract, or change the code chunk entirely.\nIn this example, we first clear the memory, then we do three operations\n\nAdd 5 and 13 or \\((5 + 13)\\)\nMultiply 5 and 13 or \\((5 \\times 13)\\)\nDivide 5 by 13 or \\(\\frac{5}{13}\\)\nRaise 5 to the 5th power or \\(5^{5}\\)\nTake the square root of 52 or \\(\\sqrt{64}\\)\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIn the next section, we will explore how to create variables in R and how to assign values to variables.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "mod-1-3-intro-to-r-variables.html",
    "href": "mod-1-3-intro-to-r-variables.html",
    "title": "7  Variables in R",
    "section": "",
    "text": "7.1 Assigning Numeric Values to Variables\nWe can use the assignment operator in R to assign values to a variable. The assignment operator can be thought of as moving a value, character, date, or other form into a named variable.\nThe assignment operator is “&lt;-”.\nThe line, “x2 &lt;- 1056”, means assign the value 1056 to the variable x2.\nIn the code below, we assign the numerical value of 2 to the variable x1 and the numerical value of 4.5 to the variable x2.\nWe can assign the value contained in one variable in another. For example, we can assign x2 to x3 and also specify that the integer value of x2 is assigned to x3.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Variables in R</span>"
    ]
  },
  {
    "objectID": "mod-1-3-intro-to-r-variables.html#assigning-numeric-values-to-variables",
    "href": "mod-1-3-intro-to-r-variables.html#assigning-numeric-values-to-variables",
    "title": "7  Variables in R",
    "section": "",
    "text": "Please enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Variables in R</span>"
    ]
  },
  {
    "objectID": "mod-1-3-intro-to-r-variables.html#assigning-character-values-to-a-variable",
    "href": "mod-1-3-intro-to-r-variables.html#assigning-character-values-to-a-variable",
    "title": "7  Variables in R",
    "section": "7.2 Assigning Character Values to a Variable",
    "text": "7.2 Assigning Character Values to a Variable\nWe can assign character values to a variable. In the following code, we assign the name “Timothy” as a character to the variable name1.\nThere is a subtle but important difference to storing a value as a character or as a numeric variable.\nWe assign the numeric or quantitative value 06250 to x4 and 06250 as a character or qualitative value to name2.\nNote what happens: when stored as a numeric value, 06250 is stored as 6250. When 06250 is stored as “06250” as a character variable, the character variable does not “lose” then “0” in the front of “06250”.\nThis can be very important if you are storing an identification value with a leading 0.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Variables in R</span>"
    ]
  },
  {
    "objectID": "mod-1-3-intro-to-r-variables.html#assigning-date-values-to-a-variable",
    "href": "mod-1-3-intro-to-r-variables.html#assigning-date-values-to-a-variable",
    "title": "7  Variables in R",
    "section": "7.3 Assigning Date Values to a Variable",
    "text": "7.3 Assigning Date Values to a Variable\nWe can also store dates and work with dates. We assign 2020-09-01 a date value using the lubridate package. Note that we tell the package that the format of the date is year-month-day (ymd).\nLikewise, we assign 09-05-2020 to date2 as a date variable but tell the package that the format of the date is now month-day-year (mdy).\nNote that even though the dates are in different formats, the lubridate package transforms the dates into a format recognizable by R.\nHaving transformed the date values, we can calculate the time difference by assigning the difference between the two dates to variable date3.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Variables in R</span>"
    ]
  },
  {
    "objectID": "mod-1-3-intro-to-r-variables.html#creating-vectors",
    "href": "mod-1-3-intro-to-r-variables.html#creating-vectors",
    "title": "7  Variables in R",
    "section": "7.4 Creating Vectors",
    "text": "7.4 Creating Vectors\nWe can now create a vector that contains several values. In the following example, we combine several numbers and assign them to the variable x5.\nWe can transpose the variable x5. Notice the difference between x5 and its transpose. X5, when output to a table, has five rows and one column. The transpose of X5, when it output to a table, is one row and five columns.\nWe can also combine other variables (if they are the same type) to form a new vector. You can combine single element variables with vectors to create a new vector x6.\nWe can also combine vectors. We create x7 by combining the variables x1, x2, x3, x4 with the vector x5.\n\nrm(list = ls())\n\n#Load packages\n\nlibrary(kableExtra, quiet = TRUE)\n\n# Assign values to x1, x2, x3, x4\n\nx1 &lt;- 1\nx2 &lt;- -5\nx3 &lt;- 136\nx4 &lt;- 0.47\n\n#Create a vector \n\nx5 &lt;- c(12, 13, 14, 25, 100)\n\n#Transpose x5\n\nx6 &lt;- t(x5)\n\n#Use existing variables to create a row vector\n\nx7 &lt;- c(x1, x2, x3, x4, x5)\n\n# Use kable To Create Tables of X5 and X6\n# align = 'c' -&gt; aligns to center\n# digits = 0  -&gt; no decimals\n# col.names -&gt; sets a name for the column\n\nkable(x5,\n      align = 'c',\n      digits = 0,\n      col.names = c('X5'),\n      caption = 'Table Containing Variable X5')\n\nkable(x6,\n      align = 'c',\n      col.names = c('A','B','C','D','E'),\n      caption = 'Table Containing Transpose of X5')\n\nkable(x7,\n      align = 'c',\n      digits = 0,\n      col.names = c('X7'),\n      caption = 'Table Containing Variable X7')\n\n\nTable Containing Variable X5\n\n\nX5\n\n\n\n\n12\n\n\n13\n\n\n14\n\n\n25\n\n\n100\n\n\n\n\nTable Containing Transpose of X5\n\n\nA\nB\nC\nD\nE\n\n\n\n\n12\n13\n14\n25\n100\n\n\n\n\nTable Containing Variable X7\n\n\nX7\n\n\n\n\n1\n\n\n-5\n\n\n136\n\n\n0\n\n\n12\n\n\n13\n\n\n14\n\n\n25\n\n\n100\n\n\n\n\n\nWhen we display x5, for example, we note is a vector 5 elements. We can also directly manipulate x5 by multiplying it by two.\nWe should, however, understand that product of x5 and 2 is not stored, it’s a direct manipulation and not available for future use.\nIf we wanted to store it, we would have to assign it to a variable. Here, we assign the product of x5 and 2 to the variable x8.\n\nrm(list = ls())\n\n#Load packages\n\nlibrary(kableExtra, quiet = TRUE)\n\n#Create a vector \n\nx5 &lt;- c(12, 13, 14, 25, 100)\n\nkable(x5*2,\n      align = 'c',\n      digits = 2,\n      col.names = c('X5 * 2'),\n      caption = 'Table of Variable X5 - Each Element Times 2')\n\n#Assign x5*2 to x8\n\nx8 &lt;- x5*2\n\nkable(x8,\n      align = 'c',\n      digits = 1,\n      col.names = c('X8'),\n      caption = 'Table of Variable X8')\n\n\nTable of Variable X5 - Each Element Times 2\n\n\nX5 * 2\n\n\n\n\n24\n\n\n26\n\n\n28\n\n\n50\n\n\n200\n\n\n\n\nTable of Variable X8\n\n\nX8\n\n\n\n\n24\n\n\n26\n\n\n28\n\n\n50\n\n\n200",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Variables in R</span>"
    ]
  },
  {
    "objectID": "mod-1-3-intro-to-r-variables.html#assignment-and-vector-practice",
    "href": "mod-1-3-intro-to-r-variables.html#assignment-and-vector-practice",
    "title": "7  Variables in R",
    "section": "7.5 Assignment and Vector Practice",
    "text": "7.5 Assignment and Vector Practice\nWe have now worked on the basics of assignment elements to vectors and manipulating vectors in R.\nAs practice, try the following.\nCreate a vector made of the following numbers: 5, 10, 12, 24.\nCreate a second vector that divides each element of the first vector by 2.\nOutput the second vector to a table.\nYou can build off the example below.\n\nrm(list = ls())\n\nlibrary(kableExtra, quiet = TRUE)\n\nvector_1 &lt;- c(4, 16, 25, 81, 10000)\n\nvector_2 &lt;- sqrt(vector_1)\n\nkable(vector_2,\n      align = 'c',\n      col.names = 'Vector 2')\n\n\n\n\nVector 2\n\n\n\n\n2\n\n\n4\n\n\n5\n\n\n9\n\n\n100",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Variables in R</span>"
    ]
  },
  {
    "objectID": "mod-1-4-intro-to-r-dataframes.html",
    "href": "mod-1-4-intro-to-r-dataframes.html",
    "title": "8  Data frames in R",
    "section": "",
    "text": "8.1 Creating a Data Frame\nSo far, everything we have created has been stored as a variable or vector. A matrix consists of rows and columns. The organization of the matrix is important. In some cases, the rows will correspond to individual observations with variables in the columns. In other cases, the variables are in the rows and individuals in columns. Matrices can contain quantitative and qualitative elements.\nA matrix is a table or a two-dimensional array-like structure. In R, a data frame is a list of variables with the same number of rows with unique row names. If we use the dplyr package, a tibble is similar to a data frame.\nA data frame can have one column with numeric elements, another with date elements, another with character elements, and so on.\nIn the webR code chunk below, we create two vectors, x8 and x9.\nWe can bind these vectors together, that is, ‘merge’ the two vectors. How we choose to put the vectors together is important.\nThere are two functions at our disposal: rbind and cbind.\nThe rbind function “stacks” the observations in the vectors.\nThe cbind function “adds” the column of the second vector to the first vector, creating a data frame with two columns.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data frames in R</span>"
    ]
  },
  {
    "objectID": "mod-1-4-intro-to-r-dataframes.html#creating-a-data-frame",
    "href": "mod-1-4-intro-to-r-dataframes.html#creating-a-data-frame",
    "title": "8  Data frames in R",
    "section": "",
    "text": "8.1.1 Using rbind\nIn the chunk below, we use the rbind function to bind the two vectors together.\nFirst, we bind x9 to x8. We then bind x8 to x9.\nNote that we bind within the kable function. We do not create a new matrix.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n8.1.2 Using cbind\nIn the following chunk, we demonstrate the cbind function, that is, column bind.\nFirst, we bind x9 to x8. We then bind x8 to x9.\nNote that we bind within the kable function. We do not create a new matrix.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data frames in R</span>"
    ]
  },
  {
    "objectID": "mod-1-4-intro-to-r-dataframes.html#creating-a-data-frame-1",
    "href": "mod-1-4-intro-to-r-dataframes.html#creating-a-data-frame-1",
    "title": "8  Data frames in R",
    "section": "8.2 Creating a Data Frame",
    "text": "8.2 Creating a Data Frame\nPreviously, we created a matrix by binding two vectors together.\nWe can now create a data frame or tibble.\nA tibble is a more modern version of the standard R data frame. It is part of the tidyverse collection of packages.\nSelect the link to learn more about tibbles.\nIn the code chunk below, we again create our two variables. This time, however, we store the two variables in a data frame (original R) and a tibble (new and improved).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data frames in R</span>"
    ]
  },
  {
    "objectID": "mod-1-4-intro-to-r-dataframes.html#using-an-existing-dataframe",
    "href": "mod-1-4-intro-to-r-dataframes.html#using-an-existing-dataframe",
    "title": "8  Data frames in R",
    "section": "8.3 Using an Existing Dataframe",
    "text": "8.3 Using an Existing Dataframe\nR comes with a number of “built in” data sets. For the following discussion, we use the mtcars data. The data contain observations on a number of cars from the Motor Trend magazine.\nOne can work directly with the mtcars data or assign the mtcars data to a data frame. In the following, we assign mtcars to the cardata data frame. We use the assignment operator so that data flows from mtcars to cardata.\nWe can now use the head or tail functions to examine the structure of the cardata data frame. Note that the head function returns the first six rows while the tail function returns the last six rows.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data frames in R</span>"
    ]
  },
  {
    "objectID": "mod-1-4-intro-to-r-dataframes.html#dimensions-of-a-data-frame",
    "href": "mod-1-4-intro-to-r-dataframes.html#dimensions-of-a-data-frame",
    "title": "8  Data frames in R",
    "section": "8.4 Dimensions of a Data Frame",
    "text": "8.4 Dimensions of a Data Frame\nTo find the dimensions of a data frame, we can determine the number of rows and the number of columns separately or together. First, the nrow and ncol functions determine the number of rows and columns in a data frame, respectively.\nWe can also use the dim function to return the dimensions of the data frame. Note that the dim function returns rows then columns.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data frames in R</span>"
    ]
  },
  {
    "objectID": "mod-1-4-intro-to-r-dataframes.html#selecting-rows-or-columns-in-a-data-frame",
    "href": "mod-1-4-intro-to-r-dataframes.html#selecting-rows-or-columns-in-a-data-frame",
    "title": "8  Data frames in R",
    "section": "8.5 Selecting Rows or Columns in a Data Frame",
    "text": "8.5 Selecting Rows or Columns in a Data Frame\nHaving determined the dimensions of the cardata data frame, we can now select parts of the data frame.\nWe can approach this by recognizing that a data frame is organized by rows and columns.\nFor example, mtcars[1,1] returns the value stored in the intersection of the first row and first column in the data frame. The first row in the data frame is for the Mazda RX4 and the first column in the data frame is for miles per gallon, so the intersection of these two produces a value of 21.0. In other words, the Maxda RX4 earns 21.0 miles per gallon.\nSo, in general, for a data frame or tibble, data[row,column].\nFor example, cardata[1:2, 2:3] returns rows 1 to 2 and columns 2 to 3.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIf the row or column is left blank, all values for the column are the result.\nFor example, cardata[,1] would return all rows for the first column of data. On the other hand, cardata[1:2,] would return the first two rows and all the columns.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data frames in R</span>"
    ]
  },
  {
    "objectID": "mod-1-4-intro-to-r-dataframes.html#manipulating-the-dataframe-and-adding-new-variables",
    "href": "mod-1-4-intro-to-r-dataframes.html#manipulating-the-dataframe-and-adding-new-variables",
    "title": "8  Data frames in R",
    "section": "8.6 Manipulating the Dataframe and Adding New Variables",
    "text": "8.6 Manipulating the Dataframe and Adding New Variables\nStarting with the cardata data frame, we can now utilize one of the features of the packages to pipe the data.\nPiping means that the data ‘flows’ in the direction of the pipe. It’s actually called a pipeline of data.\nA bit of housekeeping. We want to convert the rownames in cardata to a variable.\nWe pipe the data frame and then use the function rownames_to_column() to do just that. We then pipe the data to rename the new variable rowname to car.\nWe can now use the mutate function to create a new variable or replace an existing variable.\nIn the following code, the data flow from cardata to the next line of code to create a new variable km_per_gallon using the mutate function.\nImagine a temporary data frame that consists of cardata with a new column for the km_per_gallon variable.\nEach new line of within the mutate function creates a new variable. The second variable that we create converts kilometers per gallon to kilometers per liter or km_per_liter.\nYour temporary data frame now consists of the cardata data frame plus the two new variables.\nThe third line of the mutate statement creates a new variable that converts miles per gallon into kilometers per liter, essentially duplicating the process of the first two lines of the mutate statement. The new variable is called km2_per_liter.\nAt this point, you decide you only need to keep a subset of variables.\nWe pipe the resulting data frame to the select statement. The variables included in the select statement are kept in the temporary data frame while the variables excluded from the select statement are deleted from the temporary data frame.\nYou have now reached the end of the pipe. The “&lt;-” at the beginning then assigns all the data in the temporary data frame to the cardata2 data frame. Because we have used the grammar of piping, the original data frame is unchanged.\nThe grammar of piping is very useful as it allows you to manipulate a data frame without changing the contents of the original data frame.\nNote how we can start using some of the additional features in the kableExtra package to rename column names for improve presentation.\n\nrm(list = ls())\n\nlibrary(dplyr)\nlibrary(kableExtra)\nlibrary(tidyverse)\n\ncardata &lt;- mtcars \n\ncardata2 &lt;- cardata %&gt;%\n  rownames_to_column() %&gt;%\n  rename(car           = rowname) %&gt;%\n  mutate(km_per_gallon = mpg*1.60934,\n         km_per_liter  = km_per_gallon*3.78541,\n         km2_per_liter = mpg*1.60934*3.78541) %&gt;%\n  select(car, mpg, cyl, wt, km_per_gallon, km_per_liter, km2_per_liter)\n\nkable(cardata[1:5,])\n\nkable(cardata2[1:5,],\n      col.names = c(\"Car\",\n                    \"MPG\",\n                    \"Cylinders\",\n                    \"Weight\",\n                    \"Kilometers per Gallon\",\n                    \"Kilometers per Liter\",\n                    \"Squared KM/Gallon\")) %&gt;%\nkable_classic()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\n\n\n\n\n\nCar\n\n\nMPG\n\n\nCylinders\n\n\nWeight\n\n\nKilometers per Gallon\n\n\nKilometers per Liter\n\n\nSquared KM/Gallon\n\n\n\n\n\n\nMazda RX4\n\n\n21.0\n\n\n6\n\n\n2.620\n\n\n33.79614\n\n\n127.9322\n\n\n127.9322\n\n\n\n\nMazda RX4 Wag\n\n\n21.0\n\n\n6\n\n\n2.875\n\n\n33.79614\n\n\n127.9322\n\n\n127.9322\n\n\n\n\nDatsun 710\n\n\n22.8\n\n\n4\n\n\n2.320\n\n\n36.69295\n\n\n138.8979\n\n\n138.8979\n\n\n\n\nHornet 4 Drive\n\n\n21.4\n\n\n6\n\n\n3.215\n\n\n34.43988\n\n\n130.3691\n\n\n130.3691\n\n\n\n\nHornet Sportabout\n\n\n18.7\n\n\n8\n\n\n3.440\n\n\n30.09466\n\n\n113.9206\n\n\n113.9206",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data frames in R</span>"
    ]
  },
  {
    "objectID": "mod-1-4-intro-to-r-dataframes.html#the-fred-api-for-real-gdp",
    "href": "mod-1-4-intro-to-r-dataframes.html#the-fred-api-for-real-gdp",
    "title": "8  Data frames in R",
    "section": "8.7 The FRED API for Real GDP",
    "text": "8.7 The FRED API for Real GDP\nAn Application Programming Interface (API) allows us to obtain data from a source without having to manually downloading the data. In effect, an API allows us to “grab” data from an external source. If the external source updates the data, then our next API call with obtain the updated data.\nFor example, let’s say we wanted to obtain data from FRED on real Gross Domestic Product (GDP) for the United States from 1980 to the most current data available. We could go to FRED, search for real GDP, find the real GDP variable is called GDPC1, and download the data into an Excel file. This process is labor-intensive and subject to error.\n\nFRED\nReal GDP\n\nWe can use an API call to obtain the data. Instead of having to write the code for an API call, we can use a package called tidyquant.\nWe have the variable name GDPC1 and so we can simply use the tq_get function in tidyquant to retrieve the real GDP series.\nWe rename the price variable to rgdp and we convert the date using the lubridate package.\nWe can use the head or kable to print out the contents of the first rows of the resulting data frame.\nWe see the data are organized such that each row corresponds to a time period, that is, the first row represents real GDP for the 1st quarter of 1950, the second row represents real GDP for the 2nd quarter of 1950, and so on.\n\nrm(list = ls())\n\n#Load packages\n\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(tidyverse)\nlibrary(kableExtra)\nlibrary(tidyquant)\n\n#Use tq_get to obtain real GDP from FRED\n\nrgdp &lt;- tq_get(\"GDPC1\",\n                    get  = \"economic.data\",\n                    from = \"1950-01-01\" ) %&gt;%\n        rename(rgdp = price) %&gt;%\n        mutate(date = lubridate::ymd(date))\n\nkable(rgdp[1:5,]) %&gt;%\n  kable_classic()\n\n\n\n\nsymbol\ndate\nrgdp\n\n\n\n\nGDPC1\n1950-01-01\n2346.104\n\n\nGDPC1\n1950-04-01\n2417.682\n\n\nGDPC1\n1950-07-01\n2511.127\n\n\nGDPC1\n1950-10-01\n2559.214\n\n\nGDPC1\n1951-01-01\n2593.967",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data frames in R</span>"
    ]
  },
  {
    "objectID": "mod-1-4-intro-to-r-dataframes.html#the-fred-api-for-nominal-gdp",
    "href": "mod-1-4-intro-to-r-dataframes.html#the-fred-api-for-nominal-gdp",
    "title": "8  Data frames in R",
    "section": "8.8 The FRED API for Nominal GDP",
    "text": "8.8 The FRED API for Nominal GDP\nWe can also make an API call to FRED for the nominal GDP series. Much like the real GDP API call, we have a data frame with the price variable containing the values for nominal GDP. We rename the variable to gdp and also use the lubridate package for the dates.\nSelect the link to view the nominal GDP series: Nominal GDP\n\nrm(list = ls())\n\n#Load packages\n\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(tidyverse)\nlibrary(kableExtra)\nlibrary(tidyquant)\n\n#Use tq_get to obtain nominal GDP from FRED\n\ngdp &lt;- tq_get(\"GDP\",\n                    get  = \"economic.data\",\n                    from = \"1950-01-01\" ) %&gt;%\n        rename(gdp = price) %&gt;%\n        mutate(date = lubridate::ymd(date))\n\nkable(gdp[1:5,]) %&gt;%\n  kable_classic()\n\n\n\n\nsymbol\ndate\ngdp\n\n\n\n\nGDP\n1950-01-01\n280.828\n\n\nGDP\n1950-04-01\n290.383\n\n\nGDP\n1950-07-01\n308.153\n\n\nGDP\n1950-10-01\n319.945\n\n\nGDP\n1951-01-01\n336.000",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data frames in R</span>"
    ]
  },
  {
    "objectID": "mod-1-4-intro-to-r-dataframes.html#the-fred-api-for-nominal-and-real-gdp",
    "href": "mod-1-4-intro-to-r-dataframes.html#the-fred-api-for-nominal-and-real-gdp",
    "title": "8  Data frames in R",
    "section": "8.9 The FRED API for Nominal and Real GDP",
    "text": "8.9 The FRED API for Nominal and Real GDP\nIn the previous examples, we made an API call for one variable at a time. The result was two data frames, one for nominal GDP, and one for real GDP. It would be more efficient to make one API call that results in a single data frame with both variables.\nIn the following code, we combine the names of the two variables, that is, c(“GDP”,“GDPC1”) creates a row vector with the two names. This row vector is used by the tq_get function to make an API call to obtain the two variables from FRED.\nWhat’s important to note is that the all_gdp data fame is in long format. In other words, each row corresponds to one period of time for one variable.\nWe have to do some housecleaning. When we obtain the data from FRED, the variable names are contained in a variable called symbol and the values are contained in a variable named price. We rename those to variable and value, respectively.\nWe then plot the data using the ggplot2 package.\nThe ggplot function is very flexible and you can modify almost every element of a graph. For now, we provide the example and will work on developing our graphing skills later on in the course.\n\nrm(list = ls())\n\n#Load packages\n\nlibrary(dplyr)\nlibrary(kableExtra)\nlibrary(lubridate)\nlibrary(tidyquant)\nlibrary(tidyverse)\nlibrary(ggplot2)\n\n#Use tq_get to obtain nominal and real GDP from FRED\n\nall_gdp &lt;- tq_get(c(\"GDP\", \"GDPC1\"), \n                  get  = \"economic.data\",\n                    from = \"1950-01-01\") %&gt;%\n        mutate(date     = lubridate::ymd(date)) %&gt;%\n        rename(variable = symbol,\n               value    = price)\n\n# Produce Table of First 10 Observations\n\nkable(all_gdp[1:10,],\n      col.names = c(\"Variable\",\n                    \"Date\",\n                    \"Value\"))\n\n\n\nVariable\nDate\nValue\n\n\n\n\nGDP\n1950-01-01\n280.828\n\n\nGDP\n1950-04-01\n290.383\n\n\nGDP\n1950-07-01\n308.153\n\n\nGDP\n1950-10-01\n319.945\n\n\nGDP\n1951-01-01\n336.000\n\n\nGDP\n1951-04-01\n344.090\n\n\nGDP\n1951-07-01\n351.385\n\n\nGDP\n1951-10-01\n356.178\n\n\nGDP\n1952-01-01\n359.820\n\n\nGDP\n1952-04-01\n361.030\n\n\n\n#Use GGPLOT \n#GDP over time\n\nggplot(data = all_gdp, \n       aes(x = date, \n           y = value,\n           color = variable,\n           group = variable)) +\ngeom_line(linewidth = 1) +\nscale_y_continuous(labels = scales::dollar_format()) +\ntheme_minimal() +\ntheme(legend.position = 'bottom',\n      legend.title    = element_blank()) +\nlabs(title    = \"Real and Nominal GDP, United States\",\n     subtitle = \"Billions of Dollars\",\n     x        = \"Date\",\n     y        = \"Billions of Dollars\")",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data frames in R</span>"
    ]
  },
  {
    "objectID": "mod-1-5-quarto.html",
    "href": "mod-1-5-quarto.html",
    "title": "9  Using Quarto",
    "section": "",
    "text": "9.1 What is Quarto?\nQuartro is how we will work with R and RStudio to produce documents, assignments, and exams.\nOne way to think about Quarto is that it is like Microsoft Word, however, it is much more powerful and flexible.\nIn fact, the entire ECON 700 course is built with R, RStudio, and Quarto.\nAs with R and RStudio, the more you work with Quarto, the easier it comes to work with Quarto.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "mod-1-5-quarto.html#getting-started",
    "href": "mod-1-5-quarto.html#getting-started",
    "title": "9  Using Quarto",
    "section": "9.2 Getting Started",
    "text": "9.2 Getting Started\n\nFirst, ensure that you have R and RStudio installed correctly.\nTo install Quarto, go to Quarto\nOnce you have installed Quarto, go to the getting started in the RStudio page.\nThe getting started page will help you understand the capabilities of Quarto. \n\n\n\n\nQuarto Installation Page",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "mod-1-5-quarto.html#creating-a-file",
    "href": "mod-1-5-quarto.html#creating-a-file",
    "title": "9  Using Quarto",
    "section": "9.3 Creating a File",
    "text": "9.3 Creating a File\nIn RStudio, go to File -&gt; New File -&gt; New Quarto Document.\nYou will see the window below. Let’s say we name our document: Example-1.\n\nOnce you have created this file, you will see the following window in RStudio:\n\n\n\nScreenshot of New Quarto Document",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "mod-1-5-quarto.html#code-chunks",
    "href": "mod-1-5-quarto.html#code-chunks",
    "title": "9  Using Quarto",
    "section": "9.4 Code Chunks",
    "text": "9.4 Code Chunks\nR code chunks can be identified with {r}.\nTo start a code chunk, you use ```{r} and you close off the code chunk with ```.\nYou can run the code chunk interactively in RStudio, that is, you can click on the green arrow icon at the top of the code chunk and RStudio will display the results.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "mod-1-5-quarto.html#working-with-quartro",
    "href": "mod-1-5-quarto.html#working-with-quartro",
    "title": "9  Using Quarto",
    "section": "9.5 Working with Quartro",
    "text": "9.5 Working with Quartro\nWe want to create a text description of in our new Quarto document. We also want to create a simple code chunk in our Quarto document that will execute when we render our file.\nIn your new file, let’s write the following text:\nThis is an example Quarto file. You can write text here like a normal document.\nAnd let’s include the following code chunk:\n\nx &lt;- 3\ny &lt;- 6\nz &lt;- 10\n\nx*y\ny*z\nx/z\n\nYour file should look like the following picture:\n\n\n\nScreenshot of Quarto Example Document",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "mod-1-5-quarto.html#rendering",
    "href": "mod-1-5-quarto.html#rendering",
    "title": "9  Using Quarto",
    "section": "9.6 Rendering",
    "text": "9.6 Rendering\nUse the Render button in RStudio to render the file and preview the output. If you prefer to automatically render whenever you save, check the Render on Save button in RStudio.\nWhen you render, you can save your file as Example1.qmd and the rendered file should appear in a browser window.\nQuarto is a powerful approach to displaying your work in R and RStudio.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "mod-1-5-quarto.html#additional-resources",
    "href": "mod-1-5-quarto.html#additional-resources",
    "title": "9  Using Quarto",
    "section": "9.7 Additional Resources",
    "text": "9.7 Additional Resources\n\nGetting started with Quarto video from Posit\nQuarto crash course\nR for Data Science\nIntroduction to Quarto",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "mod-2-0-overview.html",
    "href": "mod-2-0-overview.html",
    "title": "10  Module 2 Overview",
    "section": "",
    "text": "10.1 Introduction\nWelcome to Module 2 of ECON 700.\nIn this module, you will deepen your understanding of how to summarize and interpret data using descriptive statistics. You will learn to organize data into frequency and relative frequency distributions, calculate measures that describe the center and spread of data, and assess how individual observations compare to the overall dataset.\nEmphasis will be placed on using R to compute and visualize descriptive statistics, including the mean, median, mode, range, variance, standard deviation, coefficient of variation, and z-scores. By the end of this module, you will be able to use these tools to describe key features of economic data accurately and communicate your findings effectively through reproducible R-based analyses.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Module 2 Overview</span>"
    ]
  },
  {
    "objectID": "mod-2-0-overview.html#learning-objectives",
    "href": "mod-2-0-overview.html#learning-objectives",
    "title": "10  Module 2 Overview",
    "section": "10.2 Learning Objectives",
    "text": "10.2 Learning Objectives\nBy the end of this module, you should be able to:\n\nMLO 1: Construct and interpret frequency and relative frequency distributions to summarize quantitative data. (CLO 2, 3, 5)\nMLO 2: Compute and interpret measures of central tendency, including the mean, median, mode, and trimmed and weighted means. (CLO 2, 3, 5)\nMLO 3: Calculate and explain measures of variability, including the range, variance, standard deviation, and coefficient of variation. (CLO 2, 3, 5)\nMLO 4: Use z-scores to assess relative position of an observation and identify outliers within a dataset. (CLO 2, 3, 4, 5)\nMLO 5: Apply R to generate, visualize, and interpret descriptive statistics and graphical summaries of economic data. (CLO 5)",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Module 2 Overview</span>"
    ]
  },
  {
    "objectID": "mod-2-0-overview.html#required-texts",
    "href": "mod-2-0-overview.html#required-texts",
    "title": "10  Module 2 Overview",
    "section": "10.3 Required Texts",
    "text": "10.3 Required Texts\nThe following textbooks are available for free. Please select the links provided to access.\n\nBarbara Illowsky and Susan Dean. Introductory Statistics 2e. OpenStax. Chapter 1.\nRafael Irizarry. (2025). Introduction to Data Science: Data Wrangling and Visualization with R. Chapters 1-2.\nHadley Wickham, Mine Cetinkaya-Rundel, and Garrett Grolemund. (2025) R for Data Science. Sections 1-19.\nOnline R documentation\nRStudio Cheat Sheets",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Module 2 Overview</span>"
    ]
  },
  {
    "objectID": "mod-2-0-overview.html#module-to-do-list",
    "href": "mod-2-0-overview.html#module-to-do-list",
    "title": "10  Module 2 Overview",
    "section": "10.4 Module To Do List",
    "text": "10.4 Module To Do List\n\nComplete the hands-on coding exercises embedded in each lesson (MLO 3, MLO 5)\n\nComplete the weekly class assignment (MLO 1 - MLO 5)\n\nParticipate in the discussion form (MLO 1-3)\n\nTake the weekly knowledge quiz (MLO 1 - MLO 5)",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Module 2 Overview</span>"
    ]
  },
  {
    "objectID": "mod-2-0-overview.html#lessons-in-this-module",
    "href": "mod-2-0-overview.html#lessons-in-this-module",
    "title": "10  Module 2 Overview",
    "section": "10.5 Lessons in this Module",
    "text": "10.5 Lessons in this Module\n\n2.1 – Frequency Distributions\n2.2 - Measures of Central Tendency\n2.3 - Measures of Dispersion\n\n\nNext: Start with Frequency Distributions.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Module 2 Overview</span>"
    ]
  },
  {
    "objectID": "mod-2-1-frequency.html",
    "href": "mod-2-1-frequency.html",
    "title": "11  Frequency Distributions",
    "section": "",
    "text": "11.1 Frequency Distributions\nThe frequency of a category, class, or value is the number of times it occurs in a data set.\nA frequency distribution is summary of data.\nA frequency distribution is the pattern of frequencies of variable of interest.\nGiven several non-overlapping categories or classes, a frequency distribution contains the number of observations per class or category.\nA frequency distribution, to be complete, must be:\nA frequency distribution may provide insights about the data that cannot be quickly obtained by examining the data itself.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Frequency Distributions</span>"
    ]
  },
  {
    "objectID": "mod-2-1-frequency.html#frequency-distributions",
    "href": "mod-2-1-frequency.html#frequency-distributions",
    "title": "11  Frequency Distributions",
    "section": "",
    "text": "Mutually exclusive - the classes or categories do not overlap.\nCollectively exhaustive - all the observations are described by the distribution",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Frequency Distributions</span>"
    ]
  },
  {
    "objectID": "mod-2-1-frequency.html#frequency-example",
    "href": "mod-2-1-frequency.html#frequency-example",
    "title": "11  Frequency Distributions",
    "section": "11.2 Frequency Example",
    "text": "11.2 Frequency Example\nIn the example below, we have seven observations (students).\nThe students can be grouped by their graduating year. There are 3 students that graduate in 2028 and 4 students that graduate in 2029.\nThe number of students for each group is the frequency (count) associated with the group, that is,\n\nGraduating Year 2028: 3\nGraduating Year 2029: 4\n\n\nTable of Observations\n\n\nStudent\nGraduating Year\nGrade Point Average\n\n\n\n\n1\n2028\n3.0\n\n\n2\n2028\n2.8\n\n\n3\n2028\n3.5\n\n\n4\n2029\n2.7\n\n\n5\n2029\n2.8\n\n\n6\n2029\n3.0\n\n\n7\n2029\n3.6",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Frequency Distributions</span>"
    ]
  },
  {
    "objectID": "mod-2-1-frequency.html#mtcars-frequency",
    "href": "mod-2-1-frequency.html#mtcars-frequency",
    "title": "11  Frequency Distributions",
    "section": "11.3 Mtcars Frequency",
    "text": "11.3 Mtcars Frequency\nFor the following example, we use the mtcars data frame.\nFirst, we group the data by the cylinder variable. The group_by function notifies R that the data are organized by a specific variable or variables.\nHere, we notify R that the cylinder variable is how the data are grouped together, that is, R should consider all the observations with 4 cylinders one group, all the observations with 6 cylinders one group, and all the observations with 8 cylinders one group.\nSecond, we create a new variable freq_cyl using the summarize function. The variable freq_cyl is equal to the number of observations in each group.\nWe then use the arrange function to sort the resulting frequencies from lowest number to the highest number.\nWe create a frequency table using the kable function.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Frequency Distributions</span>"
    ]
  },
  {
    "objectID": "mod-2-1-frequency.html#census-frequency",
    "href": "mod-2-1-frequency.html#census-frequency",
    "title": "11  Frequency Distributions",
    "section": "11.4 Census Frequency",
    "text": "11.4 Census Frequency\nIn the example below, we use the censusapi package to obtain data from the U.S. Census Bureau’s American Community Survey.\nWe use the API to download data on counties and county-equivalent geographies for the United States. We specify two variables, the name of each geography and the population of each geography.\nWe rename the census population variable, B01001_001E, to total_pop.\nWe want to create categories for different ranges of population. To do this, we use the case_when function.\nHaving created the population categories, we group_by the population category variable and then use the summarize function to obtain a count of the number of observations in each category.\n\n# Clear Memory \nrm(list = ls())\n\n# Load Libraries\n\nlibrary(censusapi)\nlibrary(dplyr)\nlibrary(kableExtra)\nlibrary(tidyverse)\n\n#B01001_001E is total population\n#Case When to Create Categories\n\ncounty_pop &lt;- getCensus(name = \"acs/acs5\",\n            vintage = 2023,\n            key = \"9c1637a56ff93f0af6b4b1d0547ea048fe668175\",\n            vars = c(\"NAME\",\n                     \"B01001_001E\"),\n            region = \"county:*\") %&gt;%\nrename(total_pop = B01001_001E) %&gt;%\nmutate(pop_cat = case_when(total_pop &gt; 99999 ~ \"Greater than 99,999\",\n                           total_pop &gt; 49999 ~ \"50,000 - 99,999\",\n                           total_pop &gt; 24999 ~ \"25,000 - 49,999\",\n                           total_pop &gt; 9999  ~ \"10,000 - 24,999\",\n                           TRUE ~ \"0 - 9,999\"))\n\n# Summarize to Create Frequencies by Categories\n\ncounty_freq &lt;- county_pop %&gt;%\n  group_by(pop_cat) %&gt;%\n  summarize(freq_pop = n()) %&gt;%\n  arrange(pop_cat)\n\n# Use frequency data frame to produce table\n\nkable(county_freq,\n      col.names = c(\"Category\", \"Frequency\"),\n      align = c('c','c'),\n      caption = 'Frequency of County Population, 2023') %&gt;%\nkable_styling(font_size = 14)\n\n\nFrequency of County Population, 2023\n\n\nCategory\nFrequency\n\n\n\n\n0 - 9,999\n743\n\n\n10,000 - 24,999\n823\n\n\n25,000 - 49,999\n643\n\n\n50,000 - 99,999\n397\n\n\nGreater than 99,999\n616",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Frequency Distributions</span>"
    ]
  },
  {
    "objectID": "mod-2-1-frequency.html#frequency-distributions-1",
    "href": "mod-2-1-frequency.html#frequency-distributions-1",
    "title": "11  Frequency Distributions",
    "section": "11.5 Frequency Distributions",
    "text": "11.5 Frequency Distributions\nWe can graphically depict the frequency distribution.\nWe use the ggplot package to create a graphical plot of the frequency distribution.\nSelect the link to learn more about the capabilities of ggplot.\nWe have categories (population size) on the x-axis and the frequencies on the y-axis.\nWe use the geom_col command to create a column chart of the frequencies. We choose to fill the columns with the darkblue color.\nThe labs command specifies the labels for the plot, to include the x-axis, y-axis, title, and subtitle.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Frequency Distributions</span>"
    ]
  },
  {
    "objectID": "mod-2-1-frequency.html#relative-frequency-distributions",
    "href": "mod-2-1-frequency.html#relative-frequency-distributions",
    "title": "11  Frequency Distributions",
    "section": "11.6 Relative Frequency Distributions",
    "text": "11.6 Relative Frequency Distributions\nThe frequency distribution may not be as informative as we would like if the number of observations is sufficiently large.\nIn the frequency distribution for the population of U.S. counties, there were more than 600 counties with populations between 10,000 and 24,999 individuals. How does this compare with the number of counties with populations less than 10,000?\nA relative frequency distribution contains information on the proportion of observations in each class relative to the total number of observations.\nLet \\(n\\) be the total number of observations.\nLet \\(i\\) represent the number of classes or categories.\nLet \\(n_i\\) be the total number of observations in the i-th class.\n\\[\\text{Relative Frequency of Class}_i = \\frac{n_i}{n}\\]",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Frequency Distributions</span>"
    ]
  },
  {
    "objectID": "mod-2-1-frequency.html#relative-frequency-mtcars",
    "href": "mod-2-1-frequency.html#relative-frequency-mtcars",
    "title": "11  Frequency Distributions",
    "section": "11.7 Relative Frequency: Mtcars",
    "text": "11.7 Relative Frequency: Mtcars\nWe use the mtcars example data.\nWe group the data by the cylinder variable.\nWe estimate the frequency by cylinder.\nWe estimate the relative frequency using the mutate function.\nWe create a frequency table using the kable function.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Frequency Distributions</span>"
    ]
  },
  {
    "objectID": "mod-2-1-frequency.html#relative-frequency-census",
    "href": "mod-2-1-frequency.html#relative-frequency-census",
    "title": "11  Frequency Distributions",
    "section": "11.8 Relative Frequency: Census",
    "text": "11.8 Relative Frequency: Census\nWe can also graphically depict the frequency distribution.\nWe return to our previous example using data from the U.S. Census Bureau. We obtain data on population for counties and county-equivalent geographies in the United States.\nWe use case_when to create a categorical variable that groups geographies into population bins.\nAs before, we use summarize to count the number of observations in each bin.\nWe then estimate the relative frequency by diving the number of observations in each bin by the total number of observations or\n\\[\\text{Relative Frequency of Population Bin}_i = \\frac{\\text{Observations in Bin}_i}{\\text{Total Observations}}\\]\nIn other words, the relative frequency of the 0 to 9,999 population bin is equal to the number of counties in that bin divided by the total number of counties across all bins.\nWe then ggplot to create a graphical plot of the frequency distribution.\nWe have categories (population size) on the x-axis and the frequencies on the y-axis.\n\nrm(list = ls())\n\nlibrary(censusapi)\nlibrary(dplyr)\nlibrary(kableExtra)\nlibrary(tidyverse)\n\n#B01001_001E is total population\n#Case When to Create Categories\n\ncounty_pop &lt;- getCensus(name = \"acs/acs5\",\n            vintage = 2023,\n            key = \"9c1637a56ff93f0af6b4b1d0547ea048fe668175\",\n            vars = c(\"NAME\",\n                     \"B01001_001E\"),\n            region = \"county:*\") %&gt;%\nrename(total_pop = B01001_001E) %&gt;%\nmutate(pop_cat = case_when(total_pop &gt; 99999 ~ \"Greater than 99,999\",\n                           total_pop &gt; 49999 ~ \"50,000 - 99,999\",\n                           total_pop &gt; 24999 ~ \"25,000 - 49,999\",\n                           total_pop &gt; 9999  ~ \"10,000 - 24,999\",\n                           TRUE ~ \"0 - 9,999\"))\n\ncounty_freq &lt;- county_pop %&gt;%\n  group_by(pop_cat) %&gt;%\n  summarize(freq_pop = n()) %&gt;%\n  mutate(rel_freq = freq_pop/sum(freq_pop)) %&gt;%\n  arrange(pop_cat)\n\nggplot(data = county_freq,\n       aes(x = pop_cat,\n           y = rel_freq,\n           fill = rel_freq)) +\ngeom_col() +\ntheme_minimal() +\ntheme(legend.position=\"none\") +\nlabs(title = \"Relative Frequency of Population Bins\",\n     x     = \"Population Bin\",\n     y     = \"Relative Frequency\")",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Frequency Distributions</span>"
    ]
  },
  {
    "objectID": "mod-2-1-frequency.html#scatterplots",
    "href": "mod-2-1-frequency.html#scatterplots",
    "title": "11  Frequency Distributions",
    "section": "11.9 Scatterplots",
    "text": "11.9 Scatterplots\nA scatterplot is a graphical display of a relationship between two variables of interest.\nBefore constructing a scatterplot, one must think about the nature of the relationship between the two variables.\nThe independent variable is typically represented on the horizontal axis while the dependent variable is typically represented on the vertical axis.\nThe hypothesized relationship between the \\(x\\) and \\(y\\) variables can be expressed as:\n\\[y = f(x)\\]\nCare must be taken to avoid interpreting the visual relationship.\nIn other words, what if the true relationship is:\n\\[y = f(x, w, z)\\]\nIf you plot \\(y\\) and \\(x\\) and interpret the visual relationship, you are ignoring the potential relationships between \\(y\\) and \\(w\\) and \\(y\\) and \\(z\\).\nScatterplots may inform, but we should always be careful with any conclusions we derive from merely observing the pattern of two variables.\nIn the code below, we plot weight in pounds on the x-axis and miles per gallon on the y-axis. It would seem that there is a negative relationship between the weight of cars and their fuel efficiency. Heavier cars appear to have lower fuel efficiency.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Frequency Distributions</span>"
    ]
  },
  {
    "objectID": "mod-2-2-central.html",
    "href": "mod-2-2-central.html",
    "title": "12  Central Tendency",
    "section": "",
    "text": "12.1 Data Properties\nThe properties of a set of data can be described (in general) by:\nThe center of the data refers to the measure of central tendency of the data, that is, what is an estimate of the “middle” of the data.\nThe spread of the data refers to how “far apart” observations are in the data relative to center of the data.\nThere are two typical measures of the shape of the data:\nThe measures of the center, spread, and shape of the data help us understand the properties of the data.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Central Tendency</span>"
    ]
  },
  {
    "objectID": "mod-2-2-central.html#data-properties",
    "href": "mod-2-2-central.html#data-properties",
    "title": "12  Central Tendency",
    "section": "",
    "text": "The center of the data.\nThe spread of the data.\nThe shape of the data\n\n\n\n\n\nAre the data symmetrically or asymmetrically distributed around the mean?\nAre the data concentrated in one tail of the distribution?",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Central Tendency</span>"
    ]
  },
  {
    "objectID": "mod-2-2-central.html#population-mean-and-sample-average",
    "href": "mod-2-2-central.html#population-mean-and-sample-average",
    "title": "12  Central Tendency",
    "section": "12.2 Population Mean and Sample Average",
    "text": "12.2 Population Mean and Sample Average\nRecall that population data is comprehensive information collected on all members of a group or entity under study. The population is the universe of members of the group.\nRecall that sample data is a subset of the population data. Samples may range from 1 member of the population of interest to many members of a population of interest.\nLet \\(N\\) be the number of members of a specified group.\n\\(N\\) defines the size of the population of the group of interest.\nLet \\(n = N - j\\) where \\(0 \\le j \\le N\\).\n\\(n\\) is the size of a sample of the population size \\(N\\).\nGiven a population size \\(N\\), the population mean, \\(\\mu\\), is defined as:\n\\[\\mu = \\frac{1}{N} \\sum_{i = 1}^N x_i\\]\nFor a sample size \\(n\\) from a population size \\(N\\), then the sample average \\(\\bar{x}\\) is:\n\\[\\bar{x} = \\frac{1}{n} \\sum_{i = 1}^n x_i\\] Here we must be careful in our use of language. When we say mean we refer to the population mean, that is, the mean obtained using all the members of the population. When we say sample, we refer to the sample average, that is, the average obtained using a subset of the members of the population.\nIn the example below, we can use the summarize function to determine number of observations, the minimum value for mpg, the maximum value for mpg, and the sample average for mpg.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Central Tendency</span>"
    ]
  },
  {
    "objectID": "mod-2-2-central.html#mean-and-median",
    "href": "mod-2-2-central.html#mean-and-median",
    "title": "12  Central Tendency",
    "section": "12.3 Mean and Median",
    "text": "12.3 Mean and Median\nThe population mean, \\(\\mu\\), is one measure of the center of the data.\nThe population median is another measure of the center of the data.\nThe median is a value that separates the observations in half.\nFor a given median, 50% of the observations are below the median value and 50% of the observations are above the median value.Depending on the spread and shape of the data, the mean and median may be relatively close or far apart.\nOutlying observations can “pull” the mean far from the “center” of the data.\nIn the example below, we use the censusapi package to access the U.S. Census API to obtain data on total population for county and county-equivalent geographies.\nNote the difference between the mtcars example and the census example: the mtcars example is a sample while the census example uses population data.\nWe then estimate the following descriptive statistics for the population of geographies:\n\nNumber of observations\nMinimum value for population\nMaximum value for population\nMedian value for population\nMean value for population\n\n\nrm(list = ls())\n\nlibrary(censusapi)\nlibrary(dplyr)\nlibrary(kableExtra)\nlibrary(tidyverse)\n\ncounty_pop &lt;- getCensus(name = \"acs/acs5\",\n            vintage = 2023,\n            key = \"9c1637a56ff93f0af6b4b1d0547ea048fe668175\",\n            vars = c(\"NAME\",\n                     \"B01001_001E\"),\n            region = \"county:*\") %&gt;%\nrename(pop = B01001_001E) %&gt;%\nsummarize(n_pop    = n(),\n          min_pop  = min(pop),\n          max_pop  = max(pop),\n          med_pop  = median(pop),\n          mean_pop = mean(pop))\n\nkable(county_pop,\n      col.names = c(\"N\", \"Minimum\", \"Maximum\",\n                    \"Median\", \"Mean\"),\n      align     = 'c',\n      format.args = list(big.mark = \",\"),\n      caption = 'Descriptive Statistics of U.S. Counties - Population') %&gt;%\nkable_styling(font_size = 12)\n\n\nDescriptive Statistics of U.S. Counties - Population\n\n\nN\nMinimum\nMaximum\nMedian\nMean\n\n\n\n\n3,222\n43\n9,848,406\n25,967\n104,172.1",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Central Tendency</span>"
    ]
  },
  {
    "objectID": "mod-2-2-central.html#mean-deviations",
    "href": "mod-2-2-central.html#mean-deviations",
    "title": "12  Central Tendency",
    "section": "12.4 Mean Deviations",
    "text": "12.4 Mean Deviations\nWe have estimated the population mean (\\(\\mu\\)) and the sample average (\\(\\bar{x}\\)).\nWe can now construct an estimate of how much each observation deviates from the population mean or sample average.\nGiven the i-th observation for a variable \\(x\\), that is, \\(x_i\\), the mean deviation of \\(x_i\\) is:\n\\[d_i = x_i - \\mu\\] Given the i-th observation for a variable \\(x\\), that is, \\(x_i\\), the average deviation of \\(x_i\\) for a sample is:\n\\[d_i = x_i - \\bar{x}\\]\nThe sum of the mean deviations is zero for the population or approximately zero for a sample.\n\\[\\sum_{i = 1}^N d_i = \\sum_{i = 1}^N (x_i - \\mu) = 0\\]\n\\[\\sum_{i = 1}^n d_i = \\sum_{i = 1}^n (x_i - \\bar{x}) \\approx 0\\]",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Central Tendency</span>"
    ]
  },
  {
    "objectID": "mod-2-2-central.html#mean-deviations-example",
    "href": "mod-2-2-central.html#mean-deviations-example",
    "title": "12  Central Tendency",
    "section": "12.5 Mean Deviations Example",
    "text": "12.5 Mean Deviations Example\nIn the example below, we estimate the sample average deviation for the mtcars example. We select the miles per gallon variable, estimate the sample average, and then estimate the average deviation for every observation in the sample.\nWe then output the first five observations to illustrate the estimation of average deviations.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIn this example, we select two variables: miles per gallon and weight.\nWe estimate the sample average for both variables. We then sum the average deviations for each variable.\nThe table illustrates, that for the sample, the sum of the average deviations is approximately zero for each of the variables.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Central Tendency</span>"
    ]
  },
  {
    "objectID": "mod-2-2-central.html#trimmed-mean",
    "href": "mod-2-2-central.html#trimmed-mean",
    "title": "12  Central Tendency",
    "section": "12.6 Trimmed Mean",
    "text": "12.6 Trimmed Mean\nThe U.S. Census population data illustrates how an outlying observation (Los Angeles County) can ‘pull’ the population mean away from the “center” of the data.\nWe can trim the data to account for the presence of outliers.\nTrimming the data is a transformation of the data\nIn most cases, 5% to 25% of the data from the ends of the distribution are discarded to estimate the trimmed mean.\nThe median is a fully truncated mean, that is, the median represents the discard of 50% of the observations from each end of the distribution.\nThe trimmed mean is less sensitive to outliers and will provide a “reasonable” estimate of the population mean in many cases.\nRemoving the highest and lowest scores in a competition to obtained a trimmed mean makes the resulting score more robust to outliers.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Central Tendency</span>"
    ]
  },
  {
    "objectID": "mod-2-2-central.html#trimmed-mean-example",
    "href": "mod-2-2-central.html#trimmed-mean-example",
    "title": "12  Central Tendency",
    "section": "12.7 Trimmed Mean Example",
    "text": "12.7 Trimmed Mean Example\nWe estimate the number of observations, minimum and maximum values, and mean and median for population for all U.S. counties using the data from the U.S. Census.\nWe note that several large counties “pull” the mean population away from the median population.\nWe can estimate the trimmed mean using the mean function with the trim option.\nThe trimmed mean removes a specified percentage of the smallest and largest values from a dataset to reduce the impact of outliers.\nNote how even with a 5% trim, the mean of the remaining observations quickly approaches the population mean.\nNote how the 50% trim results in the population mean.\n\nrm(list = ls())\n\nlibrary(censusapi)\nlibrary(dplyr)\nlibrary(kableExtra)\nlibrary(tidyverse)\n\ncounty_pop &lt;- getCensus(name = \"acs/acs5\",\n            vintage = 2023,\n            key = \"9c1637a56ff93f0af6b4b1d0547ea048fe668175\",\n            vars = c(\"NAME\",\n                     \"B01001_001E\"),\n            region = \"county:*\") %&gt;%\nrename(pop = B01001_001E) %&gt;%\nsummarize(med_pop  = median(pop),\n          mean_pop = mean(pop),\n          mean_05  = mean(pop, trim = 0.05),\n          mean_10  = mean(pop, trim = 0.10),\n          mean_20  = mean(pop, trim = 0.20),\n          mean_50  = mean(pop, trim = 0.50))\n\nkable(county_pop,\n      col.names = c('Median', 'Mean',\n                    'Trim 0.05', 'Trim 0.10',\n                    'Trim 0.20', 'Trim 0.50'),\n      align     = 'c',\n      digits    = 2,\n      caption = 'Mean and Trimmed Means, County Population') %&gt;%\nkable_styling(font_size = 12)\n\n\nMean and Trimmed Means, County Population\n\n\nMedian\nMean\nTrim 0.05\nTrim 0.10\nTrim 0.20\nTrim 0.50\n\n\n\n\n25967\n104172.1\n55627.37\n42982.97\n32445.04\n25967",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Central Tendency</span>"
    ]
  },
  {
    "objectID": "mod-2-2-central.html#weighted-mean",
    "href": "mod-2-2-central.html#weighted-mean",
    "title": "12  Central Tendency",
    "section": "12.8 Weighted Mean",
    "text": "12.8 Weighted Mean\nGiven \\(N\\) observations of \\(x\\) and weights \\(w_i\\), the weighted population mean, \\(\\mu_w\\), is\n\\[\\mu_w = \\sum_{i = 1}^N w_i \\times x_i\\]\nFor the population mean, \\(\\mu\\), the weights are equal to \\(1/N\\) or\n\\[\\mu = \\frac{1}{N} \\sum_{i = 1}^N = \\frac{1}{N}x_1 + \\frac{1}{N}x_2 + \\dots + \\frac{1}{N}x_N\\]\nGiven \\(n\\) observations of \\(x\\), the weighted sample average \\(\\bar{x}_w\\), is defined as:\n\\[\\bar{x}_w = \\sum_{i = 1}^n w_i \\times x_i\\]\nFor the sample average, \\(\\bar{x}\\), the weights are equal to \\(1/n\\) or\n\\[\\bar{x} = \\frac{1}{n} \\sum_{i = 1}^n = \\frac{1}{n}x_1 + \\frac{1}{n}x_2 + \\dots + \\frac{1}{n}x_n\\]",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Central Tendency</span>"
    ]
  },
  {
    "objectID": "mod-2-2-central.html#weighted-mean-example",
    "href": "mod-2-2-central.html#weighted-mean-example",
    "title": "12  Central Tendency",
    "section": "12.9 Weighted Mean Example",
    "text": "12.9 Weighted Mean Example\nLet’s create a data frame with three variables\nLet x be a vector of squares\nLet wt_1 be a vector of equal weights.\nLet wt_2 be a vector of unequal weights\nWe estimate the mean using the mean function and the weighted mean using the weighted.mean function\nAs expected, using equal weights returns the same value as the mean function.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Central Tendency</span>"
    ]
  },
  {
    "objectID": "mod-2-2-central.html#quantiles-and-percentiles",
    "href": "mod-2-2-central.html#quantiles-and-percentiles",
    "title": "12  Central Tendency",
    "section": "12.10 Quantiles and Percentiles",
    "text": "12.10 Quantiles and Percentiles\nLet the p-th quantile be the value in the data for which \\(100 \\times p\\) of the data are less than the value.\nA percentile is the same concept as the quantile but the range is 0 to 100.\nA quantile refers to the 0, 25, 50, 75, and 100 percentiles.\nA quintile refers to the 0, 20, 40, 60, 80, and 100 percentiles.\nA decile refers to the 0, 10, 20, 30, 40, 50, 60, 70, 80, 90, and 100 percentiles.\nGiven sorted data, the p-th quantile is at position\n\\[1 + p \\times (n+1)\\]\nIf you want to determine the location of the p-th percent,\n\\[L_p = \\frac{p}{100} \\times (n+1)\\]",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Central Tendency</span>"
    ]
  },
  {
    "objectID": "mod-2-2-central.html#quantiles-and-percentiles-example",
    "href": "mod-2-2-central.html#quantiles-and-percentiles-example",
    "title": "12  Central Tendency",
    "section": "12.11 Quantiles and Percentiles Example",
    "text": "12.11 Quantiles and Percentiles Example\n\nClears the environment:\n\nRemoves all objects from memory using rm(list = ls()) to ensure a clean workspace.\n\nLoads the mtcars dataset:\n\nUses the built-in R dataset containing car performance data such as miles per gallon, cylinders, and weight.\n\nCalculates quantiles for selected variables:\nUses reframe() from dplyr to summarize key statistics.\nComputes quantiles at 0%, 25%, 50%, 75%, and 100% using quantile().\n\nThese represent the minimum, first quartile (Q1), median (Q2), third quartile (Q3), and maximum values.\n\nApplies the scales::percent() function:\n\nConverts numeric quantile probabilities (0, 0.25, 0.5, etc.) into formatted percentages (0%, 25%, 50%, 75%, 100%).\n\nCreates a clean summary table:\nUses kable() to display the quantile data in a readable format.\n\nCustomizes column names: Percentile, MPG, Cylinders, and Weight.\nCenters all columns and adds a caption: “Quantiles of MTCARS”.\n\nImproves table appearance:\n\nUses kable_styling(font_size = 12) from kableExtra to make the table more visually appealing.\n\n\n\nrm(list = ls())\n\ncars &lt;- mtcars %&gt;%\n  reframe(quantile   = scales::percent(c(0, 0.25, 0.5, 0.75 ,1)),\n            mpg      = quantile(mpg, c(0, 0.25, 0.5, 0.75, 1)),\n            cyl      = quantile(cyl, c(0, 0.25, 0.5, 0.75, 1)),\n            wt       = quantile(wt, c(0, 0.25, 0.5, 0.75, 1)))\n\nkable(cars,\n      col.names = c('Percentile', \n                    'MPG',\n                    'Cyliners',\n                    'Weight'),\n      align = c('c','c','c','c'),\n      caption = 'Quantiles of MTCARS') %&gt;%\nkable_styling(font_size = 12)\n\n\nQuantiles of MTCARS\n\n\nPercentile\nMPG\nCyliners\nWeight\n\n\n\n\n0%\n10.400\n4\n1.51300\n\n\n25%\n15.425\n4\n2.58125\n\n\n50%\n19.200\n6\n3.32500\n\n\n75%\n22.800\n8\n3.61000\n\n\n100%\n33.900\n8\n5.42400",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Central Tendency</span>"
    ]
  },
  {
    "objectID": "mod-2-3-variance.html",
    "href": "mod-2-3-variance.html",
    "title": "13  Measures of Dispersion",
    "section": "",
    "text": "13.1 Data Properties\nPreviously, we discussed measures of the central tendency of the data.\nWe used population data from the U.S. Census to estimate the population mean, \\(\\mu\\), as well as the median.\nWe used sample data from the mtcars dataset to estimate the sample average, \\(\\bar{x}\\), as well as the sample median.\nWe also used R and RStudio to estimate the number of observations, the minimum value for a variable, and the maximum value for a variable.\nThe measures of central tendency estimate the middle of the data, however, we also noted that outliers can influence the sample average. We discussed how to trim the data to reduce the influence of outliers.\nThe measures of central tendency do not provide us information about the spread or dispersion of the data.\nAre the data “tightly clustered” around the measure of central tendency or are the data “spread out?”\nWe want to know the spread of the data relative to the center of the data.\nTo this end, we will estimate measure of the variance and standard deviation.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Measures of Dispersion</span>"
    ]
  },
  {
    "objectID": "mod-2-3-variance.html#population-variance",
    "href": "mod-2-3-variance.html#population-variance",
    "title": "13  Measures of Dispersion",
    "section": "13.2 Population Variance",
    "text": "13.2 Population Variance\nOne measure of the dispersion (spread) of the data is the range which is equal to the distance between the minimum and maximum observations in the data.\nWe could estimate the average distance of the observations from the mean or:\n\\[\\text{average mean deviation} = \\frac{1}{N} \\sum_{i = 1}^N (x_i - \\mu)\\]\nHowever, if sum of the mean deviations is equal to zero in the population, the average of the mean deviations will also be zero in the population.\nWe note that the sum of the squared mean deviations will not be zero.\nThe population variance, \\(\\sigma^2\\), is equal to the average of the sum of the squared mean deviations:\n\\[\\text{Population Variance} = \\sigma^2 = \\frac{1}{N} \\sum_{i = 1}^N (x_i - \\mu)^2\\]",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Measures of Dispersion</span>"
    ]
  },
  {
    "objectID": "mod-2-3-variance.html#population-standard-deviation",
    "href": "mod-2-3-variance.html#population-standard-deviation",
    "title": "13  Measures of Dispersion",
    "section": "13.3 Population Standard Deviation",
    "text": "13.3 Population Standard Deviation\nAs noted previously, the population variance, \\(\\sigma^2\\), is the average of the squared mean deviations:\n\\[\\sigma^2 = \\frac{1}{N} \\sum_{i = 1}^N (x_i - \\mu)^2\\]\nThe population variance, \\(\\sigma^2\\), may difficult to interpret as \\(\\sigma^2\\) is measured in the squared units of the variable of interest.\nIn other words, if we were measuring the variance of total population, the variance would be in individuals-squared units. If we were measuring the variance of income, the variance would be in income-squared units.\nThe population standard deviation, \\(\\sigma\\), is the positive square root of the population standard deviation.\n\\[\\sigma = \\sqrt{\\frac{1}{N} \\sum_{i = 1}^N (x_i - \\mu)^2}\\]The standard deviation is measured in the same units as the variable of interest.\nLastly, we note that if we have two variables \\(x\\) and \\(y\\), then \\(x\\) is less dispersed than \\(y\\) if \\(\\sigma_x &lt; \\sigma_y\\).",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Measures of Dispersion</span>"
    ]
  },
  {
    "objectID": "mod-2-3-variance.html#variance-example",
    "href": "mod-2-3-variance.html#variance-example",
    "title": "13  Measures of Dispersion",
    "section": "13.4 Variance Example",
    "text": "13.4 Variance Example\nIn the example below, we create a tibble of simulated population data.\nThere are 1,000,000 observations drawn from a random normal distribution with a population mean of 50.5 and a standard deviation of 5.\nIn other words, we are creating a population with a known mean and known standard deviation.\nSince the variance is the square of the standard deviation, we know the variance also.\nFor a random variable, \\(X\\) that is distributed normally with mean \\(\\mu\\) and variance \\(\\sigma^2\\), we can express this distribution of \\(X\\) as:\n\\[X \\sim N(\\mu, \\sigma^2 )\\]\nWe use the summarize function for this example.\nWe estimate the population variance manually and using the var function.\nWe then estimate the population standard deviation manually and using the sd function.\nWe do this to illustrate the manual estimates and estimates using the functions are the same.\nAn additional note that each time you run the code, a new population is generated, and the descriptive statistics may change.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Measures of Dispersion</span>"
    ]
  },
  {
    "objectID": "mod-2-3-variance.html#census-example",
    "href": "mod-2-3-variance.html#census-example",
    "title": "13  Measures of Dispersion",
    "section": "13.5 Census Example",
    "text": "13.5 Census Example\nIn the code chunk below, we replicate the example above but with actual data from the U.S. Census for the total population of counties and county-equivalent geographies in the United States.\nNote the difference between the mean and median population estimates. The relatively large counties influence the population mean and ‘pull’ it away from the population median.\n\nrm(list = ls())\n\nlibrary(censusapi)\nlibrary(dplyr)\nlibrary(kableExtra)\nlibrary(tidyverse)\n\ncounty_pop &lt;- getCensus(name = \"acs/acs5\",\n            vintage = 2023,\n            key     = \"9c1637a56ff93f0af6b4b1d0547ea048fe668175\",\n            vars    = c(\"NAME\",\n                        \"B01001_001E\"),\n            region  = \"county:*\") %&gt;%\nrename(pop = B01001_001E) %&gt;%\nsummarize(med_pop  = median(pop),\n          mean_pop = mean(pop),\n          var_pop  = var(pop),\n          sd_pop   = sd(pop))\n\nkable(county_pop,\n      col.names = c('Median', 'Mean',\n                    'Variance', 'Std. Dev'),\n      align     = 'c',\n      digits    = 2,\n      caption = 'Mean, Variance, and Standard Deviation, County Population') %&gt;%\nkable_styling(font_size = 12)\n\n\nMean, Variance, and Standard Deviation, County Population\n\n\nMedian\nMean\nVariance\nStd. Dev\n\n\n\n\n25967\n104172.1\n108681584571\n329668.9",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Measures of Dispersion</span>"
    ]
  },
  {
    "objectID": "mod-2-3-variance.html#sample-variance",
    "href": "mod-2-3-variance.html#sample-variance",
    "title": "13  Measures of Dispersion",
    "section": "13.6 Sample Variance",
    "text": "13.6 Sample Variance\nThe sample variance, \\(s^2\\), is estimated by:\n\\[s^2 = \\frac{1}{n-1} \\sum_{i = 1}^n (x_i - \\bar{x})^2\\]\nOne might ask, “why do we divide by \\(n-1\\) instead of \\(n\\)?”\nThe population mean \\(\\mu\\) is a known, definite mean. It does not require estimation.\nThe sample average, \\(\\bar{x}\\) is an estimate of the population mean \\(\\mu\\).\nIf \\(\\mu\\) is known, then we can reconstitute the data with \\(n-1\\) values.\nWith 100 observations and \\(\\mu = 50\\), then the \\(n-1\\) values can take on any value, but the n-th value is constrained so \\(\\mu = 50\\).\nThe loss of a degree of freedom requires the use \\(n-1\\) in the sample variance instead of \\(n\\).",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Measures of Dispersion</span>"
    ]
  },
  {
    "objectID": "mod-2-3-variance.html#sample-standard-deviation",
    "href": "mod-2-3-variance.html#sample-standard-deviation",
    "title": "13  Measures of Dispersion",
    "section": "13.7 Sample Standard Deviation",
    "text": "13.7 Sample Standard Deviation\nThe sample variance, \\(s^2\\), is estimated by:\n\\[s^2 = \\frac{1}{n-1} \\sum_{i = 1}^n (x_i - \\bar{x})^2\\]\nThe sample standard deviation, \\(s\\), is estimated by:\n\\[s = \\sqrt{\\frac{1}{n-1} \\sum_{i = 1}^n (x_i - \\bar{x})^2}\\] R (as with most programs) calculates the sample variance and sample standard deviation.\nAs \\(n\\) increases, the impact of dividing by \\(n-1\\) rather than \\(N\\) declines.\nIf you are worried about \\(n-1\\), then you are likely to have other issues (sample size) of higher concern.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Measures of Dispersion</span>"
    ]
  },
  {
    "objectID": "mod-2-3-variance.html#sample-example",
    "href": "mod-2-3-variance.html#sample-example",
    "title": "13  Measures of Dispersion",
    "section": "13.8 Sample Example",
    "text": "13.8 Sample Example\nWe simulate 100,000 observations from a normal distribution with \\(\\mu = 100\\) and \\(\\sigma = 8.5\\).\nWe then take a random sample of 1,000 observations from the population and estimate the variance manually and with the var function in summarize\nThe slice_sample function is used to randomly select rows from a data frame.\nThe manually calculated sample variance using \\((n-1)\\) is equal to the variance obtained from the var function in R illustrating that R generates its variance estimate using the sample variance formula.\nAs \\(\\sigma\\) is the square root of \\(\\sigma^2\\), the same discussion applies.\nIt is interesting to note that a new set of values for \\(X\\) is generated each time the code is run, changing the resulting sample descriptive statistics.\nTry changing the sample size, that is, changing from n = 1000 to n = 50, and observe what happens to the sample statistics for variance and standard deviation across several runs.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Measures of Dispersion</span>"
    ]
  },
  {
    "objectID": "mod-2-3-variance.html#coefficient-of-variation",
    "href": "mod-2-3-variance.html#coefficient-of-variation",
    "title": "13  Measures of Dispersion",
    "section": "13.9 Coefficient of Variation",
    "text": "13.9 Coefficient of Variation\nThe coefficient of variation is a measure of how “large” the standard deviation is relative to the mean.\nThe coefficient of variation for the population is:\n\\[\\text{coefficient of variation} = \\frac{\\sigma}{\\mu}\\] The coefficient of variation for a sample is:\n\\[\\text{coefficient of variation} = \\frac{s}{\\bar{x}}\\]\nThe higher the coefficient of variation, the greater the level of dispersion of observations around the mean.\nIn finance, the coefficient of variation estimates how much volatility, or risk, is assumed in comparison to the amount of return expected from investments.\nThe lower the coefficient of variation, the better risk-return trade-off.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Measures of Dispersion</span>"
    ]
  },
  {
    "objectID": "mod-2-3-variance.html#tidyquant-example",
    "href": "mod-2-3-variance.html#tidyquant-example",
    "title": "13  Measures of Dispersion",
    "section": "13.10 Tidyquant Example",
    "text": "13.10 Tidyquant Example\nWe use the tidyquant package to obtain stock price data for Apple, Ford, General Motors, Gamestop, IBM, and Netflix.\nWe then estimate the coefficient of variation for each of these corporations.\nFrom January 2020 to July 2025, IBM had the lowest coefficient of variation, that is, it was the least volatile relative to its mean.\nOver the same period, Netflix and Gamestop has the highest coefficients of variation, that is, these stocks were relatively volatile when compared to IBM or Ford.\n\nrm(list = ls())\n\nlibrary(censusapi)\nlibrary(dplyr)\nlibrary(kableExtra)\nlibrary(tidyverse)\nlibrary(tidyquant)\n\ntickers &lt;- c('AAPL', 'F',\n             'GM', 'GME',\n             'IBM', 'NFLX')\n\nprices &lt;- tq_get(tickers, \n                  from = '2010-01-01',\n                  to   = \"2025-07-31\",\n                  get  = \"stock.prices\")\n\n\ncoef_var &lt;- prices %&gt;%\n  group_by(symbol) %&gt;%\n  summarize(cv = sd(close)/mean(close)) %&gt;%\n  arrange(cv)\n\nkable(coef_var,\n      col.names = c('Company',\n                    'Coefficient of Variation'),\n      align     = c('c','c'),\n      caption   = 'Coefficient of Variation') %&gt;%\nkable_styling(font_size = 12)\n\n\nCoefficient of Variation\n\n\nCompany\nCoefficient of Variation\n\n\n\n\nIBM\n0.2096402\n\n\nF\n0.2307120\n\n\nGM\n0.2413333\n\n\nAAPL\n0.9291908\n\n\nGME\n0.9739563\n\n\nNFLX\n0.9909600",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Measures of Dispersion</span>"
    ]
  },
  {
    "objectID": "mod-2-4-zscores.html",
    "href": "mod-2-4-zscores.html",
    "title": "14  Z-Scores",
    "section": "",
    "text": "14.1 Z-Scores\nFor \\(i\\) observations in a population, the population mean deviations are equal to:\n\\[d_i = x_i - \\mu\\]\nFor \\(i\\) observations in a sample, the sample mean deviations are equal to:\n\\[d_i = x_i - \\bar{x}\\] The mean deviations frame our discussion of the data in terms of the center of the data.\nWe lack information, however, on whether the mean deviation is “large” or “small” relative to the dispersion of the data.\nThe z-score or standard score is a measure of the mean deviation of an observation relative to the standard deviation of the variable.\nIn the population, the z-score for an observation is:\n\\[Z = \\frac{x_i - \\mu}{\\sigma}\\]\nIn the sample, the Z-score for an observation is:\n\\[Z = \\frac{x_i - \\bar{x}}{s}\\]\nThe Z-score can be placed on a normal distribution curve and provides a measure in standard deviation units.\nIf \\(Z_{x1} = 2\\) then the observation \\(x_1\\) is two standard deviations to the right of \\(\\mu\\).\nIf \\(Z_{x2} = -0.75\\), then the observation \\(x_2\\) is 0.75 observations to the left of \\(\\mu\\).\nZ-scores allow you to compare results to a normally distributed population. If you know, for example, that someone’s weight is 175 pounds, you might want to compare it to the average persons weight. The z-score compares the difference of the individual’s weight with the average person’s weight and adjusts it for the dispersion of the data.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Z-Scores</span>"
    ]
  },
  {
    "objectID": "mod-2-4-zscores.html#calculating-z-scores",
    "href": "mod-2-4-zscores.html#calculating-z-scores",
    "title": "14  Z-Scores",
    "section": "14.2 Calculating Z-scores",
    "text": "14.2 Calculating Z-scores\nAssume that you have a standardized test where the mean is \\(\\mu = 150\\) and the standard deviation is \\(\\sigma = 25\\).\nIf we assume a normal distribution and an individual scores 190.\n\\[Z_{190} = \\frac{x_i - \\mu}{\\sigma} = \\frac{190 - 150}{25} = 1.6\\] A Z-score of 1.6 means that the individual’s score of 190 is 1.6 standard deviations from the population mean. Given the value of the Z-score is positive, this means the score of 190 lies 1.6 standard deviations to the right of the population mean.\nWhat if an individual scored 100?\n\\[Z_{100} = \\frac{x_i - \\mu}{\\sigma} = \\frac{100 - 150}{25} = -2\\] A Z-score of -2 means that the individual’s score is 2 standard deviations to the left of the population mean. The negative value of the Z-score means that the individual scored less than the population mean.\nLater in the class we will use a Z-table and then R to determine the probabilities associated with Z-scores.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Z-Scores</span>"
    ]
  },
  {
    "objectID": "mod-2-4-zscores.html#z-scores-in-r",
    "href": "mod-2-4-zscores.html#z-scores-in-r",
    "title": "14  Z-Scores",
    "section": "14.3 Z-Scores in R",
    "text": "14.3 Z-Scores in R\nIn the following example, we generate a simulated population of 10,000 observations with \\(\\mu = -25\\) and \\(\\sigma = 2.9\\).\nWe then generate the population mean and population standard deviation for the simulated population.\nThe next step is estimating the z-score of each observation.\nWe then output the first five observations to the table. Again, you may find it useful to note how the data in the table changes each time your run the code.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Z-Scores</span>"
    ]
  },
  {
    "objectID": "mod-2-4-zscores.html#module-summary",
    "href": "mod-2-4-zscores.html#module-summary",
    "title": "14  Z-Scores",
    "section": "14.4 Module Summary",
    "text": "14.4 Module Summary\nWe use descriptive statistics to develop an understanding of the properties of the data.\nWe have discussed two broad properties of the data:\n\nThe center of the data\nThe spread of the data\n\nWe will work on the shape of the data in coming modules.\nNow, having worked with measures of the center and spread, we can turn our attention to the question of co-movement.\nDo two variables “move” together in some systematic fashion?\nWhat measures might suggest a potential relationship between two variables of interest?\nIn our next module, we will focus on covariance and correlation.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Z-Scores</span>"
    ]
  },
  {
    "objectID": "mod-3-0-overview.html",
    "href": "mod-3-0-overview.html",
    "title": "15  Module 3 Overview",
    "section": "",
    "text": "15.1 Introduction\nWelcome to Module 3 of ECON 700.\nThis module introduces students to bivariate data analysis through the concepts of covariance and correlation. Building on prior discussions of univariate statistics, it explores how two variables move together and the extent to which their relationship can be quantified. Covariance is presented as a measure of joint variability that indicates whether two variables tend to increase or decrease together. Students learn that while the sign of covariance conveys direction (positive or negative association), its magnitude is difficult to interpret due to dependence on the units of measurement.\nTo overcome this limitation, the module introduces the Pearson correlation coefficient—a standardized, unitless measure that captures the strength and direction of linear relationships between two variables. Through practical examples using R and real-world data such as mtcars and World Bank indicators, students compute, interpret, and compare covariance and correlation values. The module concludes by emphasizing that correlation measures association, not causation, and that careful interpretation is required when evaluating statistical relationships.",
    "crumbs": [
      "Covariance and Correlation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Module 3 Overview</span>"
    ]
  },
  {
    "objectID": "mod-3-0-overview.html#learning-objectives",
    "href": "mod-3-0-overview.html#learning-objectives",
    "title": "15  Module 3 Overview",
    "section": "15.2 Learning Objectives",
    "text": "15.2 Learning Objectives\nBy the end of this module, you should be able to:\n\nMLO 1: Define covariance and correlation as measures of association. (CLO 3)\nMLO 2: Compute sample and population covariance and correlation using R. (CLO 2, 3)\nMLO 3: Interpret the sign and magnitude of covariance and correlation coefficients. (CLO 1, 2, 3)\nMLO 4: Explain how changes in units of measurement affect covariance but not correlation. (CLO 2,3)\nMLO 5: Distinguish between statistical association and causal relationships in bivariate data. (CLO 1, 2, 3, 4)\nMLO 6: Apply covariance and correlation measures using R. (CLO 5)",
    "crumbs": [
      "Covariance and Correlation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Module 3 Overview</span>"
    ]
  },
  {
    "objectID": "mod-3-0-overview.html#required-texts",
    "href": "mod-3-0-overview.html#required-texts",
    "title": "15  Module 3 Overview",
    "section": "15.3 Required Texts",
    "text": "15.3 Required Texts\nThe following textbooks are available for free. Please select the links provided to access.\n\nBarbara Illowsky and Susan Dean. Introductory Statistics 2e. OpenStax. Chapter 2.\nRafael Irizarry. (2025). Introduction to Data Science: Data Wrangling and Visualization with R. Chapters 1-2.\nHadley Wickham, Mine Cetinkaya-Rundel, and Garrett Grolemund. (2025) R for Data Science. Sections 1-19.\nOnline R documentation\nRStudio Cheat Sheets",
    "crumbs": [
      "Covariance and Correlation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Module 3 Overview</span>"
    ]
  },
  {
    "objectID": "mod-3-0-overview.html#module-to-do-list",
    "href": "mod-3-0-overview.html#module-to-do-list",
    "title": "15  Module 3 Overview",
    "section": "15.4 Module To Do List",
    "text": "15.4 Module To Do List\n\nComplete the hands-on coding exercises embedded in each lesson (MLO 6)\n\nComplete the weekly class assignment (MLO 1 - MLO 6)\n\nParticipate in the discussion form (MLO 3 and MLO 4)\n\nTake the weekly knowledge quiz (MLO 1 - MLO 6)",
    "crumbs": [
      "Covariance and Correlation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Module 3 Overview</span>"
    ]
  },
  {
    "objectID": "mod-3-0-overview.html#lessons-in-this-module",
    "href": "mod-3-0-overview.html#lessons-in-this-module",
    "title": "15  Module 3 Overview",
    "section": "15.5 Lessons in this Module",
    "text": "15.5 Lessons in this Module\n\n3.1 – Skewness\n3.2 – Chebyshev’s Theorem\n3.3 - Covariance and Correlation\n\n\nNext: Start with Skewness.",
    "crumbs": [
      "Covariance and Correlation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Module 3 Overview</span>"
    ]
  },
  {
    "objectID": "mod-3-1-skewness.html",
    "href": "mod-3-1-skewness.html",
    "title": "16  Skewness",
    "section": "",
    "text": "16.1 Defining Skewness\nIn addition to measures of central tendency (population mean and sample average) and measures of dispersion (variance and standard deviation), we can investigate the skew of data.\nAre the data “shifted” to the left or right of the center? Are the data “clustered” around the measure of central tendency?\nSkewness measures the asymmetry of a probability distribution of a real-valued random variable around its mean. Skewness can be positive, negative, or undefined.\nFor a distribution with a single peak, a negative skew implies that the tail is on the left side of the distribution. A positive skew implies that the tail is on the right side of the distribution. A zero skew implies that the tails ‘balance’ which may mean the distribution is symmetric.\nThe Fisher-Pearson standardized moment coefficient is used by R and other major statistical software packages to measure skewness in a univariate sample. The sample skewness measure is equal to:\n\\[G_1 = \\frac{n}{(n-1)(n-2)}\\sum_{i=1}^{n} \\Big( \\frac{x_i - \\bar{x}}{s} \\Big)^3\\]",
    "crumbs": [
      "Covariance and Correlation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Skewness</span>"
    ]
  },
  {
    "objectID": "mod-3-1-skewness.html#skewness-example",
    "href": "mod-3-1-skewness.html#skewness-example",
    "title": "16  Skewness",
    "section": "16.2 Skewness Example",
    "text": "16.2 Skewness Example\nIn the following example, we construct a vector of observations for the variables x, y, and z.\nWe install and use the moments package to estimate skewness.\nThe first vector, x, has a mean of 7 and a median of 7. The observations are symmetrically distributed around 7, that is, the mirror are ‘reflected’ around the mean.\nIn a perfectly symmetrical distribution, the mean and median are the same. Note also that the estimate of skewness is equal to zero, that is, the tails of the distribution are symmetrical.\nThe second vector, y, has a mean of 6.3 and a median of 6.5. The estimate of skewness is -0.613. The negative skew means that the tail of the left side of the distribution is longer than the tail on the right side of the distribution.\nIn other words, if skewness is negative, the mass of the distribution is concentrated to the right, that is, the left tail is longer.\nThe third vector, z has a mean of 7.214 and a median of 7.0. The estimate of skewness of is 0.442. The positive skew means that the right tail of the distribution longer.\nIn other words, if skewness is positive, the mass of the distribution is concentrated to the left, that is, the right tail is longer.\n\nrm(list = ls())\n\n#Use moments package for skewness\n\nlibrary(dplyr)\nlibrary(kableExtra)\nlibrary(moments)\n\n# Define data vectors\n\nx &lt;- c(4, 5, 6, 6, 6, 7, 7, 7, 7, 7, 7, 8, 8, 8, 9, 10)\n\ny &lt;- c(4, 5, 6, 6, 6, 7, 7, 7, 7, 8)\n\nz &lt;- c(5, 6, 6, 6, 7, 7, 7, 7, 7, 8, 8, 8, 9, 10)\n\nx_stats &lt;- tibble(name = 'X-Stats',\n                  mean   = mean(x),\n                  median = median(x),\n                  skew   = skewness(x))\n\ny_stats &lt;- tibble(name = 'Y-Stats',\n                  mean   = mean(y),\n                  median = median(y),\n                  skew   = skewness(y))\n\nz_stats &lt;- tibble(name = 'Z-Stats',\n                  mean   = mean(z),\n                  median = median(z),\n                  skew   = skewness(z))\n\nstats &lt;- rbind(x_stats, y_stats, z_stats)\n\nkable(stats,\n      align     = 'c',\n      digits    = 3,\n      col.names = c('Source','Mean','Median','Skew')) %&gt;%\nkable_classic()\n\n\n\n\nSource\nMean\nMedian\nSkew\n\n\n\n\nX-Stats\n7.000\n7.0\n0.000\n\n\nY-Stats\n6.300\n6.5\n-0.613\n\n\nZ-Stats\n7.214\n7.0\n0.442",
    "crumbs": [
      "Covariance and Correlation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Skewness</span>"
    ]
  },
  {
    "objectID": "mod-3-1-skewness.html#interpreting-skewness",
    "href": "mod-3-1-skewness.html#interpreting-skewness",
    "title": "16  Skewness",
    "section": "16.3 Interpreting Skewness",
    "text": "16.3 Interpreting Skewness\nSkewness is a measure of the symmetry of the data. Negative skewness generally implies that the mean is less than the median. Negative skewness means that the left tail of the data is longer than the right tail of the data. Positive skewness implies that the mean is greater than the median and implies that the right tail of the data are longer than the left tail of the data.\nIf skewness is negative, the data are left-skewed.\nIf skewness is positive, the data are right-skewed.1\nFollowing Bulmer (1979), the following rules of thumb are useful in interpreting skewness.\n\nIf skewness is less than -1 or greater than 1, the data are highly skewed.\nIf skewness is between -1 and -1/2 or between 1/2 and 1, the data are moderately skewed.\nIf skewness is between -1/2 and 1/2, the data are approximately symmetric.\nIf skewness is 0, the data are perfectly symmetric.\n\nAt a minimum, the following points regarding the skewness measure can be made:\n\nThe sign represents the direction of skewness.\nThe measure compares to a symmetric distribution (i.e. the normal distribution).\nValues that are far from zero imply a non-normal or skewed population.\nThe measure has an adjustment for sample size.\nIn large samples, the adjustment is of little consequence.",
    "crumbs": [
      "Covariance and Correlation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Skewness</span>"
    ]
  },
  {
    "objectID": "mod-3-1-skewness.html#skewness-illustration",
    "href": "mod-3-1-skewness.html#skewness-illustration",
    "title": "16  Skewness",
    "section": "16.4 Skewness Illustration",
    "text": "16.4 Skewness Illustration\n\n\n\nRelationship Between Mean, Median, and Mode\n\n\nBy Diva Jain - https://commons.wikimedia.org/w/index.php?curid=84219892",
    "crumbs": [
      "Covariance and Correlation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Skewness</span>"
    ]
  },
  {
    "objectID": "mod-3-1-skewness.html#footnotes",
    "href": "mod-3-1-skewness.html#footnotes",
    "title": "16  Skewness",
    "section": "",
    "text": "For an extended discussion, see Doane and Seward (2011), “Measuring Skewness: A Forgotten Statistic?” Available at: http://jse.amstat.org/v19n2/doane.pdf↩︎",
    "crumbs": [
      "Covariance and Correlation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Skewness</span>"
    ]
  },
  {
    "objectID": "mod-3-2-normal.html",
    "href": "mod-3-2-normal.html",
    "title": "17  Chebyshev’s Theorem",
    "section": "",
    "text": "17.1 Z-Scores\nRecall that we previously defined a Z-score as a measure of the number of standardized deviations an observation is from the mean.\nGiven population data distributed with mean, \\(\\mu\\), and standard deviation, \\(\\sigma\\), the Z-score for an observation, \\(x\\), is denoted as:\n\\[\nZ_x = \\frac{x - \\mu}{\\sigma}\n\\]\nIf the Z-score of an observation was -2.5, then we would state that the observation lies 2.5 standard deviations to the left of the mean. If the Z-score is equal to 1.75, then we would state that the observation lies 1.75 standard deviations to the right of the population mean.\nLet \\(x_1 = 50\\), \\(\\mu_{x_1} = 20\\), and \\(\\sigma_{x_1}=5\\), then the \\(z_{x_1}\\) is equal to\n\\[\nz_{x_1} = \\frac{x_1 - \\mu}{\\sigma} = \\frac{50-20}{5} = 6\n\\]\nThe observation \\(x_1\\) is 6 standard deviations from the mean of 20. Given that \\(z_{x_1}\\) is positive, we know that the observation \\(x_1\\) is 6 standard deviations to the right of \\(\\mu_{x_1}\\).",
    "crumbs": [
      "Covariance and Correlation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Chebyshev's Theorem</span>"
    ]
  },
  {
    "objectID": "mod-3-2-normal.html#chebyshevs-theorem",
    "href": "mod-3-2-normal.html#chebyshevs-theorem",
    "title": "17  Chebyshev’s Theorem",
    "section": "17.2 Chebyshev’s Theorem",
    "text": "17.2 Chebyshev’s Theorem\nChebyshev’s Theorem states at least \\((1 - 1 / z^2)\\) of the data values must be within \\(z\\) standard deviations of the mean where \\(z\\) is any value greater than 1.\nSince the theorem is independent of the underlying distribution, we can apply Chebyshev’s theorem to a data set without worrying about how the data are distributed.\nWith this in mind, let \\(z = 2\\), \\(z = 3\\), and \\(z=4\\), then\n\\[z = 2 \\rightarrow (1 - \\frac{1}{2^2}) = 1 - 1/4 = 0.750 \\]\n\\[z = 3 \\rightarrow (1 - \\frac{1}{3^2}) = 1 - 1/9 = 0.889\\] \\[z = 4 \\rightarrow (1 - \\frac{1}{4^2}) = 1 - 1/16 = 0.972\\]\nChebyshev’s theorem implies that at least 75% of observations will lie within 2 standard deviations of the mean and at least 88.9% of observations with lie within 3 standard deviations of the mean.\nThis is quite useful when you do not know the underlying distribution of the data. The theorem provides a conservative estimate relative to the Empirical Rule which applies only to normal distributions and that we will explore later.",
    "crumbs": [
      "Covariance and Correlation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Chebyshev's Theorem</span>"
    ]
  },
  {
    "objectID": "mod-3-2-normal.html#chebyshevs-examples",
    "href": "mod-3-2-normal.html#chebyshevs-examples",
    "title": "17  Chebyshev’s Theorem",
    "section": "17.3 Chebyshev’s Examples",
    "text": "17.3 Chebyshev’s Examples\n\n17.3.1 Example 1\nAssume that you have a dataset with a mean of 100 and a standard deviation of 10.\n\\[\n\\mu = 100\n\\] \\[\n\\sigma = 10\n\\]\nYou are interested in a range of 3 standard deviations.\n\\[\n\\mu - 3\\sigma = 100 - 3 \\times 10 = 70\n\\] \\[\n\\mu + 3\\sigma = 100 + 3 \\times 10 = 130\n\\]\nChebyshev’s Theorem tells you at least 89% of observations fall between 70 and 130. This implies that no more than 11% of observations fall outside this range.\n\n\n17.3.2 Example 2\nAssume that a class takes a test. The mean score on the test is 75 and the standard deviation is 5. What proportion of scores fall between 65 and 85?\nGiven the mean is 75, you should note that 65 is 2 standard deviations less than the mean and 85 is 2 standard deviations greater than the mean.\nBy Chebyshev’s Theorem, this means that at least 75% of observations fall between 65 and 85. This implies that no more than 25% of observations fall outside this range.\n\n\n17.3.3 Example 3\nLet \\(\\mu = 80\\) and \\(\\sigma = 4\\).\n\\[ \\mu - 2\\sigma = 80 - (2 \\times 4) = 72\\] \\[\\mu + 2\\sigma = 80 + (2 \\times 4) = 88\\]\nAt least 75% of observations for a distribution with \\(\\mu = 80\\) and \\(\\sigma = 4\\) lie in the (72,88) range.\n\\[ \\mu - 3\\sigma = 80 - (3 \\times 4) = 68\\] \\[\\mu + 3\\sigma = 80 + (3 \\times 4) = 92 \\]\nAt least 75% of observations lie in the (68,92) range.",
    "crumbs": [
      "Covariance and Correlation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Chebyshev's Theorem</span>"
    ]
  },
  {
    "objectID": "mod-3-2-normal.html#the-normal-distribution",
    "href": "mod-3-2-normal.html#the-normal-distribution",
    "title": "17  Chebyshev’s Theorem",
    "section": "17.4 The Normal Distribution",
    "text": "17.4 The Normal Distribution\nThe normal distribution is undoubtedly one of the most important, if not the most important, distribution in statistics. While there are technically an infinite number of normal distributions, we often refer to “the normal distribution” to discuss the properties of the distribution. Unlike the previous examples which have been collections of discrete observations, the normal distribution is continuous.\nA normal distribution in \\(X\\) with mean \\(\\mu\\) and variance \\(\\sigma^2\\) is a distribution resulting from the probability distribution function that is defined on the \\(x \\in (-\\infty, \\infty)\\) interval and is equal to:\n\\[\nP(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{-(x - \\mu) / (2 \\sigma^2)}\n\\]\nWe will discuss the properties of the normal and other distributions at greater length in coming modules.\nFor now, we will focus on the general properties that help us think about measures of central tendency, variability, and the shape of the data.\nThe normal distribution has the following properties.\n\nThe mean, mode, and median are all equal.\nThe normal distribution is symmetric around \\(\\mu\\)\nExactly 50% of the values lie to the left of \\(\\mu\\)\nExactly 50% of the values lie to the right of \\(\\mu\\)\nThe total area under the curve is equal to 1",
    "crumbs": [
      "Covariance and Correlation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Chebyshev's Theorem</span>"
    ]
  },
  {
    "objectID": "mod-3-2-normal.html#the-empirical-rule",
    "href": "mod-3-2-normal.html#the-empirical-rule",
    "title": "17  Chebyshev’s Theorem",
    "section": "17.5 The Empirical Rule",
    "text": "17.5 The Empirical Rule\nThe Empirical rule is based on Chebyshev’s theorem and, given a normal distribution, states that:\n\nApproximately 68% of observations lie within one standard deviation of \\(\\mu\\).\nApproximately 95% of observations lie within two standard deviations of \\(\\mu\\).\nApproximately 99% of observations lie within three standard deviations of \\(\\mu\\).\n\n\n\n\nThe Normal Distribution\n\n\n\n17.5.1 Example 1\nLet \\(\\mu = 25\\) and \\(\\sigma = 5\\) then, by the empirical rule for a normal distribution, we know:\n\nApproximately 68% of observations lie between 20 and 30 or \\(\\mu \\pm \\sigma\\).\nApproximately 95% of observations lie between 15 and 35 or \\(\\mu \\pm 2\\sigma\\).\nApproximately 99% of observations lie between 10 and 40 or \\(\\mu \\pm 3\\sigma\\).\n\nWe can confirm this by estimating the Z-Score for 10, 15, 20, 30, 35, and 40.\n\\[\nZ(10) = \\frac{10 - 25}{5} = -3\n\\]\n\\[\nZ(15) = \\frac{15 - 25}{5} = -2\n\\]\n\\[\nZ(20) = \\frac{20 - 25}{5} = -1\n\\]\n\\[\nZ(30) = \\frac{30 - 25}{5} = 1\n\\]\n\\[\nZ(35) = \\frac{35 - 35}{5} = 2\n\\]\n\\[\nZ(40) = \\frac{40 - 25}{5} = 3\n\\]\n\n\n17.5.2 Example 2\nAssume that a pizza delivery restaurant has a mean delivery time of 32 minutes with a standard deviation of 4 minutes. Assume that delivery times are normally distributed.\nWhat is the range of times in which 68%, 95%, and 95% of deliveries occur?\nAccording to the Empirical Rule,\n\\[\n\\text{68\\%} \\rightarrow  \\mu \\pm \\sigma = 32 \\pm 4 = (28,36)\n\\]\n\\[\n\\text{95\\%} \\rightarrow \\mu \\pm 2\\sigma = 32 \\pm 8 = (24,40)\n\\]\n\\[\n\\text{99\\%} \\rightarrow \\mu \\pm 3\\sigma = 32 \\pm 12 = (20,44)\n\\]",
    "crumbs": [
      "Covariance and Correlation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Chebyshev's Theorem</span>"
    ]
  },
  {
    "objectID": "mod-3-3-correlation.html",
    "href": "mod-3-3-correlation.html",
    "title": "18  Correlation",
    "section": "",
    "text": "18.1 Introduction\nTo this point, we have mostly examined methods of describing one variable, that is, univariate analysis.\nHowever, we may wish to do more than describe the properties of one variable.\nWe may want to explore a potential relationship (or absence of one) between two variables. Moving from univariate to bivariate analysis starts with a discussion of measures of association between two variables.\nTo start our discussion, we need to understand the expectations operator.",
    "crumbs": [
      "Covariance and Correlation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "mod-3-3-correlation.html#the-expectations-operator",
    "href": "mod-3-3-correlation.html#the-expectations-operator",
    "title": "18  Correlation",
    "section": "18.2 The Expectations Operator",
    "text": "18.2 The Expectations Operator\nGiven a function \\(f(x)\\) in the variable \\(x\\), the expectation value is denoted as \\(E[X]\\).\nFor a single, discrete variable \\(X\\) with probability density function \\(P(X)\\), the expectation of \\(X\\) is defined as:\n\\[\nE[X] = \\sum_{X} f(x) P(x)\n\\]\nIf \\(X\\) is a continuous variable, then \\(E[X]\\) is defined as:\n\\[\nE[X] = \\int f(x) P(X) dx\n\\]\nThe expected value of a random variable is the arithmetic mean of that variable. This term has been retained in mathematical statistics to mean the long-run average for any random variable over an indefinite number of trials or samples. In other words,\n\\[\nE[X] = \\mu_x\n\\]\nThe variance of a random variable \\(X\\) is defined as the expected (average) squared deviation of the values of this random variable about their mean. That is,\n\\[var(x) = E[(X - \\mu)^2] = E[X^2] - \\mu^2 = \\sigma_X^2\\]\nThe expectations operator adheres to the following rules.\nLet \\(X\\) and \\(Y\\) be random variables and let \\(k\\) and \\(j\\) be constants.\nThe first rule is that the expectation of a constant is a constant.\nIn other words, if \\(k=4\\), then every observation of \\(k\\) is 4 and the mean of \\(k\\) is \\(E[k] = \\mu_k = 4\\).\n\\[\nE[k] = k\n\\]\nThe second rule is that the expectation of the product of a constant and a random variable is equal to the product of a constant and the expectation of a random variable. In other words, assume that we multiply every observation of a random variable by 3 and then find the long-run mean of the random variable. This is the same as finding the long-run mean and multiplying the mean by 3.\n\\[\nE[kX] = kE[X] = k \\mu_x\n\\]\nThe third rule is that the expectation of the sum of two random variables is equal to the addition of the expectiations of the two random variables. In other words, if we add two random variables, one with the mean of 5 and the other with the mean of 10, this is the same as adding the two means together.\n\\[\nE[X+Y] = E[X] + E[Y]\n\\]\nThe fourth rule combines the second and third rules in that if you have the sum of the two random variables where each random variable is multiplied by a constant, then you can restate this as the product of the constant and the expectation of the first random variable plus the product of the constant and the expectation of the second random variable.\n\\[\nE[kX + jY] = kE[X] + jE[Y]\n\\]\nThere are a number of other expectation rules which we will cover later on in the course.",
    "crumbs": [
      "Covariance and Correlation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "mod-3-3-correlation.html#covariance",
    "href": "mod-3-3-correlation.html#covariance",
    "title": "18  Correlation",
    "section": "18.3 Covariance",
    "text": "18.3 Covariance\nGiven two random variables, \\(X\\) and \\(Y\\), the covariance between these two variables can be defined as:\n\\[\ncov(X,Y) = E[(X - E[X])(Y - E[Y])]\n\\]\n\\[\ncov(X,Y) = E[(X - \\mu_x)(Y - \\mu_Y)] = E[XY] - E[X]E[Y]\n\\] The population covariance \\(\\sigma_{xy}\\) between two random variables is defined as:\n\\[\ncov(X,Y) = \\sigma_{XY} = \\sum_{i=1}^{N} \\frac{(x_i - \\mu_x)(y_i - \\mu_y)}{N}\n\\]\nThe sample covariance \\(s_{xy}\\) between two random variables is defined as:\n\\[\ncov(X,Y) = s_{XY} = \\sum_{i=1}^{n} \\frac{(x_i - \\bar{x})(y_i - \\bar{y})}{n-1}\n\\]\nOne point to note is that the covariance between a variable and itself is the variance of the variable.\nWe can easily see this for the population covariance by substituting \\(X\\) for \\(Y\\) or\n\\[\ncov(X,X) = \\sigma_{XX} = \\sigma_X^2 = \\sum_{i=1}^{N} \\frac{(x_i - \\mu_x)(x_i - \\mu_x)}{N} = \\sum_{i=1}^{N} \\frac{(x_i - \\mu_x)^2}{N}\n\\]",
    "crumbs": [
      "Covariance and Correlation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "mod-3-3-correlation.html#scatterplots",
    "href": "mod-3-3-correlation.html#scatterplots",
    "title": "18  Correlation",
    "section": "18.4 Scatterplots",
    "text": "18.4 Scatterplots\nIn the following code chunks, we create scatterplots of car weight and miles per gallon from the mtcars data set.\nThe scatterplot of the two variables suggests that as car weight increases, miles per gallon declines.\nIn the first scatterplot, we have weight on the x-axis and miles per gallon on the y-axis.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWhat happens if we have miles per gallon on the x-axis and weight on the y-axis?\nThe same negative association appears to be present between the two variables, however, the important point is that that association is not causation, that is, we do not know if variation in one variable causes variation in another variable.\nAnother way to think about this is that we are investigating whether variables ‘move’ together in a systematic fashion.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Covariance and Correlation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "mod-3-3-correlation.html#covariance-example",
    "href": "mod-3-3-correlation.html#covariance-example",
    "title": "18  Correlation",
    "section": "18.5 Covariance Example",
    "text": "18.5 Covariance Example\nLet’s explore the covariance between two variables, \\(X\\), and \\(Y\\) in the following code.\nWe first create a tibble with \\(X\\) and \\(Y\\).\nWe then estimate the covariance between \\(X\\) and \\(Y\\) manually:\n\nWe estimate the mean deviations of \\(X\\)\nWe estimate the mean deviations of \\(Y\\)\nWe estimate the product of the mean deviations of \\(X\\) and \\(Y\\)\nWe estimate the product of \\(1/(n-1)\\) and the product of the mean deviations of \\(X\\) and \\(Y\\)\n\nThe result is an estimate of the sample covariance between \\(X\\) and \\(Y\\).\nWe then use the cov function to estimate the sample covariance between \\(X\\) and \\(Y\\).\nThis also illustrate that R uses the sample covariance function to estimate the covariance between two variables.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Covariance and Correlation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "mod-3-3-correlation.html#fred-covariance",
    "href": "mod-3-3-correlation.html#fred-covariance",
    "title": "18  Correlation",
    "section": "18.6 FRED Covariance",
    "text": "18.6 FRED Covariance\nIn the following code chunk, we want to estimate the covariance between the headline unemployment rate and the headline inflation rate.\nUsing the tidyquant package, we obtain the data from FRED.\nHowever, the data from FRED are in ‘long’ format, that is, one column contains symbols that identify the series and another column contains the values. We want to translate the data into ‘wide’ format, where each row (date) contains several columns of data.\nWe use the pivot_wider command to translate the data from long to wide format and then estimate the monthly inflation rate as the year-over-year change in the Consumer Price Index. Since the unemployment rate is not in decimal form, we divide the unemployment series by 100.\nWe then create a scatterplot before estimating the mean of each series and the covariance between the two series.\n\nrm(list = ls())\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(tidyquant)\nlibrary(scales)\n\ndf_3 &lt;- tq_get(c('UNRATE', 'CPIAUCSL'),\n               get  = 'economic.data',\n               from = '1969-01-01') %&gt;%\n  pivot_wider(names_from = symbol, values_from = price) %&gt;%\n  mutate(inflation = (CPIAUCSL - lag(CPIAUCSL, 12)) / lag(CPIAUCSL, 12),\n         UNRATE    = UNRATE/100) %&gt;%\n  filter(!is.na(inflation))\n\nggplot(data = df_3, \n       aes(x = inflation, \n           y = UNRATE)) +\ngeom_point() +\nscale_x_continuous(labels = scales::percent) +\nscale_y_continuous(labels = scales::percent) +\ntheme_classic() +\nlabs(x = \"Inflation (YoY %)\",\n     y = \"Unemployment Rate (%)\",\n     title = \"Scatterplot of Inflation and Unemployment\")\n\n\n\n\n\n\n\ndf_4 &lt;- df_3 %&gt;%\n        summarize(mean_inf = mean(inflation),\n                  mean_unr = mean(UNRATE),\n                  cov_i_u  = cov(inflation, UNRATE))\n\nkable(df_4,\n      col.names = c('Mean Inflation',\n                    'Mean Unemployment',\n                    'Covariance Inflation and Unemployment'),\n      align     = 'c') %&gt;%\nkable_classic()\n\n\n\n\nMean Inflation\nMean Unemployment\nCovariance Inflation and Unemployment\n\n\n\n\n0.0399943\n0.0606164\n2.81e-05",
    "crumbs": [
      "Covariance and Correlation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "mod-3-3-correlation.html#mtcars-covariance",
    "href": "mod-3-3-correlation.html#mtcars-covariance",
    "title": "18  Correlation",
    "section": "18.7 MTCARS Covariance",
    "text": "18.7 MTCARS Covariance\nIn our last example of covariance, we return to the mtcars dataset and the variables for the weight of each car (in thousands of pounds) and the miles per gallon of each car.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Covariance and Correlation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "mod-3-3-correlation.html#interpreting-the-covariance",
    "href": "mod-3-3-correlation.html#interpreting-the-covariance",
    "title": "18  Correlation",
    "section": "18.8 Interpreting the Covariance",
    "text": "18.8 Interpreting the Covariance\nGiven that the sample covariance in the previous example is equal to approximately -5.12, what does this mean about the potential relationship between the two variables?\nIf \\(\\sigma_{XY} &gt; 0\\) or \\(s_{XY} &gt; 0\\), this implies that \\(X\\) and \\(Y\\) are positively linearly related, that is, as \\(X\\) increases, \\(Y\\) increases in a linear fashion.\nLikewise, If \\(\\sigma_{XY} &lt; 0\\) or \\(s_{XY} &lt; 0\\), this implies that \\(X\\) and \\(Y\\) are negatively linearly related, that is, as \\(X\\) increases, \\(Y\\) declines in a linear fashion.\nIf \\(\\sigma_{XY} \\approx 0\\) or \\(s_{XY} \\approx 0\\), this implies that \\(X\\) and \\(Y\\) are not linearly related, that is, an increase in \\(X\\) does not appear to influence \\(Y\\).\nIf \\(X\\) and \\(Y\\) are independent, then \\(\\sigma_{XY} = 0\\) and \\(s_{XY} = 0\\).\nHowever, we typically do not rely on covariance to measure the association between two variables.",
    "crumbs": [
      "Covariance and Correlation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "mod-3-3-correlation.html#scaling-and-covariance",
    "href": "mod-3-3-correlation.html#scaling-and-covariance",
    "title": "18  Correlation",
    "section": "18.9 Scaling and Covariance",
    "text": "18.9 Scaling and Covariance\nThe choice of units can alter the covariance measure. In other words, you can increase or decrease covariance between two variables by changing the units of measures of one or both variables.\nWe can easily see this in the following example. We add two new variables, one that converts miles per gallon into feet per gallon mpg_ft and one that converts weight in pounds into weight in ounces wt_oz.\nThe covariance is dependent upon how each variable is measured.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Covariance and Correlation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "mod-3-3-correlation.html#correlation-coefficient",
    "href": "mod-3-3-correlation.html#correlation-coefficient",
    "title": "18  Correlation",
    "section": "18.10 Correlation Coefficient",
    "text": "18.10 Correlation Coefficient\nGiven that the covariance measure is affected by the choice of units of measurement, we can use the correlation coefficient to examine the relationship between two random variables.\nThe Pearson correlation coefficient is defined as the ratio of the covariance between \\(X\\) and \\(Y\\) to the product of the standard deviations of \\(X\\) and \\(Y\\) and is denoted as \\(\\rho_{XY}\\) in the population and \\(r_{XY}\\) in the sample.\nIn expectations, the Pearson correlation coefficient is equal to:\n\\[\\begin{equation}\n\\rho_{XY} = \\frac{E[(X - \\mu_x)(Y - \\mu_Y)]}{\\sigma_X \\sigma_Y}\n\\end{equation}\\]\nIn the population, the Pearson correlation coefficient, \\(\\rho_{XY}\\), is equal to:\n\\[\\begin{equation}\n\\rho_{XY} = \\frac{\\sigma_{XY}}{\\sigma_X \\sigma_Y}\n\\end{equation}\\]\nIn the sample, the Pearson correlation coefficient, \\(r_{XY}\\), is equal to:\n\\[\\begin{equation}\nr_{XY} = \\frac{s_{XY}}{s_X s_Y}\n\\end{equation}\\]\nThe Pearson correlation coefficient is bounded between -1 and 1.\nA negative sign implies a negative association between the two variables of interest, while a positive sign implies a positive association. The closer the correlation coefficient is to -1 or 1, the stronger the association between the two variables. If the correlation coefficient is equal to -1 or 1, the two variables are perfectly correlated. If the correlation coefficient is equal to 0, the two variables are uncorrelated.\nReturning to the previous example, we first manually calculate the sample covariance between car weight and miles per gallon and the sample standard deviations for these variables. We then calculate the sample correlation coefficient manually and compare it the results of the correlation function.",
    "crumbs": [
      "Covariance and Correlation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "mod-3-3-correlation.html#correlation-example",
    "href": "mod-3-3-correlation.html#correlation-example",
    "title": "18  Correlation",
    "section": "18.11 Correlation Example",
    "text": "18.11 Correlation Example\nLet’s explore the correlation between two variables, \\(X\\), and \\(Y\\) in the following code.\nWe return first to our previous example.\n\nWe estimate the standard deviations of \\(X\\) and \\(Y\\)\nWe estimate the covariance between \\(X\\) and \\(Y\\)\nWe manually estimate the correlation by dividing the covariance by the product of the standard deviations.\nWe use the cor function to estimate the correlation between the two variables\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Covariance and Correlation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "mod-3-3-correlation.html#fred-correlation",
    "href": "mod-3-3-correlation.html#fred-correlation",
    "title": "18  Correlation",
    "section": "18.12 FRED Correlation",
    "text": "18.12 FRED Correlation\nIn the code chunk below, we again retrieve the headline unemployment rate and headline inflation series from FRED.\nThis time, however, we create two new variables.\n\nInflation_2 is equal to the inflation series times 1000\nUnemployment_2 is equal to the unemployment series time 100\n\nWe then estimate the covariance between\n\nInflation and unemployment\nInflation_2 and unemployment\nInflation and unemployment_2\n\nThe results demonstrate that the covariance measure is dependent on the scale or units of measurement and that we can inflate (or deflate) the covariance measure by scaling one or both variables.\nWe then estimate four correlation coefficients:\n\nInflation and unemployment\nInflation_2 and unemployment\nInflation and unemployment_2\nInflation_2 and unemployment_2\n\nThe correlation coefficients are equal to each other. In other words, the correlation coefficient is invariant to the scale of the variables.\n\nrm(list = ls())\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(tidyquant)\nlibrary(scales)\n\ndf_5 &lt;- tq_get(c('UNRATE', 'CPIAUCSL'),\n               get  = 'economic.data',\n               from = '1969-01-01') %&gt;%\n  pivot_wider(names_from = symbol, values_from = price) %&gt;%\n  mutate(inflation = (CPIAUCSL - lag(CPIAUCSL, 12)) / lag(CPIAUCSL, 12),\n         unrate    = UNRATE/100) %&gt;%\n  filter(!is.na(inflation)) %&gt;%\n  mutate(unrate_2 = unrate *100,\n         inflation_2 = inflation * 1000) %&gt;%\n  summarize(cov_i_u     = cov(inflation, unrate),\n            cov_i2_u    = cov(inflation_2, unrate),\n            cov_i_u2    = cov(inflation, unrate_2),\n            cor_i_u     = cor(inflation, unrate),\n            cor_i2_u    = cor(inflation_2, unrate),\n            cor_i_u2    = cor(inflation, unrate_2),\n            cor_i2_u2   = cor(inflation_2, unrate_2))\n\nkable(df_5,\n      align = 'c',\n      digits = 3,\n      col.names = c('COV(Inf, Unemp)', 'COV(Inf2, Unemp)',\n                    'COV(Inf, Unemp2)', 'COR(Inf,Unemp)',\n                    'COR(Inf2, Unemp)', 'COR(Inf, Unemp2)',\n                    'COR(Inf2, Unemp2)'))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCOV(Inf, Unemp)\nCOV(Inf2, Unemp)\nCOV(Inf, Unemp2)\nCOR(Inf,Unemp)\nCOR(Inf2, Unemp)\nCOR(Inf, Unemp2)\nCOR(Inf2, Unemp2)\n\n\n\n\n0\n0.028\n0.003\n0.057\n0.057\n0.057\n0.057",
    "crumbs": [
      "Covariance and Correlation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "mod-3-3-correlation.html#mtcars-correlation",
    "href": "mod-3-3-correlation.html#mtcars-correlation",
    "title": "18  Correlation",
    "section": "18.13 MTCARS Correlation",
    "text": "18.13 MTCARS Correlation\nWe return to the mtcars dataset and the variables for the weight of each car (in thousands of pounds) and the miles per gallon of each car.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Covariance and Correlation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "mod-3-3-correlation.html#correlation-and-causation",
    "href": "mod-3-3-correlation.html#correlation-and-causation",
    "title": "18  Correlation",
    "section": "18.14 Correlation and Causation",
    "text": "18.14 Correlation and Causation\nTo this point, we have examined measures of association rather than measures of causation.\nWe must be careful to understand that while association may imply causation, association, by itself, is not sufficient to conclude a causal relationship exists.\nAssociation is necessary, but not sufficient, for causation\nCorrelation between two variables may be the product of one or more confounding variables. Observational studies may fall victim to confounding variables, leading one to conclude causation exists. For example, assume that 100 patients were sick with a disease with 5% mortality after two weeks. All the patients were given an experimental drug and at the end of two weeks, only 1 patient has died. Obviously the drug has reduced mortality, or has it? What if 10,000 patients were treated and only 1% died?\nWhile the drug is associated with decreased mortality, we have not controlled for other potential confounding variables. The general health of the patients may have been different than the general population. The patients may have received other drugs that are known to reduce mortality. The age of the patients may be lower (higher) than the general population. This is why scientists use double-blind, randomized, placebo-controlled trials to examine the efficacy of new drugs. These trials reduce (but do not eliminate) the possibility of confounding so investigators can examine whether the outcomes of the control (receive placebo) and treatment (receive drug) groups are statistically different.\nA strong association between two variables is suggestive of a causal relationship but we must be cautious. How were the data gathered? Was there a mechanism to reduce the presence of confounding variables? Are the data experimental, that is, random assignment into control and treatment groups? As we move from observational studies to experimental studies, the ability to suggest that association implies causation increases, but, in the end, we must find other methods to explore whether two (or more) variables are causally related.",
    "crumbs": [
      "Covariance and Correlation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "mod-3-3-correlation.html#module-summary",
    "href": "mod-3-3-correlation.html#module-summary",
    "title": "18  Correlation",
    "section": "18.15 Module Summary",
    "text": "18.15 Module Summary\nWe use descriptive statistics to develop an understanding of the properties of the data.\n\nWe have discussed two broad properties of the data:\n\nThe center of the data\nThe spread of the data\n\nWe developed two measures of association (co-movement)\n\nCovariance\nCorrelation\n\nIn the coming modules, we will discuss randomness, the concept of probability, and probability distributions.",
    "crumbs": [
      "Covariance and Correlation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "mod-4-0-overview.html",
    "href": "mod-4-0-overview.html",
    "title": "19  Module 4 Overview",
    "section": "",
    "text": "19.1 Introduction\nWelcome to Module 4 of ECON 700.\nThis module introduces the foundational concepts of probability as a measure of uncertainty and likelihood. Students learn how random experiments generate outcomes that form a sample space, and how probability quantifies the chance that specific outcomes occur. The module progresses through the building blocks of probability theory, including combinations, permutations, complements, unions, and intersections of events. Through these principles, students develop an understanding of mutually exclusive and independent events and how they shape probability calculations.\nThe second half of the module applies probability rules to real-world decision-making and data analysis. Learners explore conditional probability, the multiplication and addition laws, and the distinction between joint and marginal probabilities. The module culminates in an introduction to Bayes’ Theorem, illustrating how prior beliefs are updated with new evidence—first conceptually, then through applications like medical testing and the Monty Hall problem. This approach emphasizes the interpretive and practical power of probability in economic reasoning, uncertainty, and risk assessment.",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Module 4 Overview</span>"
    ]
  },
  {
    "objectID": "mod-4-0-overview.html#learning-objectives",
    "href": "mod-4-0-overview.html#learning-objectives",
    "title": "19  Module 4 Overview",
    "section": "19.2 Learning Objectives",
    "text": "19.2 Learning Objectives\nBy the end of this module, you should be able to:\n\nMLO 1: Define key probability concepts, including experiments, sample spaces, and events. (CLO 3)\nMLO 2: Compute probabilities using basic rules (addition, multiplication, complement) for mutually exclusive and independent events. (CLO 3)\nMLO 3: Differentiate between permutations and combinations in counting possible outcomes. (CLO 3)\nMLO 4: Interpret conditional probabilities and construct joint and marginal probability tables. (CLO 3)\nMLO 5: Apply Bayes’ Theorem to update probabilities based on new information. (CLO 3)\nMLO 6: Analyze probabilities using R. (CLO 3, 5)",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Module 4 Overview</span>"
    ]
  },
  {
    "objectID": "mod-4-0-overview.html#required-texts",
    "href": "mod-4-0-overview.html#required-texts",
    "title": "19  Module 4 Overview",
    "section": "19.3 Required Texts",
    "text": "19.3 Required Texts\nThe following textbooks are available for free. Please select the links provided to access.\n\nBarbara Illowsky and Susan Dean. Introductory Statistics 2e. OpenStax. Chapter 3.\nRafael Irizarry. (2025). Introduction to Data Science: Data Wrangling and Visualization with R. Chapters 1-2.\nHadley Wickham, Mine Cetinkaya-Rundel, and Garrett Grolemund. (2025) R for Data Science. Sections 1-19.\nOnline R documentation\nRStudio Cheat Sheets",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Module 4 Overview</span>"
    ]
  },
  {
    "objectID": "mod-4-0-overview.html#activities",
    "href": "mod-4-0-overview.html#activities",
    "title": "19  Module 4 Overview",
    "section": "19.4 Activities",
    "text": "19.4 Activities\n\nComplete the hands-on coding exercises embedded in each lesson (MLO 6)\n\nComplete the weekly class assignment (MLO 1 - MLO 6)\n\nParticipate in the discussion form (MLO 4 and MLO 5)\n\nTake the weekly knowledge quiz (MLO 1 - MLO 6)",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Module 4 Overview</span>"
    ]
  },
  {
    "objectID": "mod-4-0-overview.html#lessons-in-this-module",
    "href": "mod-4-0-overview.html#lessons-in-this-module",
    "title": "19  Module 4 Overview",
    "section": "19.5 Lessons in this Module",
    "text": "19.5 Lessons in this Module\n\n4.1 – Introduction to Probability\n4.2 – Probability Rules\n4.3 – Conditional Probability\n\n\nNext: Start with Introduction to Probability.",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Module 4 Overview</span>"
    ]
  },
  {
    "objectID": "mod-4-1-probability.html",
    "href": "mod-4-1-probability.html",
    "title": "20  Introduction to Probability",
    "section": "",
    "text": "20.1 Defining Probablity\nProbability is a measure of the likelihood that an outcome will occur.\nIf a fair coin has tails on one side and heads on the other side, we understand, over thousands of flips (trials), that we will observe heads 50% of the time and tails 50% of the time. Assuming that no flips end up on the edge, the likelihood of heads for a fair coin is 0.50 while the likelihood of tails is 0.50.\nProbability is a numerical measure of the likelihood of an outcome and is bounded between 0 and 1, including 0 and 1. If the probability of an outcome is ‘close’ to zero, then it is less likely to occur than an outcome that is ‘closer’ to 1.\nIn other words, if the outcome of the outcome is beneficial (winning a prize), we would prefer that the probability of the outcome be higher (closer to 1) than lower. Likewise, if the outcome of the outcome if not beneficial (incurring a costly bill), we would prefer the probability of the outcome be lower (closer to 0) than higher.",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "mod-4-1-probability.html#experiments-and-randomness",
    "href": "mod-4-1-probability.html#experiments-and-randomness",
    "title": "20  Introduction to Probability",
    "section": "20.2 Experiments and Randomness",
    "text": "20.2 Experiments and Randomness\nA random experiment is a process that generates outcomes.\nIn our coin flipping example, the process of flipping the coin generates two possible outcomes, heads or tails. The outcomes are well defined prior to the start of the experiment and the outcomes are mutually exclusive. In other words, an outcome must be either heads or tails and there is not a possible outcome where heads and tails occur simultaneously.\nEach repetition of the experiment is a trial.\nAn experiment may consist of one trial, several trials, or many trials. However many trials there are in an experiment, the outcome of each trial is a random event, that is, the outcome is not pre-determined and occurs by chance.\nConsider an unfair six-sided die that is weighted so that it lands on six on every throw. Throwing the die is not a random experiment because the outcome of every trial is known prior to the experiment. Regardless of whether you throw the die one time or fifty times, the outcome is always six. Now compare this to a fair six sided die. On any given trial, the die could land on 1, 2, 3, 4, 5, or 6. The outcome of any specific trial is unknown prior to its occurrence but, over repeated trials, we know the probability of each outcome.\nWe have now discussed two random experiments, the flipping of a fair coin and the toss of a fair six sided tie. In the case of the fair coin, we have identified two possible outcomes: heads and tails. In the case of the fair six sided die, we have identified six possible outcomes: 1, 2, 3, 4, 5, and 6.\nThe set of possible outcomes for each random experiment is known as the sample space.\nThe sample space consists of the mutually exclusive and collectively exhaustive outcomes of a random experiment.\nMutually exclusive implies that a random experiment cannot have the same outcome simultaneously.\nCollectively exhaustive implies that all the outcomes of the random experiment are contained in the sample space.",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "mod-4-1-probability.html#multistep-counting-rules",
    "href": "mod-4-1-probability.html#multistep-counting-rules",
    "title": "20  Introduction to Probability",
    "section": "20.3 Multistep Counting Rules",
    "text": "20.3 Multistep Counting Rules\nConsider a fair 10-sided tie.\nThe potential outcomes of any trial are\n\\[s = \\{1, 2, 3, 4, 5, 6, 7, 8 , 9, 10\\}\\] What happens if we conduct two trails?\nThere are ten possible outcomes of the first trial and ten possible outcomes in the second trial. But what are number of outcomes for the multistep experiment?\nAssume that you roll a 1 on the first trial. The set of possible outcomes of the first trial \\(o_1\\) are:\n\\[o_1 = \\{(1,1), (1,2), (1,3), (1,4), (1,5), (1,6), (1,7), (1,8), (1,9), (1,10) \\}\\]\nAnd if you roll a 2 on the first trial, then the set of possible outcomes are:\n\\[o_2 = \\{(2,1), (2,2), (2,3), (2,4), (2,5), (2,6), (2,7), (2,8), (2,9), (2,10) \\}\\]\nIf the first roll produces a 1, there are 10 possible outcomes. If the first roll produces a 2, there are 10 different outcomes, and so on.\nExtending this logic yields 100 possible outcomes for the two trial experiment with a fair ten-sided tie.\nGiven a random experiment with a sequence of \\(k\\)-steps and \\(n_1, n_2, ..., n_k\\) outcomes for each step, then the number of outcomes in the sample space is equal to:\n\\[n_1 \\times n_2 \\times ... \\times n_k\\] The random experiment of rolling a 10-sided die two times has \\(k = 2\\) steps and the number of outcomes of each step is equal to \\(n_1 = 10, n_2 = 10\\). The number of outcomes is equal to\n\\[n_1 \\times n_2 = 10 \\times 10 = 100\\]",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "mod-4-1-probability.html#multistep-examples",
    "href": "mod-4-1-probability.html#multistep-examples",
    "title": "20  Introduction to Probability",
    "section": "20.4 Multistep Examples",
    "text": "20.4 Multistep Examples\nIn our first example, assume we are flipping a fair two-sided coin five times.\nGiven that \\(k = 5\\) and \\(n_1 = 2, n_2 = 2, n_3 = 2, n_4 = 2, n_5 = 2\\), then number outcomes in the sample space of the random experiment is equal to:\n\\[s = n_1 \\times n_2 \\times n_3 \\times n_4 \\times n_5 = 2 \\times 2 \\times 2 \\times 2 \\times 2 = 2^5 = 32\\]\nIn our second example, assume that we are throwing a fair six-sided die followed by a fair two-sided coin followed by a fair twenty-sided die.\nGiven that \\(k =3\\) and \\(n_1 = 6, n_2 = 2, n_3 = 20\\), then the number of outcomes in the sample space is equal to:\n\\[n_1 \\times n_2 \\times n_3 = 6 \\times 2 \\times20 = 240\\]",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "mod-4-1-probability.html#factorials",
    "href": "mod-4-1-probability.html#factorials",
    "title": "20  Introduction to Probability",
    "section": "20.5 Factorials",
    "text": "20.5 Factorials\nA factorial is merely a series of products and are indicated by an exclamation mark.\nThe factorial function of a positive integer \\(n\\) is defined as the product of all the positive integers not greater than \\(n\\).\n\\[\nn! = 1 \\times 2 \\times 3 \\times 4 \\dots (n-2) \\times (n-1) \\times n\n\\] For example, 5 factorial can be represented as\n\\[\n5! = 5 \\times 4 \\times 3 \\times 2 \\times 1 = 120\n\\]\nIn R, the function factorial allows one to estimate the factorial of a positive integer\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "mod-4-1-probability.html#combinations",
    "href": "mod-4-1-probability.html#combinations",
    "title": "20  Introduction to Probability",
    "section": "20.6 Combinations",
    "text": "20.6 Combinations\nIn our previous examples, the number of steps and outcomes for each step was relatively small. Determining the number of outcomes in a sample space can quickly become tedious, or, as complexity increases, hard. We can, however, rely on two counting rules to assist us in determining the number of outcomes in a sample space. The difference between the two counting rules is whether the order of the outcomes matters.\nA combination examines the number of ways the objects can be selected, without regard to the order in which they are selected.\nA bowl of vegetable soup contains a different vegetables and the number of vegetables may differ by type. We don’t care about ordering since all the vegetables together make soup.\nAssume we have four objects (A,B,C,D), and we select one object at a time without replacement and draw all the objects.\nLet’s list the possible combinations:\n\nABCD\n\nWhy is there only one combination? Remember, the order does not matter. In other words, ABCD is the same as BCAD which is the same as CABD which is the same as BACD and so on.\nAssume now that we have the four objects and we select two objects at a time without replacement.\nAssume the objects are labeled A, B, C, D and the order of drawing is not important (AC is the same as drawing CA), then the number of possible combinations:\n\nAB\nAC\nAD\nBC\nBD\nCD\n\nGiven four objects and two objects are selected at a time, there are six possible combinations.\nConsider the same number of objects (4) and draw three objects at a time without replacement.\nAssume the objects are labeled A, B, C, D and the order of drawing is not important (AC is the same as drawing CA), then the number of possible combinations:\n\nABC\nABD\nACD\nBCD\n\nThere are four possible combinations.\nObviously, if the number of objects becomes ‘large,’ it would be quite difficult to list all the possible combinations.",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "mod-4-1-probability.html#combination-formula",
    "href": "mod-4-1-probability.html#combination-formula",
    "title": "20  Introduction to Probability",
    "section": "20.7 Combination Formula",
    "text": "20.7 Combination Formula\nThe number of combinations of \\(N\\) objects taken \\(n\\) at a time is equal:\n\\[C_n^N =  \\binom{N}{n} = \\frac{N!}{n!(N-n)!}\\] Returning to the example where there were four objects and one object is selected at a time without replacement, note that \\(N = 4\\) and \\(n = 1\\), we can calculate that there are 4 possible combinations that can be drawn.\n\\[C_1^4 =  \\binom{4}{1} = \\frac{4!}{1!(4-1)!} = \\frac{24}{6} = 4\\]\nReturning to the example where there were four objects and two objects were selected at a time without replacement, note that \\(N = 4\\) and \\(n = 2\\), we can calculate that there are 6 possible combinations that can be drawn.\n\\[C_2^4 =  \\binom{4}{2} = \\frac{4!}{2!(4-2)!} = \\frac{24}{4} = 6\\] Returning to the example where there were four objects and two objects were selected at a time without replacement, note that \\(N = 4\\) and \\(n = 3\\), we can calculate that there are 4 possible combinations that can be drawn.\n\\[C_3^4 =  \\binom{4}{3} = \\frac{4!}{3!(4-3)!} = \\frac{24}{6} = 4\\]",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "mod-4-1-probability.html#combinations-in-r",
    "href": "mod-4-1-probability.html#combinations-in-r",
    "title": "20  Introduction to Probability",
    "section": "20.8 Combinations in R",
    "text": "20.8 Combinations in R\nIn the following examples, we show how to calculate the factorial of a value in R, the manual calculation of combination without replacement, and using the choose function to calculate the combination without replacement.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "mod-4-1-probability.html#lottery-example",
    "href": "mod-4-1-probability.html#lottery-example",
    "title": "20  Introduction to Probability",
    "section": "20.9 Lottery Example",
    "text": "20.9 Lottery Example\nA good example is a lottery. Assume that the lottery requires you to pick six numbers from 1 to 70. The numbers are picked without replacement. In other words, there are 70 objects (\\(N\\)) and 6 objects (\\(n\\)) are selected without replacement.\nHow many possible outcomes are there in the sample space for this lottery? The example code below demonstrates how to calculate the number of outcomes in the sample space when there are 70 objects and 6 objects are selected without replacement. There are 131,115,985 possible combinations or outcomes in the sample space for this experiment.\nHowever, lotteries found that larger jackpots attracted more customers. How do you boost the jackpot? One method is to have players pick a number of balls from one set of numbers without replacement and then another ball from another set of numbers. This “megaball” or “powerball” increases the number of possible combinations.\nIn the Virginia lottery Megamillions game, you pick 5 balls from 70 balls and then one ball from a separate 25 balls. The number of possible outcomes is equal to the combination of 70 balls picking 5 without replacement and then 1 ball from 25 possible balls. As you can see in the example below, the number of potential combinations increases to 302,575,350.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "mod-4-1-probability.html#permutations",
    "href": "mod-4-1-probability.html#permutations",
    "title": "20  Introduction to Probability",
    "section": "20.10 Permutations",
    "text": "20.10 Permutations\nWhile a combination does not require the objects to be drawn in a specific order, a permutation requires a specific ordering of objects.\nOne way to think about permutations is “this, then that.” In other words, one object must be drawn before another object for each possible outcome. Think about throwing a fair six-sided die two times. A roll of 1 on the first throw and 2 on the second throw is different than a roll of 2 on the first roll and 1 on the second roll. The order of selection is important unlike combinations.\nLet \\(N\\) be the number of objects and \\(n\\) are the number of objects that are selected without replacement. The number of permutations of \\(N\\) objects taken \\(n\\) at a time is defined as:\n\\[\nP_n^N = n! \\binom{N}{n} = \\frac{N!}{(N-n)!}\n\\]\nBecause the ordering matters, the number of outcomes from an ordered experiment is higher than the number of outcomes in an unordered experiment. In other words \\(P_n^N \\ge C_n^N\\).\nTo see this, assume that there are four possible objects:\n\nBlack (B)\nGreen (G)\nRed (R)\nWhite (W)\n\nGiven four objects, assume that two objects are selected without replacement.\nCombinations (6): BG, BR, BW, GR, GW, RW\nPermutations (12): BG, BR, BW, GB, GR, GW, RB, RG, RW, WB, WG, WR\nIn the example code below, the number of combinations and permutations are calculated manually.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "mod-4-1-probability.html#permutation-example",
    "href": "mod-4-1-probability.html#permutation-example",
    "title": "20  Introduction to Probability",
    "section": "20.11 Permutation Example",
    "text": "20.11 Permutation Example\nIn the example code below, the number of combinations and permutations are calculated using the choose and permutations functions.\nThe combination and permutations functions will create a matrix of outcomes for the random experiment.\nThese matrices grow large quickly and it may be more efficient to manually code the combinations and permutations.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "mod-4-2-rules.html",
    "href": "mod-4-2-rules.html",
    "title": "21  Probability Rules",
    "section": "",
    "text": "21.1 Probability\nWe have already touched on the idea of probability.\nWe calculated the number of combinations of a lottery and one could ask what are the odds of having a winning lottery ticket in any given draw of the lottery. We have also discussed rolling a fair die and tossing a fair coin.\nProbabilities, in general, must adhere to some basic rules. Each outcome of a random experiment is assigned a probability, each assigned probability must lie on the [0,1] interval, and the sum of the probabilities across the outcomes of an experiment must equal 1.\nMore formally:",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Probability Rules</span>"
    ]
  },
  {
    "objectID": "mod-4-2-rules.html#probability",
    "href": "mod-4-2-rules.html#probability",
    "title": "21  Probability Rules",
    "section": "",
    "text": "For a random experiment with sample space \\(s\\), each outcome \\(o_i \\in s\\) must be assigned a probability of occurrence.\nThe probability assigned to an outcome in the sample space, \\(P(o_i)\\), must lie on the [0,1] interval, that is, \\(0 \\le P(o_i) \\le 1\\).\nGiven \\(n\\) outcomes in \\(s\\), the sum of probabilities must be equal to 1 or \\(\\sum_i^n P(o_i) = 1\\).",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Probability Rules</span>"
    ]
  },
  {
    "objectID": "mod-4-2-rules.html#subjective-assessment",
    "href": "mod-4-2-rules.html#subjective-assessment",
    "title": "21  Probability Rules",
    "section": "21.2 Subjective Assessment",
    "text": "21.2 Subjective Assessment\nThe quickest method of assigning probabilities is to subjectively assign to the outcomes in the sample space. For example, what is the probability of a random student at Old Dominion University having a height less than 60 inches, 70 inches, and 80 inches?\nWe don’t have data readily on hand to compute these probabilities. In this case, the best we can do is to assign our own evaluations of the probabilities of occurrence of each outcome.\nOf course, if three people are posed this question and assign their subjective probabilities, there is a strong likelihood that the subjective probabilities will not be in exact agreement. And, without data, we cannot determine which set of subjective probabilities are correct.",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Probability Rules</span>"
    ]
  },
  {
    "objectID": "mod-4-2-rules.html#equal-outcomes",
    "href": "mod-4-2-rules.html#equal-outcomes",
    "title": "21  Probability Rules",
    "section": "21.3 Equal Outcomes",
    "text": "21.3 Equal Outcomes\nAssume that we have a random experiment with \\(n\\) equally likely outcomes in the sample space.\nThe probability of any outcome occurring is \\(1/n\\).\nReturning to the 10-sided fair die, then the probability of any of the outcomes in the sample space is\n\\[P(s) = 1/10 = 0.1\\]\nDoes this adhere to the rules of assigning probabilities?\nFirst, each outcome in the sample space is assigned a probability of \\(1/10\\).\nSecond, the probability of any outcome in the sample space is \\(1/10\\) and we can state that \\(0 \\le 1/10 \\le 1\\).\nThird, if we sum the probabilities of each outcome across all outcomes then the resulting summmation is equal to 1 or \\(\\sum_1^{10} 1/10 = 1\\).",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Probability Rules</span>"
    ]
  },
  {
    "objectID": "mod-4-2-rules.html#relative-frequency",
    "href": "mod-4-2-rules.html#relative-frequency",
    "title": "21  Probability Rules",
    "section": "21.4 Relative Frequency",
    "text": "21.4 Relative Frequency\nAssume we have a random experiment with \\(n\\) outcomes in the sample space. We can compute the probability of each outcome using the proportion of occurrence of each outcome to the total number of occurrences. In other words, the probability of an outcome \\(o_i\\) is equal to ratio of the number of times \\(o_i\\) occurs relative to the total number of occurrences across all outcomes.\nIn the following code, we create a data frame with the reactions to the administration of a drug. We calculate the relative frequency of each type of reaction by:\n\\[\nP(Reaction_i) = \\frac{Cases_i}{sum_{i=1}^{N} Cases_i}\n\\]\nBased upon these relative frequencies, we would assign a probability of 21.8% to patients having no reaction to the administration of the drug. Likewise, we would assign a probability of 67.6% to patients having a mild reaction to the drug, and so on.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Probability Rules</span>"
    ]
  },
  {
    "objectID": "mod-4-2-rules.html#complement",
    "href": "mod-4-2-rules.html#complement",
    "title": "21  Probability Rules",
    "section": "21.5 Complement",
    "text": "21.5 Complement\nIn the previous example, the sample space consisted of four outcomes: no reaction (None), a mild reaction (Mild), a moderate reaction (Moderate), and a severe reaction (Severe).\nAn event is defined as a collection of outcomes in a sample space.\nFor example, if we defined an event as having any type of reaction, then the event space would consist of three outcomes: Mild, Moderate, and Severe.\nAn event can consist of one outcome in the sample space up to all the outcomes in the sample space. Continuing our example, if one event is having any type of reaction to the administration of the drug, then there is also another event: not having any reaction to the drug in question.\nWe thus have two events:\n\nHaving a reaction \\(E\\)\nNot having a reaction \\(E^c\\) where \\(c\\) denotes the complement.\n\nWe can more formally state this as:\n\nAn event, \\(E_i\\) consists of \\(n\\) outcomes in sample space \\(s\\) where \\(s\\) has \\(N\\) outcomes.\nThe number of outcomes for \\(E_i\\) is such that \\(1 \\le n \\le N\\).\nThe probability of \\(E_i\\) is equal to sum of probabilities of all the outcomes in \\(E_i\\) such that \\(P(E_i) = \\sum_{i=1}^{n} o_i\\).\nThe probability of all the outcomes not in \\(E_i\\) is equal to \\(P(E_i)^C = 1 - P(E_i) = 1 - \\sum_{i=1}^n o_i\\).\n\nIn the code below, we summarize the number of cases that had any reaction and calculate the relative frequency of the event. The probability of having any type of reaction to the administration of the drug is 78.2% and the complement, which is having no reaction to the administration of the drug, is \\(1 - 0.782 = 0.118\\) or 11.8%.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Probability Rules</span>"
    ]
  },
  {
    "objectID": "mod-4-2-rules.html#union",
    "href": "mod-4-2-rules.html#union",
    "title": "21  Probability Rules",
    "section": "21.6 Union",
    "text": "21.6 Union\nAssume that we have two events, A and B, where the two events are part of the sample space \\(s\\) of a random experiment.\nIf we combine the two events, that is, we create the union of the two events.\nThe union of A and B is an event containing all the sample outcomes of A and B and is:\n\\[\nA \\cup B\n\\]\nLet’s return to the 10-sided die experiment.\nAssume that event A consists of the outcomes \\(A \\in \\{1, 2, 3 \\}\\) and B is an event consisting of the outcomes \\(B \\in \\{9, 10 \\}\\).\nThe union of A and B, \\(A \\cup B\\) is an event consisting of the outcomes \\(A \\cup B \\in \\{1, 2, 3, 9, 10 \\}\\).\nConsider the same experiment where \\(C \\in \\{4, 6, 8 \\}\\) and \\(D \\in \\{4, 5, 8, 10 \\}\\).\nWhat is the union of \\(C\\) and \\(D\\)?\n\\[\nC \\cup D \\in \\{4, 5, 6, 8, 10\\}\n\\]",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Probability Rules</span>"
    ]
  },
  {
    "objectID": "mod-4-2-rules.html#intersection",
    "href": "mod-4-2-rules.html#intersection",
    "title": "21  Probability Rules",
    "section": "21.7 Intersection",
    "text": "21.7 Intersection\nAssume that we have two events, C and D, where the two events are part of the sample space \\(s\\) of a random experiment.\nIf we select only those outcomes that occur in each event, that is, we create the intersection of the two events.\nThe intersection of C and D is an event containing the sample outcomes belonging to both C and D and is denoted as:\n\\[\nC \\cap D\n\\]\nRecall from the previous discussion of the union of two events that \\(C \\in \\{4, 6, 8 \\}\\) and \\(D \\in \\{4, 5, 8, 10 \\}\\).\nThe union of C and D is:\n\\[\nC \\cup D \\in \\{4, 5, 6, 8, 10\\}\n\\] The intersection of C and D is\n\\[C \\cap D \\in \\{4, 8 \\}\\]",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Probability Rules</span>"
    ]
  },
  {
    "objectID": "mod-4-2-rules.html#mutually-exclusive-events",
    "href": "mod-4-2-rules.html#mutually-exclusive-events",
    "title": "21  Probability Rules",
    "section": "21.8 Mutually Exclusive Events",
    "text": "21.8 Mutually Exclusive Events\nThe addition law provides a method of computing the probability of the union of two events. First, note that the probability of the union of two events is not equal to the sum of the probabilities of each event unless the two events are mutually exclusive, that is, there are no outcomes in common between the two events.\nIn other words, if \\(A \\cap B \\in 0\\) then \\(P(A \\cup B) = P(A) + P(B)\\).\nIntuitively, if two elements contain no elements in common, then the probability of the two events occurring is equal to the sum of the probability of two events.\nAssume a 52-card deck of cars. Let A consist of all the Kings in the deck while B consists of all the Queens.\n\\[A \\in \\{K_c, K_d, K_h, K_s \\}\\]\n\\[B \\in \\{Q_c, Q_d, Q_h, Q_s \\}\\]\nThere are no elements in common between the two events, that is,\n\\[A \\cap B = 0\\]\nGiven that A and B are mutually exclusive\n\\[$P(A) = 4/52 = 0.077\\]\n\\[P(B) = 4/52 = 0.077\\]\n\\[P(A \\cup B) = P(A) + P(B) = 0.077 + 0.077 = 0.154\\]",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Probability Rules</span>"
    ]
  },
  {
    "objectID": "mod-4-2-rules.html#addition-rule",
    "href": "mod-4-2-rules.html#addition-rule",
    "title": "21  Probability Rules",
    "section": "21.9 Addition Rule",
    "text": "21.9 Addition Rule\nIf two events are not mutually exclusive, then the probability of the union of the two events is equal to:\n\\[\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B)\n\\]\nSince A and B share outcomes in common, then the probability of either event occurring is equal to the probability of each event occurring minus the probability that of A and B occurring. The subtraction of the intersection of the event avoids double-counting.\nReturning to the 52-card deck example, what is the probability that a card is either a King or a Heart? There are 4 Kings in a deck and 13 Hearts in a deck. 1 of the 13 hearts in the deck is a King.\n\\[P(K)        = 4/52\\]\n\\[P(H)        = 13/52\\]\n\\[P(K \\cap H) = 1/52\\]\n\\[P(K \\cup H) = P(K) + P(H) - P(K \\cap H)\\]\n\\[P(K \\cup H) = 4/52 + 13/52 - 1/52 = 16/52 = 4/13 = 0.308\\]\nThe probability of drawing a King or a Heart from a 52-card deck is 0.308 or 30.8%.",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Probability Rules</span>"
    ]
  },
  {
    "objectID": "mod-4-3-conditional.html",
    "href": "mod-4-3-conditional.html",
    "title": "22  Conditional Probability",
    "section": "",
    "text": "22.1 Introduction\nGiven a standard 52-card deck, we know the probability of drawing a King from a 52-card deck is:\n\\[P(K) = 4/52 = 1/13 = 0.077\\]\nThis is an unconditional probability which is the likelihood of an event occurring without considering any other events.\nWhat if, however, you were interested in the probability of an event occurring given that another event had already occurred?\nWhat if you were interested in knowing the likelihood of drawing a King given that you had already drawn an Ace?\nWhat is the probability of hitting a single given that a base runner is on third-base with two-outs in the bottom of the 9th-inning?\nThese are examples of conditional probability.\nWhat is the probability of event A occurring given that event B has already taken place?\nWe use the following notation to denote the conditional probability of A given B or:\n\\[\nP(A | B)\n\\]",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "mod-4-3-conditional.html#conditional-probability",
    "href": "mod-4-3-conditional.html#conditional-probability",
    "title": "22  Conditional Probability",
    "section": "22.2 Conditional Probability",
    "text": "22.2 Conditional Probability\nThe conditional probability of A given B, denoted \\(P(A|B)\\), is the probability that event A has occurred in a trial of a random experiment for which it is known that event B has definitely occurred. It may be computed by means of the following:\n\\[\nP(A|B) = \\frac{P(A \\cap B)}{P(B)}\n\\]\nFor example, assume that a fair die is rolled and you are asked to give the probability that the roll resulted in a five. Invariably, you would answer that the probability of rolling a 5 was \\(1/6\\) because you know that \\(s \\in \\{1, 2, 3, 4, 5, 6 \\}\\) and each of the outcomes are equally likely or:\n\\[\nP(5) = 1/6 = 0.167\n\\]\nWhat would occur, however, if you were provided information that the roll of the fair die resulted in an odd number prior to your assessment?\nYou would no longer state that the probability of rolling a 5 was \\(1/6\\) because you know that the outcomes are \\(\\{1, 3, 5 \\}\\). Given the outcomes are equally likely, you should assess the probability of rolling a 5 given the roll resulted in an odd number as \\(1/3\\).\n\\[F = \\{5\\} \\quad O=\\{1,3,5\\}\\]\n\\[F \\cap O = \\{5\\} \\cap \\{1,3,5\\} = \\{5\\}\\]\n\\[P(O) = 3/6 \\quad P(F \\cap O) = 1/6\\]\n\\[\nP(Five | Odd) = P(F | O) = \\frac{P(F \\cap O)}{P(O)} = \\frac{1/6}{3/6} = 1/3\n\\]\nNow, let’s reverse the problem.\nWhat is the probability of rolling an odd number given that we have rolled a five? Intuitively, we know that if we have already rolled an odd number, then the probability of one of those numbers being 5 is one.\n\\[\nP(Odd | Five) = P(O | F) = \\frac{P(O \\cap F)}{P(F)} = \\frac{1/6}{1/6} = 1\n\\]",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "mod-4-3-conditional.html#conditional-probability-example",
    "href": "mod-4-3-conditional.html#conditional-probability-example",
    "title": "22  Conditional Probability",
    "section": "22.3 Conditional Probability Example",
    "text": "22.3 Conditional Probability Example\nIn the following example, data are presented for the administration of two drugs. After administration of each drug, individuals are observed and any reactions to the drug are recorded for analysis.\n\nA = event when drug Adival (A) is administered\nB = event when drug Beritol (B) is administered\nAb = event where there is no adverse reaction\nMi = event where there is a mild adverse reaction\nMo = event where there is a moderate adverse reaction\nSe = event where there is a severe adverse reaction\n\n\nrm(list = ls())\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(kableExtra)\nlibrary(tidyverse)\n\n#Create Data Frame with Example Data\n#Use janitor to add totals\n\nlibrary(janitor)\n\nreact &lt;- tibble(drug  = rep(c(\"Adival (A)\",\n                              \"Beritol (B)\"), each = 4),\n                event = rep(c(\"Absent\",  \n                              \"Mild\",\n                              \"Moderate\",\n                              \"Severe\"), times = 2),\n                cases = c(4530, 2485, 1600, 875,\n                          3683, 3125, 2745, 145)) %&gt;%  \n        arrange(drug, event) \n\ndrug_wide &lt;- react %&gt;%\n  pivot_wider(id_cols     = drug,\n              names_from  = event,\n              values_from = cases) %&gt;%\n  adorn_totals(c(\"row\",\"col\"))\n  \nkable(drug_wide,\n      align       = \"lccccc\", \n      format.args = list(big.mark = \",\", scientific = FALSE),\n      caption     = \"Drug Reaction Table\",\n      col.names   = c(\"Drug\", \"None\", \"Mild\", \"Moderate\", \"Severe\", \"Total\")) %&gt;%\nkable_classic(full_width = TRUE)\n\n\nDrug Reaction Table\n\n\nDrug\nNone\nMild\nModerate\nSevere\nTotal\n\n\n\n\nAdival (A)\n4,530\n2,485\n1,600\n875\n9,490\n\n\nBeritol (B)\n3,683\n3,125\n2,745\n145\n9,698\n\n\nTotal\n8,213\n5,610\n4,345\n1,020\n19,188\n\n\n\n\n\nSuppose that the proportions reflected in the same data in the table reflect those in the population who received either drug. If we select a person at random:\n\nWhat is the probability that the individual selected received drug A?\nWhat is the probability the individual had a mild adverse reaction.\nWhat is the probability that the individual had a mild reaction, given the person received drug A?\n\nThe first two questions ask about the probability of an event. The last question asks about the conditional probability of an event.\n\nGiven 9,490 individuals received drug A, then \\(P(A)\\) is equal to:\n\n\\[P(A) = 9490/19188 = 0.495\\]\n\n5,610 individuals had a mild adverse reaction, then \\(P(Mi)\\) is equal to:\n\n\\[P(Mi) = 5610/19188 = .292\\]\n\n9,490 individuals who received drug A and 2,485 had a mild reaction, then \\(P(Mi | A)\\) is:\n\n\\[P(Mi | A) = 2485/9490 = 0.262\\] ## Joint and Marginal Probabilities\nUsing the table below, we note that the interior (defined by the intersection of the drug and the reaction) are joint probabilities.\nFor example, the joint probability of administering Beritol (B) and having a mild reaction (Mi) is 0.163.\n\\[\nP(B \\cap Mi) = 0.163\n\\]\nThe probabilities contained in the “Total” rows are the marginal probabilities.\nFor example, the marginal probability of having no reaction to the drug (None) is equal to\n\\[P(None) = P(A \\cap None) + P(B \\cap None) = 0.236 + 0.192 = 0.428\\]\nAn easy rule to remember is that the joint probabilities are the interior of the joint probability table while the marginal probabilities are along the exterior (margins) of the joint probability table.\n\nrm(list = ls())\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(kableExtra)\nlibrary(tidyverse)\n\n#Create Data Frame with Example Data\n#Use janitor to add totals\n\nlibrary(janitor)\n\nreact &lt;- tibble(drug  = rep(c(\"Adival (A)\",\n                              \"Beritol (B)\"), each = 4),\n                event = rep(c(\"Absent\",  \n                              \"Mild\",\n                              \"Moderate\",\n                              \"Severe\"), times = 2),\n                cases = c(4530, 2485, 1600, 875,\n                          3683, 3125, 2745, 145)) %&gt;%  \n        arrange(drug, event) \n\nreact_freq &lt;- react %&gt;%\n  mutate(rel_freq = cases / sum(cases)) %&gt;%\n  pivot_wider(id_cols     = drug,\n              names_from  = event,\n              values_from = rel_freq) %&gt;%\n  adorn_totals(c(\"col\", \"row\"))\n\nkable(react_freq,\n      align       = 'c',\n      digits      = 3,\n      format.args = list(big.mark = \",\", scientific = FALSE),\n      caption     = \"Drug Reaction Relative Frequency Table\",\n      col.names   = c(\"Drug\", \"None\", \"Mild\", \"Moderate\", \"Severe\", \"Total\")) %&gt;%\nkable_classic(full_width = TRUE)\n\n\nDrug Reaction Relative Frequency Table\n\n\nDrug\nNone\nMild\nModerate\nSevere\nTotal\n\n\n\n\nAdival (A)\n0.236\n0.130\n0.083\n0.046\n0.495\n\n\nBeritol (B)\n0.192\n0.163\n0.143\n0.008\n0.505\n\n\nTotal\n0.428\n0.292\n0.226\n0.053\n1.000",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "mod-4-3-conditional.html#multiplication-law",
    "href": "mod-4-3-conditional.html#multiplication-law",
    "title": "22  Conditional Probability",
    "section": "22.4 Multiplication Law",
    "text": "22.4 Multiplication Law\nRecall that the addition law is used to compute the probability of the union of two events that are not mutually exclusive, that is:\n\\[\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B)\n\\]\nGiven a 52 card deck, then for a single draw of a card, the probability of drawing a Jack or a Spade is:\n\\[P(Jack \\cup Spade) = P(Jack) + P(Spade) - P(Jack \\cap Spade)\\]\n\\[P(Jack \\cup Spade) = 4/52 + 13/52 - 1/52 = 16/52 = 0.308\\]\nAlso, recall that conditional probability is\n\\[\nP(A | B) = \\frac{P(A \\cap B)}{P(B)}\n\\]\nSolving conditional probability for \\(P(A \\cap B)\\) yields the multiplication law for events that are not mutually exclusive:\n\\[\nP(A \\cap B) = P(B) P(A|B)\n\\]\nLikewise\n\\[\nP(B \\cap A) = P(A) P(B|A)\n\\]",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "mod-4-3-conditional.html#conditional-probability-example-1",
    "href": "mod-4-3-conditional.html#conditional-probability-example-1",
    "title": "22  Conditional Probability",
    "section": "22.5 Conditional Probability Example",
    "text": "22.5 Conditional Probability Example\nAnother way to think of conditional probability is through the use of a tree diagram.\nAssume there are 6 blue marbles and 4 red marbles in a bag. On the first draw from the bag:\n\nP(Red) = 4/10\nP(Blue) = 6/10\n\nNow, given that you have drawn a red marble, what is the probability of drawing another red marble? If we draw without replacement, there are now 9 marbles in bag. From the 9 marbles, we know that\n\nP(Red) = 3/9\nP(Blue) = 6/9\n\nSo, we can now construct the probability of drawing a red marble on the 2nd draw given that the first draw was red.\n\nP(Red | Red) = 4/10 * 3/9 = 0.133\nP(Blue | Red) = 4/10 * 6/9 = 0.267\n\nWe can apply the same logic to determine the probability of drawing a red given that we have already drawn a blue, or\n\nP(Red | Blue) = 6/10 * 4/9 = 0.267\nP(Blue | Blue) = 6/10 * 5/9 = 0.333\n\nNote that when we add all the conditional probabilities together, the sum is equal to 1.",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "mod-4-3-conditional.html#independent-events",
    "href": "mod-4-3-conditional.html#independent-events",
    "title": "22  Conditional Probability",
    "section": "22.6 Independent Events",
    "text": "22.6 Independent Events\nIn the previous example, each time we drew a marble, we did not replace it in the bag. The probabilities of the second draw were dependent on the first draw.\nWhat would occur if we replaced the marble?\nWhat is the probability of drawing a red marble on the second draw after drawing a red marble on the first draw and putting the drawn ball back in the bag?\nSince the marble is replaced, the probability of drawing a red marble on any draw is 4/10. So, the probability of drawing red given red is:\n\nP(Red | Red) = 4/10 * 4/10 = 16/100 = 0.16\nP(Blue | Red) = 4/10 * 6/10 = 24/100 = 0.24\n\nWhich leads to the multiplication law for independent events:\n\\[\nP(A \\cap B) = P(A) \\times P(B)\n\\]",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "mod-4-3-conditional.html#bayes-theorem",
    "href": "mod-4-3-conditional.html#bayes-theorem",
    "title": "22  Conditional Probability",
    "section": "22.7 Bayes’ Theorem",
    "text": "22.7 Bayes’ Theorem\nBayes’ Theorem is a method of using known probabilities and new information to obtain updated probabilities. Bayes’ Theorem is:\n\\[\nP(A|B) = \\frac{P(A)P(B|A)}{P(B)}\n\\]\nIn other words, the conditional probability of A given B is determined by the probability of A, the conditional probability of B given A, and the probability of B.",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "mod-4-3-conditional.html#bayes-theorem-example",
    "href": "mod-4-3-conditional.html#bayes-theorem-example",
    "title": "22  Conditional Probability",
    "section": "22.8 Bayes’ Theorem Example",
    "text": "22.8 Bayes’ Theorem Example\nAssume the following:\n\nP(Fire) be the probability of fire occurring\nP(Smoke) the probability of Smoke occurring\nP(Fire | Smoke) be the probability of fire given there is smoke\nP(Smoke | Fire) be the probability of smoke given there is fire.\nP(Fire) = 0.01\nP(Smoke) = 0.10\nP(Smoke | Fire) = 0.90\n\nWhat this tells us is that fire is relatively rare, smoke is more common than fire, and that if there is a fire, there is a high probability of smoke.\nWe can use Bayes’ Theorem to find that the probability of fire when there is smoke is 9%.\n\\[\nP(Fire|Smoke) = \\frac{P(F)P(S|F)}{P(S)} = \\frac{0.01 \\times 0.9}{0.1} = 0.09\n\\]",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "mod-4-3-conditional.html#bayes-theorem-example-1",
    "href": "mod-4-3-conditional.html#bayes-theorem-example-1",
    "title": "22  Conditional Probability",
    "section": "22.9 Bayes’ Theorem Example",
    "text": "22.9 Bayes’ Theorem Example\nAssume that you are trying to determine whether you have a false positive diagnostic test.\n\nFor individuals who have the disease, the test registers a positive value 90% of the time.\nFor individuals who do not have the disease, the test registers a positive value 5% of the time.\n\nIf 3% of the population are currently infected with the disease, and your test returns a positive value, what is the probability that you actually have the disease?\n\nP(Disease) = 0.03\nP(Positive | Disease) = 0.90\nP(Positive) must be determined given the information on hand.\n\nWe know that 3% of population has the disease.\nThis implies 97% of the population does not have the disease.\nWe know the test will return a positive value for 90% of the infected individuals.\nWe know the test will return a positive value for 5% of the uninfected individuals.\nTo estimate the probability of a positive test:\n\\[\nP(Positive) = P(Infected \\cap Positive) + P(Not Infected \\cap Positive)\n\\]\n\\[\nP(Positive) = (0.03 \\times 0.9) + (0.97 \\times 0.05) = 0.0755\n\\]\nWe know know all the elements.\nBayes’ Theorem:\n\\[\nP(A|B) = \\frac{P(A)P(B|A)}{P(B)}\n\\]\n\\[\nP(Disease | Positive) = \\frac{P(D)P(P|D)}{P(P)}\n\\]\n\\[\nP(Disease | Positive) = \\frac{P(D)P(P|D)}{P(P)} = \\frac{0.03 \\times 0.9}{0.0755} = 0.358\n\\]\nIn other words, if you receive a positive test result, there is a 35.8% chance you have the disease.",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "mod-4-3-conditional.html#smallpox-vs-chickenpox",
    "href": "mod-4-3-conditional.html#smallpox-vs-chickenpox",
    "title": "22  Conditional Probability",
    "section": "22.10 Smallpox vs Chickenpox",
    "text": "22.10 Smallpox vs Chickenpox\nA patient arrives with spots that appear to be smallpox. It could also be chickenpox.\n\nThe probability a person with smallpox has spots is 0.9.\nThe probability a person with chickenpox has spots is 0.8.\nThe probability that someone in the population has spots is 0.081.\nThe probability that someone in the population has smallpox is 0.0011 (prior probability)\nThe probability that someone in the population has chickenpox is 0.1 (prior probability)\n\n\\[P(symptoms | chickenpox) = 0.8\\]\n\\[P(symptoms | smallpox) = 0.9\\]\n\\[P(chickenpox) = 0.1\\]\n\\[P(smallpox) = 0.0011\\] \\[P(spots) = 0.081\\]\nThe posterior probabilities are defined as:\n\\[\\frac{P(symptoms|smallpox) \\quad P(smallpox)}{P(symptoms)}\\]\nPosterior Probability of Smallpox\n\\[\\frac{0.9 \\times 0.001}{0.081} = 0.011\\]\nPosterior probability of chickenpox\n\\[\\frac{0.8 \\times 0.1}{0.081} = 0.977\\]\nWhile a patient presenting with symptoms appears to be more consistent with smallpox, the background rate of chickenpox in the population is higher than smallpox, which makes it more likely that the patient has chickenpox.",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "mod-4-3-conditional.html#cancer-example",
    "href": "mod-4-3-conditional.html#cancer-example",
    "title": "22  Conditional Probability",
    "section": "22.11 Cancer Example",
    "text": "22.11 Cancer Example\nAssume that 1% of women over 50 have breast cancer.\n90% of women who have breast cancer test positive on mammograms.\n8% of women who do not have breast cancer test positive on mammograms.\nWe would like to know what is the probability of having breast cancer given a positive test.\nDefine B = breast cancer, A = positive test\nRecall Bayes’ Theorem is:\n\\[P(A|B) = \\frac{P(A) \\, P(B|A)}{P(B)}\\]\n\\[P(B|A) = \\frac{P(A|B) \\, P(B)}{P(A)} = \\frac{P(B \\cap A)}{P(A)}\\]\n\\[P(B|A) = \\frac{P(A|B) \\, P(B)}{P(A)} = \\frac{P(B \\cap A)}{P(A)}\\]\nWe now need to determine \\(P(B \\cap A)\\) or\n\\[P(B \\cap A) = P(A | B) \\times P(B) = 0.9 \\times 0.01 = 0.009\\]\n\\[P(B|A) = \\frac{P(A|B) \\, P(B)}{P(A)} = \\frac{P(B \\cap A)}{P(A)} = \\frac{0.009}{0.0982} = 0.09\\]\nIn other words, if a woman has a positive test result, she has a 9% probability of having cancer.\nWhat would happen if the false positive rate was lowered to 1%?\n\nAll the information remains the same except \\(P(A | B^C) = 0.01\\).\nWhat happens to the probability of having a positive test?\n\n\\[P(A) = P(B \\cap A) + P(B^C \\cap A)\\]\n\\[P(B \\cap A) = P(B) \\times P(A | B) = 0.01 \\times 0.9 = 0.009\\]\n\\[P(B^C \\cap A) = P(B^C) \\times P(A | B^C) = 0.99 \\times 0.01 = 0.0099\\]\n\\[P(A) = P(B \\cap A) + P(B^C \\cap A) = 0.009 + 0.0099 = 0.0189\\]\nWe can now see what happens\n\\[P(B|A) = \\frac{P(A|B) \\, P(B)}{P(A)} = \\frac{P(B \\cap A)}{P(A)} = \\frac{0.009}{0.0189} = 0.476\\]\nLowering the rate of false positives increases the probability that, if you have a positive test, that you have breast cancer.",
    "crumbs": [
      "Introduction to Probability",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Conditional Probability</span>"
    ]
  },
  {
    "objectID": "mod-5-0-overview.html",
    "href": "mod-5-0-overview.html",
    "title": "23  Module 5 Overview",
    "section": "",
    "text": "23.1 Introduction\nWelcome to Module 5 of ECON 700.\nThis module introduces students to the foundational concepts of probability and their application in data-driven analysis. Students begin by exploring the difference between discrete and continuous random variables, followed by the construction and interpretation of probability distributions. Using examples such as coin tosses, dice rolls, and manufacturing outcomes, the module develops an understanding of how randomness, expected value, and variance describe uncertainty and predict long-run behavior. Students will also examine empirical probability distributions using simulated and real-world data in R.\nBuilding on these principles, the module introduces the binomial and Poisson distributions—two key models for analyzing the likelihood of events within defined conditions. Learners use R to calculate and visualize probability mass functions, cumulative distribution functions, and inverse functions, as well as to simulate random variables. By the end of the module, students will understand how probability distributions underpin inferential statistics and decision-making in economics and data analysis.",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Module 5 Overview</span>"
    ]
  },
  {
    "objectID": "mod-5-0-overview.html#learning-objectives",
    "href": "mod-5-0-overview.html#learning-objectives",
    "title": "23  Module 5 Overview",
    "section": "23.2 Learning Objectives",
    "text": "23.2 Learning Objectives\nBy the end of this module, you should be able to:\n\nMLO 1: Differentiate between discrete and continuous random variables. (CLO 3)\nMLO 2: Construct and interpret discrete probability distributions. (CLO 3)\nMLO 3: Calculate expected value, variance, and standard deviation for discrete random variables. (CLO 3)\nMLO 4: Apply the binomial and Poisson probability functions to real-world random processes. (CLO 3)\nMLO 5: Use R to compute probability mass, cumulative, and inverse distribution functions. (CLO 3, 5)\nMLO 6: Evaluate how probability models support decision-making in economics. (CLO 1, 3, 5)",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Module 5 Overview</span>"
    ]
  },
  {
    "objectID": "mod-5-0-overview.html#required-texts",
    "href": "mod-5-0-overview.html#required-texts",
    "title": "23  Module 5 Overview",
    "section": "23.3 Required Texts",
    "text": "23.3 Required Texts\nThe following textbooks are available for free. Please select the links provided to access.\n\nBarbara Illowsky and Susan Dean. Introductory Statistics 2e. OpenStax. Chapter 4.\nRafael Irizarry. (2025). Introduction to Data Science: Data Wrangling and Visualization with R. Chapters 1-2.\nHadley Wickham, Mine Cetinkaya-Rundel, and Garrett Grolemund. (2025) R for Data Science. Sections 1-19.\nOnline R documentation\nRStudio Cheat Sheets",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Module 5 Overview</span>"
    ]
  },
  {
    "objectID": "mod-5-0-overview.html#activities",
    "href": "mod-5-0-overview.html#activities",
    "title": "23  Module 5 Overview",
    "section": "23.4 Activities",
    "text": "23.4 Activities\n\nComplete the hands-on coding exercises embedded in each lesson (MLO 5)\n\nComplete the weekly class assignment (MLO 1- MLO 6)\n\nParticipate in the discussion form (MLO 4 and MLO 5)\n\nTake the weekly knowledge quiz (MLO 1 - MLO 6)",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Module 5 Overview</span>"
    ]
  },
  {
    "objectID": "mod-5-0-overview.html#lessons-in-this-module",
    "href": "mod-5-0-overview.html#lessons-in-this-module",
    "title": "23  Module 5 Overview",
    "section": "23.5 Lessons in this Module",
    "text": "23.5 Lessons in this Module\n\n5.1 – Discrete Random Variables\n5.2 – Binomial Probability Distribution\n5.3 – Poisson Probability Distribution\n\n\nNext: Start with **Discrete Random Variables*.",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Module 5 Overview</span>"
    ]
  },
  {
    "objectID": "mod-5-1-discrete.html",
    "href": "mod-5-1-discrete.html",
    "title": "24  Discrete Random Variables",
    "section": "",
    "text": "24.1 Discrete Random Variables\nA random variable is a numerical description of an outcome of an experiment.\nA discrete random variable may assume a finite or infinite number of values.\nA fair coin flip or roll of a six-sided die is a random experiment with a finite number of outcomes.\nFor the coin flip, the random variable is the face of the coin, the values of which are: heads (1) and tails (0).\nFor the six-sided die, the random variable is the number of dots showing on the die, the values of which are: 1,2,3,4,5,6.\nIf the random experiment is operating a website for a 24-hour period, the random variable is the number of unique visitors to the website.\nThe values of the random variable are: 0, 1, 2, 3 ……",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "mod-5-1-discrete.html#continuous-random-variables",
    "href": "mod-5-1-discrete.html#continuous-random-variables",
    "title": "24  Discrete Random Variables",
    "section": "24.2 Continuous Random Variables",
    "text": "24.2 Continuous Random Variables\nA continuous random variable may assume any value in an interval or collection of intervals.\nIf we select 1,000 students at random who come to ODU today, the driving distance is bounded below at 0.\nThe driving distance is practically bounded above (maybe 200 miles?).\nDriving distance is a continuous random variable as it can take on any value from 0 to infinity (and beyond!).\nIf the random experiment is a visit to a webpage by a customer, the random variable is the time spent on the page, with values of the random variable being \\(x \\ge 0\\).\nIf the random experiment is to fill a kiloliter container, the random variable is the amount of liquid in the container, with values of the random variable being \\(0 \\le x \\le 1\\, kl\\).",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "mod-5-1-discrete.html#discrete-probability-distributions",
    "href": "mod-5-1-discrete.html#discrete-probability-distributions",
    "title": "24  Discrete Random Variables",
    "section": "24.3 Discrete Probability Distributions",
    "text": "24.3 Discrete Probability Distributions\nThe probability distribution for a random variable describes how probabilities are distributed over the values of the random variable.\nGiven a discrete random variable, \\(x\\), a probability function, \\(f(x)\\), provides the probability for each value of the random variable.\nThe classical method of assigning probabilities of the values of a random variable is used when the experimental outcomes generate values of the random variable that are equally likely.\nConsider the experiment of rolling a six-sided die.\n\nWe know the sample space is \\(s = \\{1, 2, 3, 4, 5, 6\\}\\)\nLet \\(x\\) be the number obtained on one throw of the die\nLet \\(f(x)\\) be the probability of \\(x\\)\nWe can represent the probability of \\(x\\) with a frequency table or graph.\n\nIn the code below, we run a simulated six-sided die 10,000 times.\n\nThe sample function takes a sample of the sequence of integers from 1 to 6\nThe size option sets the number of samples to be taken\nThe replace = TRUE replaces the number sampled\nThe group_by identifies the bins\nThe summarize command estimates the frequencies and relative frequencies\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "mod-5-1-discrete.html#frequency-tables",
    "href": "mod-5-1-discrete.html#frequency-tables",
    "title": "24  Discrete Random Variables",
    "section": "24.4 Frequency Tables",
    "text": "24.4 Frequency Tables\nThe relative frequency method is applicable when the data are “reasonably” large.\nThe discrete probability distribution that is developed using the relative frequency method is known as the empirical discrete distribution.\nTwo conditions are required for a discrete probability distribution.\n\\[f(x) \\ge 0\\]\n\\[\\sum f(x) = 1\\] In the code below, we replicate the experiment of throwing a six-sided die 10,000 times. We estimate the relative frequency for each of the outcomes of the experiment and create a relative frequency table that mirrors the graph.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "mod-5-1-discrete.html#discrete-probability-distribution",
    "href": "mod-5-1-discrete.html#discrete-probability-distribution",
    "title": "24  Discrete Random Variables",
    "section": "24.5 Discrete Probability Distribution",
    "text": "24.5 Discrete Probability Distribution\nA formula that gives the probability function, \\(f(x)\\) for every value of \\(x\\) is known as a discrete probability distribution.\nUsing data, we developed the empirical discrete probability function for a fair six-sided die.\nLet \\(n\\) be equal to the number of values of that the random variable can take.\nThe discrete uniform probability distribution is then defined as:\n\\[f(x) = \\frac{1}{n}\\]\nNote that we can apply the discrete uniform probability distribution function to several of our previous examples.\nFlipping a fair coin: \\[f(x) = \\frac{1}{2}\\]\nRolling a fair six-sided die: \\[f(x) = \\frac{1}{6}\\]",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "mod-5-1-discrete.html#expected-value",
    "href": "mod-5-1-discrete.html#expected-value",
    "title": "24  Discrete Random Variables",
    "section": "24.6 Expected Value",
    "text": "24.6 Expected Value\nThe expected value of a random variable is a measure of the central tendency of the random variable.\nThe expected value of \\(x\\) is defined as\n\\[E(x) = \\mu = \\sum x \\, f(x)\\]\nThe expected value of a random variable is essentially a weighted average.\nEach value of the random variable, \\(x\\), is multiplied by the probability that the random variable takes that value or \\(p(x_i)\\). The \\(x_i \\times p(x_i)\\) are then summed over all possible values.\nThe expected value of a random variable can also be interpreted as the long-run value of the random variable. If we repeat the random experiment ‘many’ times and take the average of all the outcomes, we would obtain the expected value of the random variable.\nThe expected value, as it is the mean of the random variable, is also the measure of central tendency of the random variable and, as such, represents the center of mass of the probability mass function.",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "mod-5-1-discrete.html#expected-value-example",
    "href": "mod-5-1-discrete.html#expected-value-example",
    "title": "24  Discrete Random Variables",
    "section": "24.7 Expected Value Example",
    "text": "24.7 Expected Value Example\nAssume that you have a game where you can win various prizes with the following probabilities:\n\n$0 with \\(P(X = 0) = 0.1\\)\n$1 with \\(P(X = 1) = 0.15\\)\n$2 with \\(P(X = 2) = 0.4\\)\n$3 with \\(P(X = 3) = 0.25\\)\n$4 with \\(P(X = 4) = 0.1\\)\n\nWhat is the expected value of the game? In other words, if the game were played ‘many’ times, what would be the average expected gross winnings?\n\\[\nE(X) = \\sum_{i=1}^{5} x_i f(x_i)\n\\]\n\\[\nE(X) = (0 \\times 0.1) + (1 \\times 0.15) + (2 \\times 0.4) + (3 \\times 0.25) + (4 \\times 0.1)\n\\]\n\\[\nE(X) = 2.1\n\\]\nThe expected value of the game is $2.1. On average, over repeated trials, one would expect to win $2.1 from this game. If the game cost less than $2.1 per play, it would make sense to play the game.",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "mod-5-1-discrete.html#expected-value-example-1",
    "href": "mod-5-1-discrete.html#expected-value-example-1",
    "title": "24  Discrete Random Variables",
    "section": "24.8 Expected Value Example",
    "text": "24.8 Expected Value Example\nLet’s return to our experiment where we simulate rolling a fair six-sided die.\nWe increase the number of simulated rolls in this experiment to 1,000,000.\nWe estimate the expected value as:\n\\[\nEV(X) = \\sum_{i=1}^6 x_i f(x_i)\n\\]\nWe produce the table of frequencies and relative frequencies as well as a histogram.\nThe expected value of our experiment mirrors that obtained manually.\n\n1 with \\(P(X = 1) = 0.167\\)\n2 with \\(P(X = 2) = 0.167\\)\n3 with \\(P(X = 3) = 0.167\\)\n4 with \\(P(X = 4) = 0.167\\)\n5 with \\(P(X = 5) = 0.167\\)\n6 with \\(P(X = 6) = 0.167\\)\n\n\\[\nE(X) = (1 \\times 0.167) + (2 \\times 0.167) + (3 \\times 0.167) \\\\+ (4 \\times 0.167) + (5 \\times 0.167) + (6 \\times 0.167)\n\\]\n\\[\nEV = 3.5\n\\]\n\nrm(list = ls())\n\nset.seed(1234)\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(janitor)\nlibrary(kableExtra)\nlibrary(tidyverse)\n\nthrows = 1000000\n\nrolls &lt;- tibble(throw = sample(1:6, \n                               size = throws, \n                               replace = TRUE)) %&gt;%\n  group_by(throw) %&gt;%\n  summarize(freq     = n(),\n            rel_freq = n()/throws) %&gt;%\n  mutate(xfx = throw * rel_freq) \n\n\nkable(rolls %&gt;%  \n      adorn_totals(where = \"row\", name = \"Sums\"),\n      align       = \"lcccc\", \n      digits      = 3,\n      format.args = list(big.mark = \",\", \n                         scientific = FALSE),\n      caption     = \"Rolls of Six-Sided Die\",\n      col.names   = c(\"Number (x)\", \"Frequency of X\", \n                      \"Probability of X - f(x)\", \"x * f(x)\")) %&gt;%\nkable_classic(full_width = TRUE)\n\nggplot(data = rolls,\n        aes(x = throw,\n            y = rel_freq,\n            fill = throw)) +\ngeom_bar(stat = 'identity') +\ngeom_text(data  = rolls,\n          aes(x = throw,\n              y = rel_freq,\n              label = scales::label_number(a=0.01)(rel_freq)),\n          nudge_y   = 0.01) +\ntheme_minimal() +\ntheme(legend.position = \" \") +\nlabs(title    = \"Throws of a Six Sided Die\",\n     x        = \" \",\n     y        = \"Relative Frequency\")\n\n\n\n\n\n\n\n\n\nRolls of Six-Sided Die\n\n\nNumber (x)\nFrequency of X\nProbability of X - f(x)\nx * f(x)\n\n\n\n\n1\n166,642\n0.167\n0.167\n\n\n2\n167,094\n0.167\n0.334\n\n\n3\n166,137\n0.166\n0.498\n\n\n4\n166,539\n0.167\n0.666\n\n\n5\n167,400\n0.167\n0.837\n\n\n6\n166,188\n0.166\n0.997\n\n\nSums\n1,000,000\n1.000\n3.500",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "mod-5-1-discrete.html#variance",
    "href": "mod-5-1-discrete.html#variance",
    "title": "24  Discrete Random Variables",
    "section": "24.9 Variance",
    "text": "24.9 Variance\nThe expected value, \\(\\mu\\), provides a measure of central tendency of a random variable.\nThe variance is a measure of the dispersion of the values of the random variable.\nThe variance of a discrete random variable is defined as:\n\\[var(x) = \\sigma^2 = \\sum (x - \\mu)^2 \\, f(x)\\]\nOne can think about the variance as the weighted average of the squared mean deviations, where the weights are the probabilities of the values of \\(x\\).\nThe standard deviation of a discrete random variable is defined as:\n\\[sd(x) = \\sqrt{\\sigma^2} = \\sqrt{\\sum (x - \\mu)^2 \\, f(x)}\\]\nThe standard deviation is preferred in describing the dispersion of a random variable as it is in the same units of the random variable as opposed to the variance which is measured in squared units and is more difficult to interpret.\n\nrm(list = ls())\n\nset.seed(1234)\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(janitor)\nlibrary(kableExtra)\nlibrary(tidyverse)\n\nthrows = 1000000\n\nrolls &lt;- tibble(throw = sample(1:6, \n                               size = throws, \n                               replace = TRUE)) %&gt;%\n  group_by(throw) %&gt;%\n  summarize(freq     = n(),\n            rel_freq = n()/throws) %&gt;%\n  mutate(xfx = throw * rel_freq,\n         deviations    = as.numeric(throw) - sum(xfx),\n         sq_deviations = deviations^2,\n         sqdev_fx      = sq_deviations * rel_freq) %&gt;%\n  adorn_totals(where = \"row\", name = \"Sums\")\n\nkable(rolls,\n      align       = \"lcccccc\", \n      format.args = list(big.mark = \",\", \n                         scientific = FALSE),\n      caption     = \"Rolls of Six-Sided Die\",\n      col.names   = c(\"Number (x)\", \"Frequency of X\", \n                      \"Probability of X - f(x)\", \"x f(x)\",\n                      \"Mean Deviations\", \"Squared Deviations\",\n                      \"Squared Deviations * f(x)\")) %&gt;%\n  kable_styling(font_size = 10)\n\nvars &lt;- rolls %&gt;% \n       filter(throw != \"Sums\") %&gt;% \n       summarize(var_x = sum(sqdev_fx),\n                 sd_x  = sqrt(sum(sqdev_fx)))\n\nkable(vars,\n      col.names = c(\"Variance (X)\", \"Standard Deviation (X)\"),\n      align     = 'c',\n      digits    = 3) %&gt;%\nkable_styling()\n\n\nRolls of Six-Sided Die\n\n\nNumber (x)\nFrequency of X\nProbability of X - f(x)\nx f(x)\nMean Deviations\nSquared Deviations\nSquared Deviations * f(x)\n\n\n\n\n1\n166,642\n0.166642\n0.166642\n-2.499525\n6.2476252\n1.0411168\n\n\n2\n167,094\n0.167094\n0.334188\n-1.499525\n2.2485752\n0.3757234\n\n\n3\n166,137\n0.166137\n0.498411\n-0.499525\n0.2495252\n0.0414554\n\n\n4\n166,539\n0.166539\n0.666156\n0.500475\n0.2504752\n0.0417139\n\n\n5\n167,400\n0.167400\n0.837000\n1.500475\n2.2514252\n0.3768886\n\n\n6\n166,188\n0.166188\n0.997128\n2.500475\n6.2523752\n1.0390697\n\n\nSums\n1,000,000\n1.000000\n3.499525\n0.002850\n17.5000014\n2.9159678\n\n\n\n\n\n\n\n\n\nVariance (X)\nStandard Deviation (X)\n\n\n\n\n2.916\n1.708",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Discrete Random Variables</span>"
    ]
  },
  {
    "objectID": "mod-5-2-binomial.html",
    "href": "mod-5-2-binomial.html",
    "title": "25  Binomial Distribution",
    "section": "",
    "text": "25.1 Probability Mass Functions\nA discrete random variable \\(X\\) is a function from the sample space \\(\\Omega\\) of a probability space to a finite, or countably infinite, set of real numbers. Together with the function \\(P\\) mapping elements\n\\[\n\\omega \\in \\Omega\n\\]\na random variable determines the Probability Mass Function\n\\[\nP(X(\\omega))\n\\]\nThe Probability Mass Function (PMF) is typically denoted as\n\\[\nP(X)\n\\]\nand maps real numbers to probabilities.\nMore formally, for any value \\(x\\) in the range of the random variable \\(X\\), let \\(A\\) be part of the sample space of whose members map \\(X\\) to \\(x\\). The probability that \\(X\\) will take on the value \\(x\\) is the value that the probability function assigns to \\(A\\) or:\n\\[\nP(X = x) = P(A)\n\\]\nImagine we have a bag with four chips. Two chips are labeled “A” and two chips are labeled “B”. Two chips are drawn at the same time. The sample space for the total number of “A” chips that can be drawn is defined as:\n\\[\n\\Omega = \\{AA, AB, BA, BB \\}\n\\]\nSince we are not concerned with ordering, the outcome \\(AB\\) is considered the same as the outcome \\(BA\\).\nThe function \\(X\\) maps each possible pair of observations to the total number of A’s obtained.",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Binomial Distribution</span>"
    ]
  },
  {
    "objectID": "mod-5-2-binomial.html#probability-mass-functions",
    "href": "mod-5-2-binomial.html#probability-mass-functions",
    "title": "25  Binomial Distribution",
    "section": "",
    "text": "\\(X(AA) =2\\) and the probability of observing \\(X\\) is \\(P(X) = 1/4\\).\n\\(X(AB) = 1\\) and \\(X(BA) = 1\\) so \\(P(X) = 1/4 + 1/4 = 1/2\\).\n\\(X(BB) = 0\\) and the probability of observing \\(X\\) is \\(P(X) = 1/4\\) .\n\n\nPMF Example\n\n\n\n\n\n\n\n\\(\\omega\\)\n\\(X(\\omega)\\)\n\\(P(X)\\)\n\n\n\n\nAA\n2\n\\(\\frac{1}{4}\\)\n\n\nAB\nBA\n1\n1\n\\(\\frac{1}{2}\\)\n\n\nBB\n0\n\\(\\frac{1}{4}\\)",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Binomial Distribution</span>"
    ]
  },
  {
    "objectID": "mod-5-2-binomial.html#cumulative-distribution-functions",
    "href": "mod-5-2-binomial.html#cumulative-distribution-functions",
    "title": "25  Binomial Distribution",
    "section": "25.2 Cumulative Distribution Functions",
    "text": "25.2 Cumulative Distribution Functions\nIn the above example, the random variable \\(X\\) determines the probability mass function, \\(P(X)\\).\nWe can use the PMF to ask: what is the probability of observing no A’s when drawing two chips from a bag in which there are two A chips and two B chips?\n\\[\nP(X = x) = P(X = BB) = P(0) = 1/4 = 0.25\n\\]\nWhat if we want to know the likelihood of observing 1 or fewer A’s?\nIn other words, we want to know\n\\[\nP(X \\le x) = P(X \\le 1)\n\\]\nThe probability mass function (PMF) determines the Cumulative Distribution Function.\nThe cumulative distribution function (CDF) examines the aggregation of probabilities.\nWhat is the likelihood of observing 1 or fewer A’s?\nThis will be the sum of likelihoods of observing 1 A or 0 A, that is:\n\\[\nP(X \\le x) = P(X \\le 1) = P(X = 1) + P(X = 0) = 1/2 + 1/4 = 0.75\n\\]",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Binomial Distribution</span>"
    ]
  },
  {
    "objectID": "mod-5-2-binomial.html#binomial-experiments",
    "href": "mod-5-2-binomial.html#binomial-experiments",
    "title": "25  Binomial Distribution",
    "section": "25.3 Binomial Experiments",
    "text": "25.3 Binomial Experiments\nPreviously, we have used empirical data to generate distributions.\nThe binomial probability distribution is a discrete probability distribution.\nIn a binomial experiment, we are interested in the number of successes relative to the number of trials.\nIf \\(x\\) denotes the number of successes occurring in \\(n\\) trials, then \\(x\\) is a discrete random variable.\nAssume that we will flip a fair two-sided coin 5 times.\nWe define success as having a head appear on the upward face of the coin.\nWe count the number of successes out of the 5 trials.",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Binomial Distribution</span>"
    ]
  },
  {
    "objectID": "mod-5-2-binomial.html#properties-of-binomial-experiments",
    "href": "mod-5-2-binomial.html#properties-of-binomial-experiments",
    "title": "25  Binomial Distribution",
    "section": "25.4 Properties of Binomial Experiments",
    "text": "25.4 Properties of Binomial Experiments\nA binomial experiment exhibits the following four properties:\n\nThe experiment consists of a sequence of \\(n\\) identical trials.\nTwo outcomes: success and failure are possible on each trial.\nThe probability of success, denoted by \\(p\\), does not change from trial to trial.\nThe trials are independent.\n\nIn the previous example, we flipped a fair two-sided coin 5 times.\nThe experiment consisted of 5 trials, the outcomes were heads (success) and tails (failure), the probability of success, \\(p = 0.5\\), did not change from trial to trial, and the trials were independent.\nWe would conclude that the coin flip experiment was a binomial experiment.",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Binomial Distribution</span>"
    ]
  },
  {
    "objectID": "mod-5-2-binomial.html#combinations-and-experiments",
    "href": "mod-5-2-binomial.html#combinations-and-experiments",
    "title": "25  Binomial Distribution",
    "section": "25.5 Combinations and Experiments",
    "text": "25.5 Combinations and Experiments\nWe first need to determine the number of successes, \\(x\\), in the trials, \\(n\\).\nA combination examines the number of ways objects can be selected, without regard to the order in which they are selected.\nThe number of experimental outcomes providing exactly \\(x\\) successes in \\(n\\) trials is:\n\\[\\binom{n}{x} = \\frac{n!}{x!(n-x)!}\\] If the number of successes is 2 and number of trials is 3, then \\(x = 2\\) and \\(n = 3\\) and:\n\\[\\binom{3}{2} = \\frac{3!}{2!(3-2)!} = 3\\]\nIf the number of successes is 5 and the number of trials is 10, the \\(x = 5\\) and \\(n = 10\\) and:\n\\[\\binom{10}{5} = \\frac{10!}{5!(10-5)!} = 252\\]\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Binomial Distribution</span>"
    ]
  },
  {
    "objectID": "mod-5-2-binomial.html#binomial-probability-function",
    "href": "mod-5-2-binomial.html#binomial-probability-function",
    "title": "25  Binomial Distribution",
    "section": "25.6 Binomial Probability Function",
    "text": "25.6 Binomial Probability Function\nLet \\(x\\) be the number of successes.\nLet \\(p\\) be the probability of success on one trial.\nLet \\(n\\) be the number of trials.\nLet \\(f(x)\\) be the probability of \\(x\\) successes in \\(n\\) trials.\nLet \\(n!\\) be \\(n\\) factorial.\nThe binomial probability function is defined as:\n\\[f(x) = \\binom{n}{x} p^x (1-p)^{(n-x)}\\] \\[f(x) = \\frac{n!}{x!(n-x)!} \\, p^x (1-p)^{(n-x)}\\]",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Binomial Distribution</span>"
    ]
  },
  {
    "objectID": "mod-5-2-binomial.html#binomial-pmf-example",
    "href": "mod-5-2-binomial.html#binomial-pmf-example",
    "title": "25  Binomial Distribution",
    "section": "25.7 Binomial PMF Example",
    "text": "25.7 Binomial PMF Example\nAssume that we have a toggle switch that determines whether a manufacturing line continues to work.\n\nWhen the toggle switch is observed to be in the “on” position, this is considered a success.\nWhen the toggle switch is observed to be in the “off” position, this is considered a failure.\nWe know that the probability of the switch being on is \\(p = 0.95\\).\nWe assume that the probability remains constant.\nWe assume that the current switch position is independent of the previous switch position.\n\nWhat is the probability of 92 successes in 100 trials?\nGiven \\(n = 100\\), \\(x = 92\\), and \\(p = 0.95\\), the probability of \\(x\\) successes in \\(n\\) trials is\n\\[f(x) = \\frac{100!}{92!(100-92)!} \\times\\]\n\\[0.95^{92} \\, \\times (1-0.95)^{(100-92)}\\]\nWe can use R to calculate the probability of 92 successes in 100 trials and \\(p=0.95\\).\nIn the code below, we perform four calculations:\n\nWe first declare \\(n\\), \\(x\\), and \\(p\\).\nWe estimate the number of combinations with \\(n = 100\\) and \\(x = 92\\).\nWe estimate the binomial probability with \\(n = 100\\) and \\(x = 92\\).\nWe estimate the binomial probability using the choose function for combinations\nWe estimate the binomial probability using the dbinom function.\n\n\nrm(list = ls())\n\nlibrary(dplyr)\nlibrary(kableExtra)\n\nn = 100\nx = 92\np = 0.95\n\nfx &lt;- tibble(combo_x &lt;- factorial(n)/(factorial(x)*factorial(n - x)),\n             fx_calc &lt;- combo_x *p^(x)*(1-p)^(n-x),\n             fx_calc2 &lt;- choose(n,x)*p^(x)*(1-p)^(n-x),\n             fx_func &lt;- dbinom(x, n, p))\n\nkable(fx,\n      align       = \"cccc\", \n      digits      = 3,\n      format.args = list(big.mark = \",\", scientific = FALSE),\n      caption     = \"P(x=92) with n = 100 and p = 0.95\",\n      col.names   = c(\"Combinations\",\n                      \"Calculated \", \n                      \"Combination Function\", \n                      \"Binomial PMF\")) %&gt;%\n  kable_styling(font_size = 14)\n\n\nP(x=92) with n = 100 and p = 0.95\n\n\nCombinations\nCalculated\nCombination Function\nBinomial PMF\n\n\n\n\n186,087,894,300\n0.065\n0.065\n0.065",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Binomial Distribution</span>"
    ]
  },
  {
    "objectID": "mod-5-2-binomial.html#binomial-cdf-example",
    "href": "mod-5-2-binomial.html#binomial-cdf-example",
    "title": "25  Binomial Distribution",
    "section": "25.8 Binomial CDF Example",
    "text": "25.8 Binomial CDF Example\nIn our previous example, we were interested in the likelihood of observing 92 successes in 100 trials where the probability of success in any given trial was 0.95 and the conditions of a binomial experiment were met.\nLet’s change the problem so we can demonstrate how a binomial CDF works.\nThe problem statement:\n\nA toggle switch that determines whether a manufacturing line continues to work.\nWhen the toggle switch is observed to be in the “on” position, this is considered a success.\nWhen the toggle switch is observed to be in the “off” position, this is considered a failure.\nWe know that the probability of the switch being on is \\(p = 0.95\\).\nWe assume that the probability remains constant.\nWe assume that the current switch position is independent of the previous switch position.\nAssume that \\(n = 10\\)\nAssume \\(x = 7\\)\nAssume \\(p = 0.95\\)\n\nWhat is the probability that \\(x = 7\\), that is, \\(P(X = 7)\\)?\nWhat is the probability that \\(x\\) is less than or equal to 7, that is \\(P(x \\le 7)\\)?\nThe code below does the following:\n\nWe declare the variables\nWe estimate the PMF for the binomial using dbinom\nWe estimate the CDF for the binomial using pbinom\nWe show the CDF is equal to the sum of the PMFs\n\nIn other words:\n\nWe evaluate \\(P(X = 7)\\) using the dbinom function\nWe evaluate \\(P(X \\le 7)\\) using the pbinom function\nWe show \\(P(X \\le 7) = \\sum_{i = 0}^7 P(X = x_i)\\)\n\nWe find that the probability of observing 7 successes in an experiment with 10 trials and a probability of success of 0.95 in any given trial is equal to 0.0105.\nWe find that probability of observing 7 or fewer successes in an experiment with 10 trials and a probability of success of 0.95 in any given trial is equal to 0.0115.\n\nrm(list = ls())\n\nlibrary(dplyr)\nlibrary(kableExtra)\n\nn = 10\nx = 7\np = 0.95\n\nfx &lt;- tibble(pmf &lt;- dbinom(x, n, p),\n             cdf &lt;- pbinom(x, n, p),\n             cdf_a &lt;- dbinom(0, 10, 0.95) + dbinom(1, 10, 0.95) +\n                      dbinom(2, 10, 0.95) + dbinom(3, 10, 0.95) +\n                      dbinom(4, 10, 0.95) + dbinom(5, 10, 0.95) +\n                      dbinom(6, 10, 0.95) + dbinom(7, 10, 0.95))\n\nkable(fx,\n      align       = \"c\", \n      digits      = 4,\n      caption     = \"N = 10 and p = 0.95\",\n      col.names   = c(\"PMF(X = x) \", \"CDF(X &lt;= x)\", \"CDF(X &lt;= x)\")) %&gt;%\nkable_styling(font_size = 14,\n              full_width = TRUE)\n\n\nN = 10 and p = 0.95\n\n\nPMF(X = x)\nCDF(X &lt;= x)\nCDF(X &lt;= x)\n\n\n\n\n0.0105\n0.0115\n0.0115",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Binomial Distribution</span>"
    ]
  },
  {
    "objectID": "mod-5-2-binomial.html#graphical-binomial-pmf",
    "href": "mod-5-2-binomial.html#graphical-binomial-pmf",
    "title": "25  Binomial Distribution",
    "section": "25.9 Graphical Binomial PMF",
    "text": "25.9 Graphical Binomial PMF\nThe code below provides a graphical illustration of three binomial PMFs.\nThe number of trials is equal to 100.\nThe variable, \\(x\\), is a sequence of integers from 1 to 100.\nIn the first case, the probability of success in any given trial is 0.3.\nIn the second case, the probability of success in any given trial is 0.4.\nIn the third case, the probability of success in any given trial is 0.5.\nYou should observe that the center of each PMF shifts right as the probability of success in any given independent trial increases.\n\nrm(list = ls())\n\nx &lt;- 1:100\n\nlines_1 &lt;- tibble(x, p_x = dbinom(x, size = 100, prob = 0.3), \n                      prob = 0.1)\nlines_2 &lt;- tibble(x, p_x = dbinom(x, size = 100, prob = 0.4), \n                      prob = 0.3)\nlines_3 &lt;- tibble(x, p_x = dbinom(x, size = 100, prob = 0.5), \n                      prob = 0.5)\n\nlines &lt;- bind_rows(lines_1, lines_2, lines_3)\n\nggplot(lines,\n       aes(x     = x,\n           y     = p_x,\n           group = prob,\n           fill  = prob)) + \ngeom_col() +\nfacet_wrap(vars(prob)) +\ntheme_minimal() + \ntheme(legend.position = \" \") +\nlabs(title = \"Binomial Probability Mass Function\",\n     x     = \" \",\n     y     = \"P(X = x)\")",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Binomial Distribution</span>"
    ]
  },
  {
    "objectID": "mod-5-2-binomial.html#graphical-binomial-cdf",
    "href": "mod-5-2-binomial.html#graphical-binomial-cdf",
    "title": "25  Binomial Distribution",
    "section": "25.10 Graphical Binomial CDF",
    "text": "25.10 Graphical Binomial CDF\nThe code below provides a graphical illustration of three binomial CDFs.\nThe number of trials is equal to 100.\nThe variable, \\(x\\), is a sequence of integers from 1 to 100.\nIn the first case, the probability of success in any given trial is 0.3.\nIn the second case, the probability of success in any given trial is 0.4.\nIn the third case, the probability of success in any given trial is 0.5.\n\nrm(list = ls())\n\nx &lt;- 1:100\n\nlines_1 &lt;- tibble(x, p_x = pbinom(x, size = 100, prob = 0.3), \n                      prob = 0.1)\nlines_2 &lt;- tibble(x, p_x = pbinom(x, size = 100, prob = 0.4), \n                      prob = 0.3)\nlines_3 &lt;- tibble(x, p_x = pbinom(x, size = 100, prob = 0.5), \n                      prob = 0.5)\n\nlines &lt;- bind_rows(lines_1, lines_2, lines_3)\n\nggplot(lines,\n       aes(x     = x,\n           y     = p_x,\n           group = prob,\n           fill  = prob)) + \ngeom_col() +\nfacet_wrap(vars(prob)) +\ntheme_minimal() + \ntheme(legend.position = \" \") +\nlabs(title = \"Binomial Cumulative Distribution Function\",\n     x     = \" \",\n     y     = \"P(X &lt;= x)\")",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Binomial Distribution</span>"
    ]
  },
  {
    "objectID": "mod-5-2-binomial.html#expected-value-and-variance",
    "href": "mod-5-2-binomial.html#expected-value-and-variance",
    "title": "25  Binomial Distribution",
    "section": "25.11 Expected Value and Variance",
    "text": "25.11 Expected Value and Variance\nGiven \\(x\\), \\(n\\), and \\(p\\), the expected value for the binomial distribution is:\n\\[E(x) = \\mu = np\\]\nThe variance of the binomial distribution is:\n\\[var(x) = \\sigma^2 = np \\, (1-p)\\]\nLet\n\n\\(n\\) = 50\n\\(x = 23\\)\n\\(p\\) = 0.55\n\nThe expected value, variance, and standard deviation are equal to:\n\\[E(x) = \\mu = 50 \\times 0.55 = 27.5\\] \\[var(x) = \\sigma ^2 = np \\, (1-p) = 50 \\times 0.55 \\times (1 - 0.55) = 12.375\\]\n\\[sd(x) = \\sqrt{(\\sigma^2)} = \\sqrt{(12.375)} = 3.52\\]\n\nrm(list = ls())\n\nlibrary(dplyr)\nlibrary(kableExtra)\n\nn = 50\nx = 23\np = 0.55\n\nfx &lt;- tibble(pmf &lt;- dbinom(x, n, p),\n             cdf &lt;- pbinom(x, n, p),\n             ev  &lt;- n*p,\n             var &lt;- n*p*(1-p),\n             sd  &lt;- sqrt(n*p*(1-p)))\n\nkable(fx,\n      align       = \"c\", \n      digits      = 4,\n      caption     = \"N = 50, X = 23, and p = 0.55\",\n      col.names   = c(\"PMF(X = x) \", \n                      \"CDF(X &lt;= x)\", \n                      \"Expected Value\",\n                      \"Variance\",\n                      \"Standard Deviation\")) %&gt;%\nkable_styling(font_size = 14,\n              full_width = TRUE)\n\n\nN = 50, X = 23, and p = 0.55\n\n\nPMF(X = x)\nCDF(X &lt;= x)\nExpected Value\nVariance\nStandard Deviation\n\n\n\n\n0.05\n0.1279\n27.5\n12.375\n3.5178",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Binomial Distribution</span>"
    ]
  },
  {
    "objectID": "mod-5-3-poisson.html",
    "href": "mod-5-3-poisson.html",
    "title": "26  Poisson Distribution",
    "section": "",
    "text": "26.1 Poisson Probability Distribution\nA Poisson probability distribution is often used with the number of occurrences within an interval of time or space.\nThe number of arrivals at a bank teller window in one hour, the number of passengers screened over an 8 hour shift, the number of leaks in 100 miles of pipeline are questions that may be addressed by a Poisson distribution.\nThere are two conditions that must be satisfied for a Poisson experiment.\nAs \\(x\\) is without an upper limit, the probability of \\(x\\) occurrences, \\(f(x)\\), will approach zero as \\(x\\) becomes sufficiently large.\nIf you are examining the number of arrivals at an screening site in an hour, then as \\(x\\) increases from \\(1,2,3....\\), \\(f(x)\\) becomes approximately zero for “large” values of \\(x\\).\nFor example, you are observing how many passengers arrive at a security checkpoint in an hour. Assume this is a relatively small airport and that you are observing the checkpoint mid-day.\nThe probability of observing 1 passenger is likely close to 1. The probability of observing 100 passengers is likely less than 1. The probability of observing 1,000 passengers is likely close to 0. The probability of observing 10,000 passengers is likely very close to 0.",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Poisson Distribution</span>"
    ]
  },
  {
    "objectID": "mod-5-3-poisson.html#poisson-probability-distribution",
    "href": "mod-5-3-poisson.html#poisson-probability-distribution",
    "title": "26  Poisson Distribution",
    "section": "",
    "text": "First, the probability of occurrence is the same for any two intervals of equal length.\nSecond, the occurrence or nonoccurrence in any interval is independent of the occurrence or nonoccurrence in any other interval.",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Poisson Distribution</span>"
    ]
  },
  {
    "objectID": "mod-5-3-poisson.html#poisson-probability-distribution-1",
    "href": "mod-5-3-poisson.html#poisson-probability-distribution-1",
    "title": "26  Poisson Distribution",
    "section": "26.2 Poisson Probability Distribution",
    "text": "26.2 Poisson Probability Distribution\nLet \\(x\\) be the discrete number of occurrences in an interval\nLet \\(\\lambda\\) be the expected value or mean number of occurrences in an interval\nLet \\(e = 2.71828\\)\nLet \\(f(x)\\) be the probability of \\(x\\) occurrences in an interval.\nThe Poisson probability mass function is defined as:\n\\[P(X) = \\frac{\\lambda^x \\, e^{-\\lambda}}{x!}\\] Let \\(x\\) be the number of patients arriving in a 15-minute interval and the average number of patients observed is 10.\nWhat is the probability of observing 5 patients in a 15-minute interval?\n\\[P(X = 5) = \\frac{10^5 \\, e^{-10}}{5!} = 0.0378\\]",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Poisson Distribution</span>"
    ]
  },
  {
    "objectID": "mod-5-3-poisson.html#poisson-example",
    "href": "mod-5-3-poisson.html#poisson-example",
    "title": "26  Poisson Distribution",
    "section": "26.3 Poisson Example",
    "text": "26.3 Poisson Example\nAirline passengers arrive randomly, independently, with a mean arrival rate of 10 passengers/minute.\n\\[\n\\mu = 10\n\\]\nWhat is the probability of no arrivals in a one-minute period?\n\\[\nP(X = 0)\n\\]\nWhat is the probability 12 or fewer arrivals in a one-minute period?\n\\[\nP(X \\le 12)\n\\]\nWhat is the probability of three or fewer passengers in a one-minute period?\n\\[\nP(X \\le 3)\n\\]\nWhat is the probability of no arrivals in a 15-second period?\n\\[\n\\mu = (10/60) \\times 15 = 2.5\n\\]\n\\[\nP(X \\le 0)\n\\]\nWhat is the probability of at least one arrival in a 15-second period?\n\\[\nP(X \\ge 1)\n\\]\n\nrm(list = ls())\n\nlibrary(dplyr)\nlibrary(kableExtra)\n\n# mu_60 = 10 per 60 seconds\n# mu_15 = 2.5 per 15 second\n\npoissons &lt;- tibble(p_x0_1min &lt;- dpois(0, 10),\n                   p_x12_1min &lt;- ppois(12, 10),\n                   p_xle3_1min &lt;- ppois(3, 10),\n                   p_x0_15sec &lt;- dpois(0, 2.5),\n                   p_xge1_15sec &lt;- 1-dpois(0, 2.5))\n\nkable(poissons,\n      align       = \"ccccc\", \n      format.args = list(big.mark = \",\", scientific = FALSE),\n      caption     = \"Poisson Distribution Example\",\n      col.names   = c(\"P(x = 0, 1 min)\", \n                      \"P(x &lt;= 12, 1 min)\", \n                      \"P(x &lt;= 3, 1 min)\",\n                      \"P(x=0, 15 sec)\",\n                      \"P(x&gt;0, 15 sec)\")) %&gt;%\n  kable_styling(font_size = 14)\n\n\nPoisson Distribution Example\n\n\nP(x = 0, 1 min)\nP(x &lt;= 12, 1 min)\nP(x &lt;= 3, 1 min)\nP(x=0, 15 sec)\nP(x&gt;0, 15 sec)\n\n\n\n\n0.0000454\n0.7915565\n0.0103361\n0.082085\n0.917915",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Poisson Distribution</span>"
    ]
  },
  {
    "objectID": "mod-5-3-poisson.html#graphic-poisson-pmf",
    "href": "mod-5-3-poisson.html#graphic-poisson-pmf",
    "title": "26  Poisson Distribution",
    "section": "26.4 Graphic Poisson PMF",
    "text": "26.4 Graphic Poisson PMF\nIn the follow example, we create a sequence of integers from 1 to 20 and then evaluate the Poisson PFM with\n\n\\(\\mu = 10\\)\n\\(\\mu = 20\\)\n\n\nx &lt;- 1:20\n\nlines_1 &lt;- tibble(x, p_x = dpois(x, 5), mu = 5)\nlines_2 &lt;- tibble(x, p_x = dpois(x, 10), mu = 10)\n\nlines &lt;- bind_rows(lines_1, lines_2)\n\nggplot(lines,\n       aes(x     = x,\n           y     = p_x,\n           group = mu,\n           fill  = mu)) + \n  geom_col(colour = \"black\") +\n  facet_wrap(vars(mu)) +\n  theme_minimal() + \n  theme(legend.position = \" \") +\n  labs(title = \"Poisson Probability Function\",\n       x     = \" \",\n       y     = \"P(X = x)\")",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Poisson Distribution</span>"
    ]
  },
  {
    "objectID": "mod-5-3-poisson.html#graphical-poisson-cdf",
    "href": "mod-5-3-poisson.html#graphical-poisson-cdf",
    "title": "26  Poisson Distribution",
    "section": "26.5 Graphical Poisson CDF",
    "text": "26.5 Graphical Poisson CDF\nIn the follow example, we create a sequence of integers from 1 to 20 and then evaluate the Poisson CDF with\n\n\\(\\mu = 10\\)\n\\(\\mu = 20\\)\n\n\nx &lt;- 1:20\n\nlines_1 &lt;- tibble(x, p_x = ppois(x, 5), mu = 10)\nlines_2 &lt;- tibble(x, p_x = ppois(x, 10), mu = 20)\n\nlines &lt;- bind_rows(lines_1, lines_2)\n\nggplot(lines,\n       aes(x     = x,\n           y     = p_x,\n           group = mu,\n           fill  = mu)) + \n  geom_col(colour = \"black\") +\n  facet_wrap(vars(mu)) +\n  theme_minimal() + \n  theme(legend.position = \" \") +\n  labs(title = \"Cumulative Poisson Probability Function\",\n       x     = \" \",\n       y     = \"P(X = x)\")",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Poisson Distribution</span>"
    ]
  },
  {
    "objectID": "mod-5-3-poisson.html#expected-value-and-variance",
    "href": "mod-5-3-poisson.html#expected-value-and-variance",
    "title": "26  Poisson Distribution",
    "section": "26.6 Expected Value and Variance",
    "text": "26.6 Expected Value and Variance\nThe expected value or mean of a Poisson distribution is\n\\[\n\\mu = E(X) = \\lambda\n\\]\nThe variance of a Poisson distribution is:\n\\[\n\\sigma^2 = Var(X) = \\lambda\n\\]\nSo, if the mean arrival rate of passengers at a checkpoint every 60 minutes is 10 then\n\\[\n\\mu = 10 \\quad var = 10 \\quad sd=\\sqrt{10}\n\\]",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Poisson Distribution</span>"
    ]
  },
  {
    "objectID": "mod-5-3-poisson.html#the-d-function-in-r",
    "href": "mod-5-3-poisson.html#the-d-function-in-r",
    "title": "26  Poisson Distribution",
    "section": "26.7 The D Function in R",
    "text": "26.7 The D Function in R\nThe d function returns the value of the probability mass function (PMF) for a discrete distribution or the probability density function (PDF) for a continuous distribution.\nThe probability mass function, sometimes referred to as the discrete density function, given the probability that a discrete random variable is exactly equal to some value.\nFor a binomial distribution, to find \\(f(x)\\), we need the value of a certain random variable \\(x\\), the number of independent trials \\(n\\), and the probability of success in any given trial \\(p\\).\nThe binomial probability mass function or binomial density function is defined as:\n\\[f(x) = \\binom{n}{x} p^x (1-p)^{(n-x)}\\]\n\\[f(x) = \\frac{n!}{x!(n-x)!} \\, p^x (1-p)^{(n-x)}\\]\nWe conduct 1,000 trials of an experiment.\nThe probability of success in any given trial is equal to 0.75.\nWe want to know the probability of having 752 successes\nWe also want to know the probability of having 775 successes.\nIn other words, we want to know\n\\[P(X = 752)\\]\n\\[\nP(X = 775)\n\\]\n\nrm(list = ls())\n\nlibrary(dplyr)\nlibrary(kableExtra)\n\nn  = 1000\np  = 0.75\nx1 = 752\nx2 = 775\n\nfx &lt;- tibble(fx_1 &lt;- dbinom(x1,n,p),\n             fx_2 &lt;- dbinom(x2,n,p))\nkable(fx,\n      align       = \"cc\", \n      digits      = 3,\n      caption     = \"Binomial PMF\",\n      col.names   = c(\"P(X = 752)\", \n                      \"P(X = 775)\")) %&gt;%\nkable_styling(font_size = 14)\n\n\nBinomial PMF\n\n\nP(X = 752)\nP(X = 775)\n\n\n\n\n0.029\n0.005",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Poisson Distribution</span>"
    ]
  },
  {
    "objectID": "mod-5-3-poisson.html#the-p-function-in-r",
    "href": "mod-5-3-poisson.html#the-p-function-in-r",
    "title": "26  Poisson Distribution",
    "section": "26.8 The P Function in R",
    "text": "26.8 The P Function in R\nThe cumulative density function (CDF) of a distribution is the sum of the probabilities equal to and less than certain given values of a random variable \\(x\\).\nFor example, assume that \\(n = 10\\), \\(x = 3\\) and \\(p = 0.4\\), then the cumulative density is equal to\n\\[\\sum f(x \\in {0,3}) = P(x = 0) + P(x = 1) + P(x = 2) + P(x = 3)\\]\nReturning to the binomial distribution, we can estimate this using the probability mass function dbinom for each of the values of \\(x\\).\nWe can also calculate this using the function pbinom which returns the value of the cumulative density function of the binomial distribution given a certain random variable \\(x\\).\nThe pbinom function can be used to find \\(P(x \\le 3)\\) and \\(P(x &gt; 3)\\).\n\nAssume we are conducting a binomial experiment.\nAssume that \\(n\\) = 10, \\(x = 3\\) and \\(p = 0.3\\).\nWhat is the probability that \\(x \\le 4\\)?\nWhat is the probability that \\(x &gt; 4\\)?\nWe can use the lower.tail option to estimate \\(P(x &gt; 4)\\) by setting the lower.tail option to FALSE.\n\n\nrm(list = ls())\n\nlibrary(dplyr)\nlibrary(kableExtra)\n\nn &lt;- 10\nx &lt;- 3 \np &lt;- 0.3\n\nx_1 &lt;- 4\n\n\ncdf &lt;- tibble(cdf_x1_d &lt;- dbinom(0,10,0.3) + dbinom(1,10,0.3) +\n                          dbinom(2,10,0.3) + dbinom(3,10,0.3) +\n                          dbinom(4,10,0.3),\n              cdf_x1_p &lt;- pbinom(4,10,0.3,lower.tail = TRUE),\n              cdf_x1_right &lt;- 1 - pbinom(4,10,0.3,lower.tail = TRUE),\n              cdf_x1_right2 &lt;- pbinom(4,10,0.3,lower.tail = FALSE))\n\nkable(cdf,\n      align       = \"c\", \n      caption     = \"Binomial CDF\",\n      col.names   = c(\"P(X &lt;= 4)\", \n                      \"P(X &lt;= 4)\",\n                      \"P(X &gt; 4)\",\n                      \"P(X &gt; 4)\")) %&gt;%\nkable_styling(font_size = 14)\n\n\nBinomial CDF\n\n\nP(X &lt;= 4)\nP(X &lt;= 4)\nP(X &gt; 4)\nP(X &gt; 4)\n\n\n\n\n0.8497317\n0.8497317\n0.1502683\n0.1502683",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Poisson Distribution</span>"
    ]
  },
  {
    "objectID": "mod-5-3-poisson.html#the-q-function-in-r",
    "href": "mod-5-3-poisson.html#the-q-function-in-r",
    "title": "26  Poisson Distribution",
    "section": "26.9 The Q Function in R",
    "text": "26.9 The Q Function in R\nLet \\(f(x)\\) be the discrete probability density function and \\(F(x)\\) be the discrete cumulative density function.\nRecall that \\(F(x)\\) for a specific value \\(k\\) implies that \\(F(x = k) = P(x \\le k)\\).\nLet \\(F^{-1}(x)\\) be the inverse discrete cumulative density function.\n\\[F^{-1}(x) = c\\] \\[P(x \\le c) = q \\quad \\text{for} \\; 0 \\le q \\le 1\\]\nIn other words, given a probability \\(q\\), the inverse cumulative density function returns the value \\(c\\) from the discrete distribution function such that \\(P(X \\le c) = q\\).\nNote that with discrete distributions that the probabilities from the cumulative and inverse cumulative functions will not match exactly.\nYou can use the qbinom or qpois function to determine the value of \\(x\\) at which a cumulative density occurs.\nFor a binomial with \\(n = 100\\) and \\(p = 0.45\\), if the cumulative density is \\(q=0.25\\), then \\(x = 42\\).\nFor a Poisson given that \\(\\mu = 50\\), if the cumulative density is \\(q=0.8\\), then \\(x = 56\\).\nRemember, probabilities won’t exactly match when you go for \\(CDF^{-1}\\) to \\(x\\) to \\(CDF\\). Try increasing the density and see what happens.\n\nrm(list = ls())\n\nlibrary(dplyr)\nlibrary(kableExtra)\n\nc &lt;- 0.25\nn &lt;- 100\np &lt;- 0.45\n\nmu = 50\nd  = 0.8\n\nq_est &lt;- tibble(q_b = c,\n                q_binom &lt;- qbinom(c, n, p),\n                p_binom &lt;- pbinom(q_binom, n, p),\n                q_p = d,\n                q_poisson &lt;- qpois(d, mu),\n                p_poisson &lt;- ppois(q_poisson, mu))\n\nkable(q_est,\n      align       = \"cccccc\", \n      caption     = \"Inverse CDF\",\n      col.names   = c(\"q\",\n                      \"Inverse CDF(q)\",\n                      \"CDF(x)\",\n                      \"q\",\n                      \"Inverse CDF(q)\",\n                      \"CDF(x)\")) %&gt;%\nkable_styling(font_size = 14)\n\n\nInverse CDF\n\n\nq\nInverse CDF(q)\nCDF(x)\nq\nInverse CDF(q)\nCDF(x)\n\n\n\n\n0.25\n42\n0.3086513\n0.8\n56\n0.8221171",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Poisson Distribution</span>"
    ]
  },
  {
    "objectID": "mod-5-3-poisson.html#the-r-function-in-r",
    "href": "mod-5-3-poisson.html#the-r-function-in-r",
    "title": "26  Poisson Distribution",
    "section": "26.10 The R Function in R",
    "text": "26.10 The R Function in R\nThe function rbinom generates a vector of binomial distributed random variables given the number of experiments \\(m\\), the number of trials in each experiment \\(n\\), and the probability of success \\(p\\).\nThe function rpois generates a vector of Poisson distributed random variables given the number of experiments \\(m\\) and the arrival rate \\(\\lambda\\).\nThe function rnorm generates a vector of normally distributed random variables given \\(\\mu\\) and \\(\\sigma\\).\n\n#set the number of observations (experiments) to 5\n\nr_binom &lt;- rbinom(5,100,0.5)\n\nr_poisson &lt;- rpois(5, 10)\n\nr_norm &lt;- rnorm(5, 10, 2)\n\nr_dist &lt;- data.frame(r_binom, r_poisson, r_norm)\n\n\nkable(r_dist,\n      align       = \"ccc\", \n      caption     = \"Random Numbers\",\n      col.names   = c(\"Binomal\", \n                      \"Poisson\",\n                      \"Normal\")) %&gt;%\n  kable_styling(font_size = 14)\n\n\nRandom Numbers\n\n\nBinomal\nPoisson\nNormal\n\n\n\n\n54\n8\n11.203830\n\n\n53\n6\n8.769752\n\n\n47\n9\n7.422402\n\n\n47\n14\n10.136160\n\n\n50\n10\n9.100911",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Poisson Distribution</span>"
    ]
  },
  {
    "objectID": "mod-5-3-poisson.html#wrapping-up",
    "href": "mod-5-3-poisson.html#wrapping-up",
    "title": "26  Poisson Distribution",
    "section": "26.11 Wrapping Up",
    "text": "26.11 Wrapping Up\nA random variable is a numerical description of an outcome of an example.\nA discrete random variable may assume a finite of infinite number of values.\nIf we use empirical data to estimate probabilities, we are developing an empirical discrete distribution.\nWe discussed three discrete probability functions: uniform, binomial, and Poisson.\nWe explored the expected value, variance, and standard deviation of a discrete random variable and the probability functions.\nWe used R to generate discrete empirical probabilities as well as use discrete probability functions.",
    "crumbs": [
      "Discrete Probability",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Poisson Distribution</span>"
    ]
  },
  {
    "objectID": "mod-6-0-overview.html",
    "href": "mod-6-0-overview.html",
    "title": "27  Module 6 Overview",
    "section": "",
    "text": "27.1 Introduction\nWelcome to Module 6 of ECON 700.\nThis module extends probability theory to the study of continuous random variables and their probability distributions. Students begin by differentiating discrete from continuous variables and exploring the concept of a probability density function (PDF) as the continuous counterpart to the probability mass function (PMF). The module introduces three key continuous distributions—the uniform, normal, and exponential—each representing a different type of process.\nThe module emphasizes the normal distribution as the cornerstone of statistical analysis due to its relationship with the Central Limit Theorem. Through practical applications, students use R to compute probabilities (dnorm, pnorm, and qnorm), visualize areas under the curve, and identify key percentiles and standard deviations. The module concludes with the exponential distribution as a model of waiting times in Poisson processes, illustrating the connection between continuous distributions and real-world timing or reliability problems.",
    "crumbs": [
      "Continuous Probability",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Module 6 Overview</span>"
    ]
  },
  {
    "objectID": "mod-6-0-overview.html#learning-objectives",
    "href": "mod-6-0-overview.html#learning-objectives",
    "title": "27  Module 6 Overview",
    "section": "27.2 Learning Objectives",
    "text": "27.2 Learning Objectives\nBy the end of this module, you should be able to:\n\nMLO 1: Differentiate between discrete and continuous random variables and describe the role of probability density functions. (CLO 3)\nMLO 2: Compute and interpret probabilities using the uniform, normal, and exponential distributions. (CLO 3)\nMLO 3: Apply R functions (dnorm, pnorm, qnorm, dunif, punif, pexp) to calculate and visualize probability densities and cumulative distributions. (CLO 3, 5)\nMLO 4: Explain the properties of the normal distribution and the Central Limit Theorem. (CLO 3)\nMLO 5: Calculate expected values, variances, and probabilities for continuous random variables. (CLO 3, 5)\nMLO 6: Evaluate real-world examples that demonstrate how continuous distributions model random events such as arrival times, measurement variation, and economic outcomes. (CLO 1, 3, 5)",
    "crumbs": [
      "Continuous Probability",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Module 6 Overview</span>"
    ]
  },
  {
    "objectID": "mod-6-0-overview.html#required-texts",
    "href": "mod-6-0-overview.html#required-texts",
    "title": "27  Module 6 Overview",
    "section": "27.3 Required Texts",
    "text": "27.3 Required Texts\nThe following textbooks are available for free. Please select the links provided to access.\n\nBarbara Illowsky and Susan Dean. Introductory Statistics 2e. OpenStax. Chapter 5 - 7.\nRafael Irizarry. (2025). Introduction to Data Science: Data Wrangling and Visualization with R. Chapters 1-2.\nHadley Wickham, Mine Cetinkaya-Rundel, and Garrett Grolemund. (2025) R for Data Science. Sections 1-19.\nOnline R documentation\nRStudio Cheat Sheets",
    "crumbs": [
      "Continuous Probability",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Module 6 Overview</span>"
    ]
  },
  {
    "objectID": "mod-6-0-overview.html#activities",
    "href": "mod-6-0-overview.html#activities",
    "title": "27  Module 6 Overview",
    "section": "27.4 Activities",
    "text": "27.4 Activities\n\nComplete the hands-on coding exercises embedded in each lesson (MLO 3)\n\nComplete the weekly class assignment (MLO 1 - MLO 6)\n\nParticipate in the discussion form (MLO 3 - MLO 5)\n\nTake the weekly knowledge quiz (MLO 1 - MLO 6)",
    "crumbs": [
      "Continuous Probability",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Module 6 Overview</span>"
    ]
  },
  {
    "objectID": "mod-6-0-overview.html#lessons-in-this-module",
    "href": "mod-6-0-overview.html#lessons-in-this-module",
    "title": "27  Module 6 Overview",
    "section": "27.5 Lessons in this Module",
    "text": "27.5 Lessons in this Module\n\n6.1 – Uniform Probability Distributions\n6.2 – Normal Probability Distributions\n6.3 – Exponential Probability Distributions\n\n\nNext: Start with Uniform Probability Distributions.",
    "crumbs": [
      "Continuous Probability",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Module 6 Overview</span>"
    ]
  },
  {
    "objectID": "mod-6-1-uniform.html",
    "href": "mod-6-1-uniform.html",
    "title": "28  Uniform Distributions",
    "section": "",
    "text": "28.1 Discrete Random Variables\nA random variable is a numerical description of an outcome of an example.\nA discrete random variable may assume a finite of infinite number of values.\nThe probability mass function or PMF, \\(P(X)\\), generates the probability of \\(X\\) occurring for a specific value of \\(x\\).\nThe probability mass function is also referred to as the discrete probability density function.\nThe cumulative distribution function or CDF generates the probability of \\(X\\) being less than or equal to a specific value of \\(x\\).\nThe cumulative distribution function is also referred to as the discrete cumulative density function.\nGiven a binomial experiment with the following properties, answer the following questions using either the PDF or CDF.\nrm(list = ls())\n\nlibrary(dplyr)\nlibrary(kableExtra)\n\nn = 50\np = .8\nx = 42\n\nprobs &lt;- tibble(p_1 &lt;- dbinom(x, n, p),\n                p_2 &lt;- pbinom(x, n, p),\n                p_3 &lt;- pbinom(x, n, p, lower.tail = FALSE))\n\nkable(probs,\n      align = 'c',\n      digits = 3,\n      col.names = c('P(X = 42)', 'P(X &lt;= 42)', 'P(X &gt; 42)')) %&gt;%\nkable_classic()\n\n\n\n\nP(X = 42)\nP(X &lt;= 42)\nP(X &gt; 42)\n\n\n\n\n0.117\n0.81\n0.19",
    "crumbs": [
      "Continuous Probability",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Uniform Distributions</span>"
    ]
  },
  {
    "objectID": "mod-6-1-uniform.html#discrete-random-variables",
    "href": "mod-6-1-uniform.html#discrete-random-variables",
    "title": "28  Uniform Distributions",
    "section": "",
    "text": "\\(n = 50\\)\n\\(p = .8\\)\n\\(x = 42\\)\nWhat is \\(P(X = x) = P(X = 42)\\)?\nWhat is \\(P(X \\le x) = P(X \\le 42)\\)?\nWhat is \\(P(X &gt; x) = 1 - P(X \\le X) = 1 - P(X \\le 42)\\)?",
    "crumbs": [
      "Continuous Probability",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Uniform Distributions</span>"
    ]
  },
  {
    "objectID": "mod-6-1-uniform.html#continuous-random-variables",
    "href": "mod-6-1-uniform.html#continuous-random-variables",
    "title": "28  Uniform Distributions",
    "section": "28.2 Continuous Random Variables",
    "text": "28.2 Continuous Random Variables\nA continuous random variable may assume any value in an interval or collection of intervals.\nThe probability density function, \\(f(x)\\), is the counterpart of the discrete probability density function.\nThe probability density function for a continuous random variable is typically referred to as the PDF.\nThe PDF, \\(p(X)\\), generates a curve for a given interval of \\(x\\).\nThe PDF provides the probability that \\(x\\) takes on any value in the given interval of \\(x\\).\nThis implies that the area under \\(p(X)\\) for a specific value of \\(x\\) is zero.\nIn other words, given a continuous random value, the probability that the continuous variable, \\(x\\), is equal to a specific value is zero.",
    "crumbs": [
      "Continuous Probability",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Uniform Distributions</span>"
    ]
  },
  {
    "objectID": "mod-6-1-uniform.html#uniform-distribution",
    "href": "mod-6-1-uniform.html#uniform-distribution",
    "title": "28  Uniform Distributions",
    "section": "28.3 Uniform Distribution",
    "text": "28.3 Uniform Distribution\nThe continuous uniform probability distribution describes an experiment where there is a random outcome that is within specific bounds.\nThe bounds of the random outcome are defined by the parameters, \\(a\\) and \\(b\\).\n\n\\(a\\) is the minimum value of the random outcome.\n\\(b\\) is the maximum value of the random outcome.\n\nThe uniform distribution is defined by the parameters \\(a\\) and \\(b\\) which define the region where the density function \\(p(x)\\) is constant and outside of which \\(p(x) = 0\\). Since the area under the probability density function must total 1, the probability density function of the continuous uniform distribution is:\n\\[P(x | a,b)  = \\frac{1}{b-a} \\quad \\text{for} \\, x\\in[a,b]\\]\n\\[P(x |a,b) = 0 \\quad \\text{for} \\, x \\not\\in[a,b]\\]\nWe often denote the random variable \\(X\\) as uniformly distributed on the interval \\([a,b]\\) as:\n\\[\nX \\sim U(a,b)\n\\]",
    "crumbs": [
      "Continuous Probability",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Uniform Distributions</span>"
    ]
  },
  {
    "objectID": "mod-6-1-uniform.html#pdf-of-continuous-uniform",
    "href": "mod-6-1-uniform.html#pdf-of-continuous-uniform",
    "title": "28  Uniform Distributions",
    "section": "28.4 PDF of Continuous Uniform",
    "text": "28.4 PDF of Continuous Uniform\nThe PDF of the continuous uniform probability distribution is\n\\[P(x|a,b)  = \\frac{1}{b-a} \\quad \\text{for} \\, x\\in[a,b]\\]\n\\[\\text{for} \\, x \\in[10,30]\\]\nFor a continuous probability distribution, we know that:\n\\[P(x) \\ge 0\\] \\[\\int_{-\\infty}^{\\infty} \\, f(x) \\, dx = 1\\]\nFor the continuous uniform distribution:\n\\[a &lt; b \\rightarrow \\frac{1}{b-a} \\ge 0 \\quad \\forall \\, a \\le x \\le b \\rightarrow \\, f(x) \\ge 0 \\quad \\forall x\\]\nThis means that \\(f(x) \\ge 0\\) for all \\(x \\in [a,b]\\).\nDoes the area defined by the PDF equal to one?\nHere we can integrate the PDF and note that when \\(x &lt; a\\) and \\(x &gt; b\\) that \\(f(x) = 0\\).\n\\[lim_{n \\rightarrow \\infty} \\int_{-n}^{n} f(x)d(x) = lim_{n \\rightarrow \\infty} \\int_{-n}^{a} (0)d(x) +\\int_{a}^{b} \\frac{1}{b-a}dx + \\int_{b}^{n} (0)d(x)\\]\n\\[lim_{n \\rightarrow \\infty} \\int_{-n}^{n} f(x)d(x) = \\frac{1}{b-a}[x]_{b}^{a} = \\frac{1}{b-a}(b -a) = 1\\]\nThe PDF of the continuous uniform probability distribution fulfills the conditions for a PDF of a continuous random variable.\nThe continuous uniform is often referred to as a rectangular distribution given the shape of its PDF.\nIn the following code, we create a tibble with the following variables:\n\nA sequence of numbers from 0 to 40, in 0.01 increments\nThe probabilities associated with a continuous uniform with a = 10 and b = 30 for the numbers\nA plot with the numbers on the x-axis and the probabilities on the y-axis\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Continuous Probability",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Uniform Distributions</span>"
    ]
  },
  {
    "objectID": "mod-6-1-uniform.html#uniform-example",
    "href": "mod-6-1-uniform.html#uniform-example",
    "title": "28  Uniform Distributions",
    "section": "28.5 Uniform Example",
    "text": "28.5 Uniform Example\n\nLet the continuous uniform distribution be defined over \\([a,b]\\).\nThe probability that \\(x\\) is in specific interval \\([c,d]\\) is:\n\n\\[P(c \\le x \\le d) = \\int_{c}^{d} f(x) dx = \\int_{c}^{d} \\frac{1}{b-a}dx\\]\n\\[P(c \\le x \\le d) = \\int_{c}^{d} \\frac{1}{b-a} \\,dx = \\frac{1}{b-a}(d-c) = \\frac{d-c}{b-a}\\]\nLet an elevator arrive randomly between 10 and 30 seconds after a button is pushed.\nWhat is the probability that the elevator arrives between 15 and 20 seconds?\n\\[P(15 \\le x \\le 20) = \\frac{20-15}{30-10} = 0.25\\]",
    "crumbs": [
      "Continuous Probability",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Uniform Distributions</span>"
    ]
  },
  {
    "objectID": "mod-6-1-uniform.html#cdf-of-continuous-uniform",
    "href": "mod-6-1-uniform.html#cdf-of-continuous-uniform",
    "title": "28  Uniform Distributions",
    "section": "28.6 CDF of Continuous Uniform",
    "text": "28.6 CDF of Continuous Uniform\nGiven a uniform distribution defined over \\([a,b]\\), what is \\(P(X \\le x)\\) if \\(a &lt; x &lt; b\\).\nWe have found that \\(P(c \\le x \\le d) = \\frac{d - c}{b - a}\\), so\n\\[F_X(x) = P(X \\le x) = \\int_{-\\infty}^{x} f_x(t)dt\\]\n\\[F_X(x) = \\int_{-\\infty}^{a}f_x(t)dt + \\int_{a}^{x}f_x(t)dt\\]\n\\[F_X(x) = \\int_{-\\infty}^{a}0 \\, dt + \\int_{a}^{x} \\frac{1}{b-a}dt = \\frac{1}{b-a} \\, [t]_a^x = \\frac{x-a}{b-a}\\]\nThis yields the properties of the CDF of the uniform probability distribution:\n\\[F_X(x) = 0 \\quad \\forall x \\le a\\]\n\\[F_X(x) = \\frac{x-a}{b-a} \\quad a &lt; x &lt; b\\]\n\\[F_X(x) = 1 \\quad \\forall x \\ge b\\]",
    "crumbs": [
      "Continuous Probability",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Uniform Distributions</span>"
    ]
  },
  {
    "objectID": "mod-6-1-uniform.html#example-of-pdf-and-cdf",
    "href": "mod-6-1-uniform.html#example-of-pdf-and-cdf",
    "title": "28  Uniform Distributions",
    "section": "28.7 Example of PDF and CDF",
    "text": "28.7 Example of PDF and CDF\nWe can use the dunif and punif functions in R.\nThe dunif function calculates the uniform density function.\nThe punif function calculates the cumulative uniform density function.\nAssume that the \\(a = 10\\) and \\(b=30\\) then the PDF of the continuous uniform is:\n\\[P(x|a,b)  = \\frac{1}{b-a} \\quad \\text{for} \\, x\\in[a,b]\\] \\[P(x|10,30)  = \\frac{1}{30-10} = 0.05 \\quad \\text{for} \\, x\\in[10,30]\\]\nWe estimate this probability manually and using the duinf function in the code chunk below.\nWe then want to find \\(P(15 \\le x \\le 20)\\).\n\\[P(c \\le x \\le d) = \\int_{c}^{d} \\frac{1}{b-a} [x]_x^d = \\frac{1}{b-a}(d-c) = \\frac{d-c}{b-a}\\]\n\\[P(15 \\le x \\le 20) = \\int_{10}^{30} \\frac{1}{30-10} [x]_x^d = \\frac{1}{30-10}(20-15) = \\frac{20-15}{30-10} = 0.25\\]\nWe estimate this probability manually and using the punif function. Here we estimate\n\\[\nP(15 \\le x \\le 20) = P(x \\le 20) - P(x \\le 15) \\quad \\text{for} \\, x \\in [10,30]\n\\]\nWe also find \\(P(X \\le 20)\\) by manually and using the the cumulative density function. Note the we are evaluating the area between 20 and the lower bound of the continuous uniform.\n\\[P(10 \\le x \\le 20) = \\int_{10}^{30} \\frac{1}{30-10} [x]_x^d = \\frac{1}{30-10}(20-10) = \\frac{20-10}{30-10} = 0.5\\]\n\nrm(list = ls())\n\nlibrary(tidyverse)\nlibrary(kableExtra)\n\na = 10\nb = 30\nc = 15\nd = 20\n\nuni_frame &lt;- tibble(d_uni_man &lt;- 1/(b-a),\n                    d_uni_calc &lt;- dunif(c, min = a, max = b),\n                    d_uni_man_range &lt;- (d-c)/(b-a),\n                    d_uni_calc_range &lt;- punif(20, min = a, max = b) - \n                                        punif(15, min = a, max = b),\n                    p_uni_man &lt;- (d-a)/(b-a),\n                    p_uni_calc &lt;- punif(d, min = a, max = b))\n\nkable(uni_frame,\n      align       = 'c', \n      digits      = 4,\n      caption     = 'Continuous Uniform Distribution\n                     a=10 and b=30',\n      col.names   = c('Manual', \n                      'Uniform(10,30)',\n                      'Manual P(15 &lt;= X &lt;= 20)',\n                      'P(15 &lt;= X &lt;= 20)',\n                      'Manual P(X &lt; 20)',\n                      'P(X &lt; 20)')) %&gt;%\nkable_classic()\n\n\nContinuous Uniform Distribution a=10 and b=30\n\n\nManual\nUniform(10,30)\nManual P(15 &lt;= X &lt;= 20)\nP(15 &lt;= X &lt;= 20)\nManual P(X &lt; 20)\nP(X &lt; 20)\n\n\n\n\n0.05\n0.05\n0.25\n0.25\n0.5\n0.5",
    "crumbs": [
      "Continuous Probability",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Uniform Distributions</span>"
    ]
  },
  {
    "objectID": "mod-6-1-uniform.html#expected-value",
    "href": "mod-6-1-uniform.html#expected-value",
    "title": "28  Uniform Distributions",
    "section": "28.8 Expected Value",
    "text": "28.8 Expected Value\nThe expected value of a continuous uniform distribution is\n\\[\\mu = E[x] = lim_{n \\rightarrow \\infty} \\int_{-n}^{n} x \\, f(x) \\, dx\\]\n\\[\\mu = lim_{n \\rightarrow \\infty} \\int_{-\\infty}^{a} x \\, f(x) \\, dx + \\int_{a}^{b} x \\, f(x) \\, dx + \\int_{b}^{\\infty} x \\, f(x) \\, dx\\]\n\\[\\mu = \\int_{-\\infty}^{a} x \\, (0) \\, dx + \\int_{a}^{b} x \\, \\frac{1}{b-a} \\, dx + \\int_b^{\\infty} x \\, (0) \\, dx\\]\n\\[\\mu = \\int_{a}^{b} x \\, \\frac{1}{b-a} \\, dx = \\frac{1}{b-a} \\int_a^b x \\, dx = \\frac{1}{2(b-a)} \\, [x^2]_b^a\\]\n\\[\\mu = \\frac{1}{2(b-a)} \\, [x^2]_b^a = \\frac{1}{2(b-a)}(b^2 - a^2) = \\frac{(b - a)(b + a)}{2(b-a)}\\]\n\\[\\mu = E[x] = \\frac{a+b}{2}\\]",
    "crumbs": [
      "Continuous Probability",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Uniform Distributions</span>"
    ]
  },
  {
    "objectID": "mod-6-1-uniform.html#expected-value-and-variance",
    "href": "mod-6-1-uniform.html#expected-value-and-variance",
    "title": "28  Uniform Distributions",
    "section": "28.9 Expected Value and Variance",
    "text": "28.9 Expected Value and Variance\nThe expected value of a continuous uniform distribution is:\n\\[\\mu = E[x] = \\frac{a+b}{2}\\]\nThe variance of a continuous uniform distribution is:\n\\[\\sigma^2 = var(x) = \\frac{(b-a)^2}{12}\\]\nLet \\(a = 10\\) and \\(b = 30\\) and \\(x\\) be continuously, uniformly distributed over this range.\n\\[\\mu = \\frac{a+b}{2} = \\frac{10+30}{2} = 20\\]\n\\[\\sigma^2 = \\frac{(b-a)^2}{12} = \\frac{(30-10)^2}{12} = 33.33\\]\n\\[\\sigma = \\sqrt{\\frac{(b-a)^{2}}{12}} = \\sqrt{\\frac{(30-10)^2}{12}} = \\sqrt{33.33} = 5.78\\]",
    "crumbs": [
      "Continuous Probability",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Uniform Distributions</span>"
    ]
  },
  {
    "objectID": "mod-6-1-uniform.html#uniform-inverse-cdf",
    "href": "mod-6-1-uniform.html#uniform-inverse-cdf",
    "title": "28  Uniform Distributions",
    "section": "28.10 Uniform Inverse CDF",
    "text": "28.10 Uniform Inverse CDF\nqunif is the inverse cumulative density function for a continuous uniform distribution.\nGiven \\(a = 30\\) and \\(b = 70\\), what is \\(x\\) such that \\(P(X \\le x) = 0.1\\)?\nWe can use the inverse CDF to generate \\(x\\) given \\(p\\).\nWe can use the CDF to generate \\(p\\) given \\(x\\).\nNote that the probabilities are equal unlike the discrete CDF and inverse CDF.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Continuous Probability",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Uniform Distributions</span>"
    ]
  },
  {
    "objectID": "mod-6-1-uniform.html#random-uniform",
    "href": "mod-6-1-uniform.html#random-uniform",
    "title": "28  Uniform Distributions",
    "section": "28.11 Random Uniform",
    "text": "28.11 Random Uniform\nrunif generates random deviates using the uniform PDF.\nGiven \\(a\\) and \\(b\\), we can generate 5 observations from a uniform distribution.\nIn this example, we use a loop to generate 3 sets of random observations from a uniform distribution with \\(a = 30\\) and \\(b = 70\\).\n\nrm(list = ls())\n\nlibrary(tidyverse)\nlibrary(kableExtra)\n\na &lt;- 30\nb &lt;- 70\n\n# Generate 3 columns of 5 uniform random observations\nran_unif &lt;- as_tibble(\n  replicate(3, runif(5, min = a, max = b)),\n    .name_repair = ~ paste0(\"Sample_\", seq_along(.))) %&gt;%\n    mutate(Observation = paste0(\"Observation_\", row_number())) %&gt;%\n    relocate(Observation)\n\nkable(ran_unif,\n      align     = \"c\",\n      caption   = \"Uniform Random Observations\",\n      col.names = c(\"Observation\", \"Sample 1\", \"Sample 2\", \"Sample 3\")) %&gt;%\nkable_styling(font_size = 12)\n\n\nUniform Random Observations\n\n\nObservation\nSample 1\nSample 2\nSample 3\n\n\n\n\nObservation_1\n63.73070\n50.52599\n56.18562\n\n\nObservation_2\n35.19521\n54.98192\n53.52402\n\n\nObservation_3\n66.34170\n43.16090\n39.94813\n\n\nObservation_4\n54.19482\n54.91170\n49.04226\n\n\nObservation_5\n40.80802\n56.39408\n55.26795",
    "crumbs": [
      "Continuous Probability",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Uniform Distributions</span>"
    ]
  },
  {
    "objectID": "mod-6-2-normal.html",
    "href": "mod-6-2-normal.html",
    "title": "29  Normal Distributions",
    "section": "",
    "text": "29.1 Normal Probability Distribution\nThe normal or Gaussian probability distribution is one (if not the one) of the most widely used probability distributions in statistics.\nWhy is the normal so useful?\nAccording to the Central Limit Theorem, the average of a random variable with finite mean and variance is a random variable whose distribution converges to a normal distribution as the number of samples increases.\nA continuous random variable \\(X\\) with mean \\(\\mu\\) and variance \\(\\sigma^2\\) is said to be a normal random variable if its PDF is given by:\n\\[f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\, e^{-\\frac{1}{2} (\\frac{x - \\mu}{\\sigma})^2}\\]\nA continuous random variable \\(x\\) is said to be a standard normal random variable if \\(\\mu = 1\\) and \\(\\sigma^2 = 1\\), shown as \\(x \\sim N(0,1)\\) and its PDF is given by:\n\\[f(x) = \\frac{1}{\\sqrt{2 \\pi}} \\, e^{-\\frac{x^2}{2}}\\]",
    "crumbs": [
      "Continuous Probability",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Normal Distributions</span>"
    ]
  },
  {
    "objectID": "mod-6-2-normal.html#expected-value",
    "href": "mod-6-2-normal.html#expected-value",
    "title": "29  Normal Distributions",
    "section": "29.2 Expected Value",
    "text": "29.2 Expected Value\nThe expected value of a normally distributed random variable given\n\\[X \\sim N(0,1)\\]\n\\[E[X] = \\int_{-\\infty}^{\\infty} \\, x \\, f(x) \\, dx\\]\n\\[E[X] = \\int_{-\\infty}^{\\infty} \\, x \\, \\frac{1}{\\sqrt{2 \\pi}} \\, e^{-\\frac{x^2}{2}} \\, dx\\]\n\\[E[X] =\\frac{1}{\\sqrt{2 \\pi}} \\, \\int_{-\\infty}^{\\infty} \\, x  \\, e^{-\\frac{x^2}{2}} \\, dx\\]\nSince \\(d(-\\frac{x^2}{2})=-xdx\\) then\n\\[E[X] =\\frac{1}{\\sqrt{2 \\pi}} \\, \\int_{-\\infty}^{\\infty} \\, e^{-\\frac{x^2}{2}} \\, d(-\\frac{x^2}{2})\\]\n\\[E[X] =\\frac{1}{\\sqrt{2 \\pi}} \\,  [e^{-\\frac{x^2}{2}}]_{-\\infty}^{\\infty} = 0\\]",
    "crumbs": [
      "Continuous Probability",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Normal Distributions</span>"
    ]
  },
  {
    "objectID": "mod-6-2-normal.html#variance-of-a-standard-normal",
    "href": "mod-6-2-normal.html#variance-of-a-standard-normal",
    "title": "29  Normal Distributions",
    "section": "29.3 Variance of a Standard Normal",
    "text": "29.3 Variance of a Standard Normal\nTo find the variance of the standard normal, note that\n\\[var(X) = E[X^2] - (E[X])^2 = E[X^2] = \\int_{-\\infty}^{\\infty} \\, x^2 \\, f(x) \\, dx\\]\n\\[var(X) = \\frac{1}{\\sqrt{2 \\pi}} \\, \\int_{-\\infty}^{\\infty} \\, x^2 \\,  \\, e^{-\\frac{x^2}{2}} \\, dx\\]\nSince \\(d(-\\frac{x^2}{2})=-xdx\\) then\n\\[var[X] = \\frac{1}{\\sqrt{2 \\pi}} \\, [-x  e^{-\\frac{x^2}{2}} ]_{-\\infty}^{\\infty} + \\frac{1}{\\sqrt{2 \\pi}} \\,\n\\int_{-\\infty}^{\\infty} \\, e^{-\\frac{x^2}{2}} \\, dx\\]\n\\[var[X] = \\int_{-\\infty}^{\\infty} \\, \\frac{1}{\\sqrt{2 \\pi}} \\,  \\, e^{-\\frac{x^2}{2}} \\, dx = 1\\]\nThe variance of the standard normal probability distribution is equal to 1 because it is the equal to the integration of the standard normal PDF from \\(-\\infty\\) to \\(\\infty\\).\nThus, we have shown that if \\(X \\sim N(0,1)\\) that \\(E[X] = 0\\) and \\(var(X) = 1\\).",
    "crumbs": [
      "Continuous Probability",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Normal Distributions</span>"
    ]
  },
  {
    "objectID": "mod-6-2-normal.html#cdf-of-standard-normal",
    "href": "mod-6-2-normal.html#cdf-of-standard-normal",
    "title": "29  Normal Distributions",
    "section": "29.4 CDF of Standard Normal",
    "text": "29.4 CDF of Standard Normal\nTo find the CDF of the standard normal, we would need to integrate the PDF of the standard normal or\n\\[F_X(x) = \\int_{-\\infty}^{x} \\, \\frac{1}{\\sqrt{2 \\pi}} \\,  \\, e^{-\\frac{x^2}{2}} \\, dx\\]\nHowever, since this integral does not have a closed-form solution, this cannot be done and the values of the CDF are available through tables and software.\nThe CDF of the standard normal is denoted with the \\(\\Phi\\) function or:\n\\[\\Phi(x) = P(Z \\le x) = \\frac{1}{\\sqrt{2 \\pi}} \\, \\int_{-\\infty}^{x} \\,   \\, e^{-\\frac{x^2}{2}} \\, dx\\]\nWhere \\(lim_{x \\rightarrow \\infty} \\Phi(x) = 1\\) and \\(lim_{x \\rightarrow -\\infty} \\Phi(x) = 0\\)\n\nWhere \\(\\Phi(0) = 1/2\\)\nWhere \\(\\Phi(-x) = 1 - \\Phi(x) \\quad \\forall x \\, \\in \\,\\mathbb{R}\\)",
    "crumbs": [
      "Continuous Probability",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Normal Distributions</span>"
    ]
  },
  {
    "objectID": "mod-6-2-normal.html#normal-plot",
    "href": "mod-6-2-normal.html#normal-plot",
    "title": "29  Normal Distributions",
    "section": "29.5 Normal Plot",
    "text": "29.5 Normal Plot\nWe can use the dnorm function to generate probabilities that are then graphed to provide a visual representation of the normal distribution.\nThe stat_function is part of ggplot2 and allows use to superimpose a function on top of an existing plot.\nHere we generate 600 observations observations that are normally distributed using the rnorm function.\nWe then generate the density plot using for a normal distribution with \\(\\mu = 500\\) and \\(\\sigma = 100\\).\n\nrm(list = ls())\n\nset.seed(2525)\n\ndf_norm &lt;- tibble(x = rnorm(600, mean = 500, sd = 100))\n\nggplot(data = df_norm, \n       aes(x),\n       size  = 1.2) +\ngeom_density(linewidth = 1.2,\n             color = \"dark blue\") +\nstat_function(fun  = dnorm,\n              args  = list(mean = 500, sd = 100),\n              linewidth  = 1.2,\n              color = \"red\") +\ntheme_minimal() +\nlabs(x = \"x\",\n     y = \"Density\")\n\nWarning in fortify(data, ...): Arguments in `...` must be used.\n✖ Problematic argument:\n• size = 1.2\nℹ Did you misspell an argument name?",
    "crumbs": [
      "Continuous Probability",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Normal Distributions</span>"
    ]
  },
  {
    "objectID": "mod-6-2-normal.html#pdf-and-cdf-examples",
    "href": "mod-6-2-normal.html#pdf-and-cdf-examples",
    "title": "29  Normal Distributions",
    "section": "29.6 PDF and CDF Examples",
    "text": "29.6 PDF and CDF Examples\nAssume that IQ scores are normally distributed with a mean of 100 and a standard deviation of 15.\n\nWhat is the percentage of people with an IQ score less than or equal to 125?\nWhat is the percentage of people with an IQ score greater than or equal 110?\nWhat is the percentage of people with an IQ score between 110 and 125?\nWhat IQ score separates the lower 25% from the others or \\(P_{25}\\)?\nWhat IQ score separates the top 10% from the others or \\(P_{90}\\)?\n\n\nrm(list = ls())\n\nmean_iq &lt;- 100\nsd_iq &lt;- 15\n\nnormal_1 &lt;- tibble(p_le_125  &lt;- pnorm(125, mean = 100, sd = 15),\n                   p_ge_110  &lt;- pnorm(110, mean = 100, sd = 15, \n                                  lower.tail = FALSE),\n                   p_110_125 &lt;- pnorm(125,100,15) - pnorm(110, 100, 15),\n                   q_25      &lt;- qnorm(.25,100,15),\n                   q_10      &lt;- qnorm(.90,100,15))\n\n\nkable(normal_1,\n      align     = 'c',\n      digits    = 3,\n      caption   = 'Normal Distribution Example',\n      col.names = c(\"P(X &lt;= 125)\", \"P(X &gt;= 110)\", \n                    \"P(110 &lt;= X &lt;= 125)\",\n                    \"P(0.25)\", \"P(0.90)\")) %&gt;%\nkable_styling(font_size = 12)\n\n\nNormal Distribution Example\n\n\nP(X &lt;= 125)\nP(X &gt;= 110)\nP(110 &lt;= X &lt;= 125)\nP(0.25)\nP(0.90)\n\n\n\n\n0.952\n0.252\n0.205\n89.883\n119.223",
    "crumbs": [
      "Continuous Probability",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Normal Distributions</span>"
    ]
  },
  {
    "objectID": "mod-6-2-normal.html#z-score-example",
    "href": "mod-6-2-normal.html#z-score-example",
    "title": "29  Normal Distributions",
    "section": "29.7 Z-Score Example",
    "text": "29.7 Z-Score Example\nSAT scores are distributed normally with a mean score of 1150 and a standard deviation of 150 points.\nWhat is the probability of an SAT score exceeding 1380?\nWe can tackle this several ways.\nFirst, we can estimate the Z-score for 1380 given \\(\\mu = 1150\\) and \\(\\sigma = 150\\) or:\n\\[\nZ = \\frac{x - \\mu}{\\sigma} = \\frac{1380 - 1150}{150} = 1.53\n\\]\nWe can use the pnorm function in area with lower.tail = TRUE to estimate, for a standard normal distribution, the probability that Z is less than or equal to 1.53. We can also estimate the probability of Z being greater than 1.53 using the lower.tail = FALSE option.\n\\[\nP(Z \\le 1.53) = 0.937\n\\]\n\\[\nP(Z &gt; 1.53) = 1 - P(Z \\le 1.53) = 1 - 0.937 - 0.063\n\\]\nThe code chunk below does the following:\n\nDeclares the mean, standard deviation, and value of interest for \\(x\\)\nEstimates the Z-score for \\(x\\) given \\(\\mu\\) and \\(\\sigma\\)\nEstimates \\(P(Z)\\)\nEstimates \\(1-P(Z)\\) which is equivalent to \\(P(X&gt;1380)\\)\nEstimates \\(1- P(Z)\\) using the lower.tail = FALSE option\nEstimates \\(P(X &gt; 1380)\\) directly\n\nThe code chunk then creates a graphic that corresponds to the normal distribution with a mean of 1150 and a standard deviation of 150 and identifies the region where \\(P(X &gt;1380)\\).\n\nCreate a tibble where \\(X\\) is a sequence of numbers, where the sequence of numbers is from 4 standard deviations to the left of the mean to 4 standard deviations to the right of the mean\nUsing \\(X\\), create \\(Y\\) where \\(Y\\) is equal to \\(P(X = x)\\)\nCreate a tibble which only has values where \\(X \\ge 1380\\)\nPlots the resulting normal distribution\nPlots the shaded area representing \\(P(X &gt; 1380)\\)\n\n\nrm(list = ls())\n\nlibrary(dplyr)\nlibrary(kableExtra)\n\nmu = 1150\nsigma = 150\nx = 1380\n\nnorm &lt;- tibble(z = (x-mu)/sigma,\n               p_1 &lt;- pnorm(z, 0, 1),\n               p_2 &lt;- 1-pnorm(z, 0, 1),\n               p_3 &lt;- pnorm(z, 0, 1, lower.tail = FALSE),\n               p_4 &lt;- pnorm(x, mu, sigma, lower.tail = FALSE))\n\nkable(norm,\n      align  = 'c',\n      digits = 4,\n      col.names = c('Z', 'P(Z)', '1-P(Z)', '1-P(Z)', 'P(X &gt; 1380')) %&gt;%\nkable_classic()\n               \n\n# Create data for the normal curve\ncurve_data &lt;- tibble(X = seq(mu - 4 * sigma, \n                             mu + 4 * sigma, length.out = 1000),\n                     Y = dnorm(X, mean = mu, sd = sigma))\n\n# Data for shaded region (right tail)\nshade_data &lt;- curve_data %&gt;%\n              filter(X &gt;= x)\n\n# Plot\nggplot(curve_data, \n       aes(x = X, \n           y = Y)) +\ngeom_line(linewidth = 1, color = \"steelblue\") +\ngeom_area(data = shade_data, \n          aes(x = X, \n              y = Y),\n          fill = \"steelblue\", \n          alpha = 0.4) +\ngeom_vline(xintercept = x, \n           linetype = \"dashed\", \n           color = \"red\") +\nannotate(\"text\", \n         x = x + 40, \n         y = 0.001,\n         label = paste0(\"P(X &gt; 1380) = \",\n         round(pnorm(x, mu, sigma, lower.tail = FALSE), 4)),\n         hjust = 0, \n         color = \"black\") +\nlabs(title = \"Normal Distribution with Shaded Tail Where P(X&gt;1380)\",\n     x = \"X\",\n     y = \"Density\") +\ntheme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n\n\n\nZ\nP(Z)\n1-P(Z)\n1-P(Z)\nP(X &gt; 1380\n\n\n\n\n1.5333\n0.9374\n0.0626\n0.0626\n0.0626",
    "crumbs": [
      "Continuous Probability",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Normal Distributions</span>"
    ]
  },
  {
    "objectID": "mod-6-2-normal.html#pdf-example",
    "href": "mod-6-2-normal.html#pdf-example",
    "title": "29  Normal Distributions",
    "section": "29.8 PDF Example",
    "text": "29.8 PDF Example\nFor a given random variable x, the dnorm function returns the value of the probability density function (PDF) of the normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\).\nWe can generate a sequence of numbers from 1 to 100 and generate the value of the PDF for each of these numbers given that we have assumed that \\(\\mu = 50\\) and \\(\\sigma = 3.5\\). We also generate a second set of probabilities, however, these are using \\(\\mu = 50\\) and \\(\\sigma = 1.5\\).\nWe can also use stat_function to compute and draw a function of a continuous curve. Here we compare the distribution where \\(\\mu = 50\\) and \\(\\sigma = 3.5\\) with \\(\\mu =45\\) and \\(\\sigma = 4.5\\).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Continuous Probability",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Normal Distributions</span>"
    ]
  },
  {
    "objectID": "mod-6-2-normal.html#cdf-example",
    "href": "mod-6-2-normal.html#cdf-example",
    "title": "29  Normal Distributions",
    "section": "29.9 CDF Example",
    "text": "29.9 CDF Example\nThe cumulative density function (CDF) of the normal distribution is used to answer such questions as \\(P(X \\le x)\\) and \\(P(X &gt; x)\\) and can be thought of as the area underneath the normal distribution curve at \\(x\\).\nFor example, if the mean height of students at a school is 68 inches and the standard deviation is 4.25 inches, what is the probability of observing a student with a height greater than 75 inches?\n\\[P(X &gt; 75) = 1 - P(X \\le 75)\\]\nThe probability of observing a student with a height greater than 75 inches is 0.05 (or 95% of students have heights 75 inches and less).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nLet’s say we want to find \\(P(a \\le x \\le b)\\) with respect to a normal distribution. Here again we can use the pnorm function to estimate this probability.\nUsing the previous example, what is the probability of observing a student with a height between 63 and 67 inches given that \\(\\mu = 68\\) and \\(\\sigma = 4.25\\)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Continuous Probability",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Normal Distributions</span>"
    ]
  },
  {
    "objectID": "mod-6-2-normal.html#shading-part-of-a-curve",
    "href": "mod-6-2-normal.html#shading-part-of-a-curve",
    "title": "29  Normal Distributions",
    "section": "29.10 Shading part of a curve",
    "text": "29.10 Shading part of a curve\nIn the code below, we create a wrapper function. We first set the lower and upper bounds (defined in the previous example). The function uses values of x and the PDF of a normal distribution where \\(\\mu = 68\\) and \\(\\sigma = 4.25\\).\nWhen x is below the lower bound, the function returns “NA” values. When is between the bounds, it evaluates dnorm(x, mu, sigma) and returns the value of the PDF at x and assigns the value to y.\nWe can then use the stat_function argument to plot the normal distribution and then use stat_function again to map an area that is filled in.\nNote that the dnorm_limit function will return values that correspond to the PDF of the normal within the lower and upper bound and it will have “NA” values outside the bounds. So, you end up plotting a shaded area that corresponds to the previous example.\n\nmu = 68\nsd = 4.25\n\n#Create a limiting function\n\nlower_bound &lt;- 63\nupper_bound &lt;- 67\n\ndnorm_limit &lt;- function(x) {\n    y &lt;- dnorm(x, mean = mu, sd = sd)\n    y[x &lt; lower_bound  |  x &gt; upper_bound] &lt;- NA\n    return(y)\n}\n\n# ggplot() with dummy data\n \nggplot(data.frame(x = c(50, 90)), \n       aes(x = x)) +\n  stat_function(fun = dnorm, args = list(mean = mu, sd = sd)) +\n  stat_function(fun = dnorm_limit, geom = \"area\", fill = \"blue\", alpha = 0.2) +\nscale_y_continuous(labels   = scales::label_number(a=0.01),\n                   limits   = c(0,0.1),\n                   n.breaks = 10 ) +\ntheme_minimal() +\ntheme(legend.position = \"\") +\nlabs(x = \"X\",\n     y = \"P(X)\")\n\nWarning: Removed 91 rows containing missing values or values outside the scale range\n(`geom_area()`).",
    "crumbs": [
      "Continuous Probability",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Normal Distributions</span>"
    ]
  },
  {
    "objectID": "mod-6-2-normal.html#inverse-cdf-example",
    "href": "mod-6-2-normal.html#inverse-cdf-example",
    "title": "29  Normal Distributions",
    "section": "29.11 Inverse CDF Example",
    "text": "29.11 Inverse CDF Example\nThe inverse cumulative density function gives the value associated with a specific cumulative probability. Use the inverse CDF to determine the value of the variable associated with a specific probability.\nFor example, we want to know the 90th percentile in height in the previous example, that is,\n\\[P(X \\le x) = .9\\]\nIt turns out the answer is 73.447.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Continuous Probability",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Normal Distributions</span>"
    ]
  },
  {
    "objectID": "mod-6-3-exponential.html",
    "href": "mod-6-3-exponential.html",
    "title": "30  Exponential Distributions",
    "section": "",
    "text": "30.1 Exponential Distribution\nThe exponential probability distribution is the probability distribution of the time between events in a Poisson process.\nAn assumption is that events occur continuously and independently at a constant average rate\nThe exponential probability distribution is memoryless, that is, the distribution of ‘waiting time’ until a certain event does not depend on how much time has elapsed.\nLet the random variable, \\(x\\), represent the time until the next event, then:\n\\[P(X &gt; s+t | X &gt; t) = P(X &gt; s)\\]\nThe probability density function of the exponential probability distribution is: \\[P(x;\\lambda) = \\lambda \\, e^{-\\lambda \\, x} \\quad \\forall \\, x \\ge 0\\]\n\\[P(x;\\lambda) = 0 \\quad \\forall \\, x &lt; 0\\]\n\\(\\lambda\\) is the parameter of the distribution and can be thought of as the rate parameter.",
    "crumbs": [
      "Continuous Probability",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Exponential Distributions</span>"
    ]
  },
  {
    "objectID": "mod-6-3-exponential.html#exponential-properties",
    "href": "mod-6-3-exponential.html#exponential-properties",
    "title": "30  Exponential Distributions",
    "section": "30.2 Exponential Properties",
    "text": "30.2 Exponential Properties\nLet \\(\\mu\\) be the expected value or mean of the exponential probability distribution.\nLet \\(x\\) represent a random variable whose behavior is described by an exponential probability distribution.\nLet \\(\\mu\\) also be defined in terms of the rate parameter \\(\\lambda\\) or:\n\\[E[x] = \\mu = \\frac{1}{\\lambda}\\]\n\\[var(x) = \\frac{1}{\\lambda^2}\\]\nThe PDF of the exponential distribution can also be stated as:\n\\[f(x;\\mu) = \\frac{1}{\\mu} \\, e^{-(x/ \\mu)} \\quad \\forall \\, x \\ge 0, \\quad 0 \\, \\text{otherwise}\\]\nGiven this discussion, we can state the CDF of the exponential distribution in two ways:\n\\[F(x;\\lambda) = \\int_{-\\infty}^{x} \\, f(x) dx = 1 - e^{-\\lambda \\, x} \\quad \\forall \\, x \\ge 0, \\quad 0 \\, \\text{otherwise}\\]\n\\[F(x;\\mu) = 1 - e^{-x / \\mu} \\quad \\forall \\, x \\ge 0, \\quad 0 \\, \\text{otherwise}\\]",
    "crumbs": [
      "Continuous Probability",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Exponential Distributions</span>"
    ]
  },
  {
    "objectID": "mod-6-3-exponential.html#exponential-example",
    "href": "mod-6-3-exponential.html#exponential-example",
    "title": "30  Exponential Distributions",
    "section": "30.3 Exponential Example",
    "text": "30.3 Exponential Example\nAssume the time a customer spends in a bank is exponentially distributed with mean 15 minutes\n\nWhat is the probability that a customer spends less than six minutes in the bank?\nWhat is the probability that a customer spends more than 20 minutes?\nWhat is the probability that a customer spends between 12 and 18 minutes?\nWhat the number of minutes that separates the lowest 20% of customers from the others?\nWhat is the number of minutes that separates the top 15% of customers from the others?\n\n\nrm(list = ls())\n\nset.seed(123)\n\nlibrary(dplyr)\nlibrary(kableExtra)\n\nmu = 15\nlambda = 1/mu\n\nexp_1 &lt;- tibble(q1 &lt;- pexp(6, lambda, lower.tail = TRUE),\n                q2 &lt;- pexp(20, lambda, lower.tail = FALSE),\n                q3 &lt;- pexp(18, lambda, lower.tail = TRUE) -\n                      pexp(12, lambda, lower.tail = TRUE),\n                q4 &lt;- qexp(0.2, lambda, lower.tail = TRUE),\n                q5 &lt;- qexp(0.15, lambda, lower.tail = FALSE))\n\nkable(exp_1,\n      align     = 'c',\n      digits    = 3,\n      caption   = 'Exponential Distribution Example',\n      col.names = c('P(X &lt; 6)', \n                    'P(X &gt; 20)', \n                    'P(12 &lt; X &lt; 18)',\n                    'P(20)', \n                    'P(85)')) %&gt;%\nkable_styling(font_size = 12)\n\n\nExponential Distribution Example\n\n\nP(X &lt; 6)\nP(X &gt; 20)\nP(12 &lt; X &lt; 18)\nP(20)\nP(85)\n\n\n\n\n0.33\n0.264\n0.148\n3.347\n28.457",
    "crumbs": [
      "Continuous Probability",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Exponential Distributions</span>"
    ]
  },
  {
    "objectID": "mod-6-3-exponential.html#random-exponential",
    "href": "mod-6-3-exponential.html#random-exponential",
    "title": "30  Exponential Distributions",
    "section": "30.4 Random Exponential",
    "text": "30.4 Random Exponential\nrexp generates random deviates using the exponential PDF.\nThe constant rate \\(\\lambda\\) describes the arrival rate per each time unit.\nWe can simulate arrival times \\(x\\) arriving at rate \\(\\lambda = 15\\) per time unit.\nWe simulate 3 columns of random exponential observations.\nNote that \\(E[x] = \\mu = 1 /\\lambda = 1/15 = 0.067\\)\n\nrm(list = ls())\n\nset.seed(123)\n\nlibrary(dplyr)\nlibrary(kableExtra)\n\nexp_list &lt;- list()\n\nlambda &lt;- 15\n\nran_exp &lt;- tibble(replicate(3, rexp(5, lambda)))\n\nkable(ran_exp[1:5,],\n      align     = 'c',\n      caption   = 'Exponential Random Observations',\n      col.names = c('1', '2', '3')) %&gt;%\nkable_styling(font_size = 12)\n\n\nExponential Random Observations\n\n\n1\n2\n3\n\n\n\n\n0.056230484\n0.021100081\n0.066988671\n\n\n0.038440685\n0.020948486\n0.032014315\n\n\n0.088603658\n0.009684454\n0.018734242\n\n\n0.002105157\n0.181749098\n0.025141189\n\n\n0.003747398\n0.001943563\n0.012552269",
    "crumbs": [
      "Continuous Probability",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Exponential Distributions</span>"
    ]
  },
  {
    "objectID": "mod-6-3-exponential.html#exponential-plot",
    "href": "mod-6-3-exponential.html#exponential-plot",
    "title": "30  Exponential Distributions",
    "section": "30.5 Exponential Plot",
    "text": "30.5 Exponential Plot\nHere we can use the dexp function to generate probabilities that are then graphed to provide a visual representation of the exponential distribution.\nWe generate observations that are exponentially distributed using the rexp function with \\(\\lambda = 5\\)\nRecall that this means that \\(E(x) = \\mu = 1/\\lambda = 1/5\\).\nWe compare that to the exponential distribution where \\(\\lambda = 10\\).\n\nrm(list = ls())\n\nset.seed(123)\n\nlibrary(dplyr)\nlibrary(kableExtra)\n\nlambda = 5\n\ndf_exp &lt;- tibble(x = seq(0,1, by = 0.001),\n                 y = dexp(x, rate = 5))\n\nggplot(data = df_exp, \n       aes(x = x,\n           y = y)) +\ngeom_line(linewidth = 1.2,\n          color = \"dark blue\") +\nstat_function(fun = dexp,\n              args = list(rate = 10),\n              color = \"red\") +\ntheme_minimal() +\nlabs(x = \"x\",\n     y = \"f(x)\")",
    "crumbs": [
      "Continuous Probability",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Exponential Distributions</span>"
    ]
  },
  {
    "objectID": "mod-6-3-exponential.html#wrapping-up",
    "href": "mod-6-3-exponential.html#wrapping-up",
    "title": "30  Exponential Distributions",
    "section": "30.6 Wrapping Up",
    "text": "30.6 Wrapping Up\nA continuous probability distribution is where the random variable X can take on any value.\nSince there are infinite values for X, the probability of X taking on a specific value is 0, that is, \\(P(X = x) = 0\\).\nInstead, we often examine the probability of \\(P(X \\le x)\\) or \\(P(X &gt; x)\\).\nThere are numerous continuous probability distributions, we examined the uniform, normal, and exponential probability distributions.\nThe most commonly used continuous probability distribution is the normal distribution.\nWe have used the empirical rule, that is, 68%-95%-99.7% rule to discuss the proportion of observations that lie within 1,2,and 3 standard deviations of the mean.\nWe can now be much more precise with regards to this question.",
    "crumbs": [
      "Continuous Probability",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Exponential Distributions</span>"
    ]
  },
  {
    "objectID": "mod-7-0-overview.html",
    "href": "mod-7-0-overview.html",
    "title": "31  Module 7 Overview",
    "section": "",
    "text": "31.1 Introduction\nWelcome to Module 7 of ECON 700.\nThis module introduces the concepts of sampling and sampling distributions, emphasizing how samples drawn from populations form the basis for statistical inference. Students begin by exploring different sampling methods and learn how sampling variability arises. The module demonstrates that while each sample may differ, the sampling distribution of the sample mean or proportion has predictable properties that enable estimation of population parameters. Learners use simulations and R-based visualizations to see how the shape and spread of sampling distributions change with sample size and variability in the population.\nBuilding on these concepts, the module introduces the Central Limit Theorem (CLT) and its foundational role in inferential statistics. Students discover how, regardless of the population distribution, the sampling distribution of the mean approaches a normal distribution as sample size increases. The module applies these principles to compute the standard error, evaluate probabilities for sample means and proportions, and interpret results within real-world economic and data-driven contexts. By the end, students understand how sampling distributions link descriptive statistics to inferential conclusions about populations.",
    "crumbs": [
      "Sampling Distributions",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Module 7 Overview</span>"
    ]
  },
  {
    "objectID": "mod-7-0-overview.html#learning-objectives",
    "href": "mod-7-0-overview.html#learning-objectives",
    "title": "31  Module 7 Overview",
    "section": "31.2 Learning Objectives",
    "text": "31.2 Learning Objectives\nBy the end of this module, you should be able to:\n\nMLO 1: Explain the purpose and importance of sampling in statistical inference. (CLO 1, 2)\nMLO 2: Differentiate among simple random, stratified, and cluster sampling techniques. (CLO 1, 2)\nMLO 3: Describe how sampling variability affects sample statistics and their relationship to population parameters. (CLO 1, 2, 4)\nMLO 4: Apply the Central Limit Theorem to explain the behavior of sampling distributions for means and proportions. (CLO 1, 2, 4)\nMLO 5: Calculate and interpret the standard error of sample means and proportions. (CLO 1, 2, 4)\nMLO 6: Use R to simulate and visualize sampling distributions and assess how sample size influences variability and normality. (CLO 1, 2, 3, 4, 5)",
    "crumbs": [
      "Sampling Distributions",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Module 7 Overview</span>"
    ]
  },
  {
    "objectID": "mod-7-0-overview.html#required-texts",
    "href": "mod-7-0-overview.html#required-texts",
    "title": "31  Module 7 Overview",
    "section": "31.3 Required Texts",
    "text": "31.3 Required Texts\nThe following textbooks are available for free. Please select the links provided to access.\n\nBarbara Illowsky and Susan Dean. Introductory Statistics 2e. OpenStax. Chapter 5 - 7.\nRafael Irizarry. (2025). Introduction to Data Science: Data Wrangling and Visualization with R. Chapters 1-2.\nHadley Wickham, Mine Cetinkaya-Rundel, and Garrett Grolemund. (2025) R for Data Science. Sections 1-19.\nOnline R documentation\nRStudio Cheat Sheets",
    "crumbs": [
      "Sampling Distributions",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Module 7 Overview</span>"
    ]
  },
  {
    "objectID": "mod-7-0-overview.html#activities",
    "href": "mod-7-0-overview.html#activities",
    "title": "31  Module 7 Overview",
    "section": "31.4 Activities",
    "text": "31.4 Activities\n\nComplete the hands-on coding exercises embedded in each lesson (MLO 3)\n\nComplete the weekly class assignment (MLO 1 - MLO 6)\n\nParticipate in the discussion form (MLO 3 - MLO 5)\n\nTake the weekly knowledge quiz (MLO 1 - MLO 6)",
    "crumbs": [
      "Sampling Distributions",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Module 7 Overview</span>"
    ]
  },
  {
    "objectID": "mod-7-0-overview.html#lessons-in-this-module",
    "href": "mod-7-0-overview.html#lessons-in-this-module",
    "title": "31  Module 7 Overview",
    "section": "31.5 Lessons in this Module",
    "text": "31.5 Lessons in this Module\n\n7.1 – Sampling\n7.2 – Central Limit Theorem\n7.3 – Interval Estimation\n\n\nNext: Start with Sampling.",
    "crumbs": [
      "Sampling Distributions",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Module 7 Overview</span>"
    ]
  },
  {
    "objectID": "mod-7-1-sampling.html",
    "href": "mod-7-1-sampling.html",
    "title": "32  Sampling",
    "section": "",
    "text": "32.1 Terminology\nAn element is the entity on which data are collected.\nA population is a collection of all the elements of interest.\nA sample is a subset of the population.\nThe sampled population is the population from which the sample is drawn\nA frame is a list of elements from which the sample will be selected.\nA sample provides estimates of the population characteristics.\nThe question is whether the sample estimates are representative of the population characteristics?\nTo answer this question, we need to ask what sampling methods produce “reliable” estimates?",
    "crumbs": [
      "Sampling Distributions",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "mod-7-1-sampling.html#finite-population-sampling",
    "href": "mod-7-1-sampling.html#finite-population-sampling",
    "title": "32  Sampling",
    "section": "32.2 Finite Population Sampling",
    "text": "32.2 Finite Population Sampling\nLet \\(N\\) be the population size where \\(N &lt; \\infty\\) so \\(N\\) is finite.\nLet \\(n\\) be the sample size where \\(n \\le N\\).\nA simple random sample of size \\(n\\) from a finite population of size \\(N\\) is a sample selected such that each possible sample of size \\(n\\) has the same probability of being selected.\nWe can sample with replacement, however, sampling without replacement is more typically used.\nRemembering our discussion of combinations, we know that the number of different samples of size \\(n\\) that can be drawn from the population of size \\(N\\) is:\n\\[\\frac{N!}{n!(N-n)!}\\]",
    "crumbs": [
      "Sampling Distributions",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "mod-7-1-sampling.html#finite-sampling-example",
    "href": "mod-7-1-sampling.html#finite-sampling-example",
    "title": "32  Sampling",
    "section": "32.3 Finite Sampling Example",
    "text": "32.3 Finite Sampling Example\nCreate a finite population of numbers from 0 to 100,000, sequenced by 1.\nThe population mean is\n\\[\\mu = \\sum_{i = 1}^N \\frac{x_i}{N}\\]\nGenerate 5 samples of size \\(n = 100\\) without replacement.\nThe sample average is:\n\\[\\bar{x} = \\sum_{i = 1}^n \\frac{x_i}{n}\\]\nThe population mean is 50,000, however, the sample averages vary “around” the population mean.\nIf you run the code multiple times, the population mean does not vary, but the sample averages will change with each run as we have not set.seed the code.\nThe code below does the following:\n\nCreates a population of integers, sequenced by 1, from 0 to 100,000\nSets the samples variable to 5\nCreates a vector, avg_x, filled with zeros with the number of columns set by samples\nRuns a loop where the number of loops is set by the samples variable\nIn each loop, takes a random sample of 100 from the population and estimates the average\nCreates a tibble of the averages and row binds the population mean\nTransposes the vector of averages and the population mean\nProduces a table of results\n\n\nrm(list = ls())\n\nlibrary(dplyr)\nlibrary(kableExtra)\n\npopulation &lt;- seq(0, 100000, by = 1) \n\nsamples &lt;- 5\navg_x &lt;- rep(0,samples)\n\nfor(i in 1:samples){\n  avg_x[i] &lt;- mean(sample(population, size = 100, replace = FALSE))\n}\n\nsamples &lt;- tibble(avg_x) %&gt;% \n           rbind(mean(population)) %&gt;% t()\n\nkable(samples,\n      align       = \"c\", \n      format.args = list(big.mark = \",\", \n                         scientific = FALSE),\n      caption     = \"Sample Averages and Population Mean\",\n      col.names   = c(\"Sample 1\", \"Sample 2\", \n                      \"Sample 3\", \"Sample 4\",\n                      \"Sample 5\", \"Population\")) %&gt;%\nkable_styling(font_size = 10)\n\n\nSample Averages and Population Mean\n\n\n\nSample 1\nSample 2\nSample 3\nSample 4\nSample 5\nPopulation\n\n\n\n\navg_x\n50,824.44\n50,217.97\n54,339.49\n50,945.34\n48,033.13\n50,000",
    "crumbs": [
      "Sampling Distributions",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "mod-7-1-sampling.html#infinite-population-sampling",
    "href": "mod-7-1-sampling.html#infinite-population-sampling",
    "title": "32  Sampling",
    "section": "32.4 Infinite Population Sampling",
    "text": "32.4 Infinite Population Sampling\nWe can also sample from a population that continues to change over time.\n\\[N(t) &lt; N(t+1) &lt; N(t+2) &lt; \\dots\\]\nIn this case, we cannot find a list of all the elements in the population because the population is increasing without limit.\nWe cannot randomly select observations since the probability of selection a sample of size \\(n\\) is dependent upon when the population was sampled.\nIf we consider population of students that have enrolled at ODU, this number changes within each semester and across semesters. The population of current and former students is not static and theoretically could increase without bound.\nTo sample the ODU population we need to fulfill two conditions.\n\nEach element in the random sample must come from the population of interest.\nEach element in the random sample is selected independently.",
    "crumbs": [
      "Sampling Distributions",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "mod-7-1-sampling.html#infinite-sampling-example",
    "href": "mod-7-1-sampling.html#infinite-sampling-example",
    "title": "32  Sampling",
    "section": "32.5 Infinite Sampling Example",
    "text": "32.5 Infinite Sampling Example\nAssume we want to construct a sample of 1,000 ODU students and estimate average household income.\nFirst, we need to ensure each element in the random sample comes from the population of interest.\nOur sampling method would need to randomly select ODU students from the same time period.\nIn other words, the 2025 - 2026 Academic Year rather than across the last two decades as our interest is estimating the household income of the current student population.\nWhy?\nIf we sampled household income from 1970, would that be representative of the household income of the current and future student population?\nIn other words, currency of data matters.\nSecond, we need to ensure each element is selected independently to avoid selection bias.\nWe should not randomly sample from students at Webb Center or students in the commuter car parking lot. Both sampling mechanicisms would produce biased samples of the ODU population.\nWhat if we found students who purchased from the bookstore during the last week of October?\nWe could find each student in the registrar list and then select the student \\(x\\) number of positions alphabetically in front or behind the student.\nThe purchases of students at the bookstore are random and independent of other students. Thus, the selection of students \\(x\\) positions in front or behind in the alphabetical list of students yields a random sample that is independently drawn from the infinite population.",
    "crumbs": [
      "Sampling Distributions",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "mod-7-1-sampling.html#estimators-and-estimates",
    "href": "mod-7-1-sampling.html#estimators-and-estimates",
    "title": "32  Sampling",
    "section": "32.6 Estimators and Estimates",
    "text": "32.6 Estimators and Estimates\nPoint estimation is the use of sample data to estimate a sample statistic.\nAn estimator is a rule for calculating an estimate of a given quantity based on observed data.\n\nThe sample average, \\(\\bar{x}\\) is an estimator for the population mean.\nThe estimator \\(\\bar{x}\\) produces an estimate of the population mean \\(\\mu\\) using the \\(n\\) observations,\n\\(s^2\\) is the estimator of the population parameter \\(\\sigma^2\\).\n\\(s\\) is the estimator of the population parameter \\(\\sigma\\).\n\\(\\bar{p}\\) is the estimator of the population proportion \\(p\\).\n\nIn the code below, we demonstrate how the sample average and sample standard deviation are estimators of the population mean and the population standard deviation.\nWe generate point estimates using:\n\\[\\bar{x} = \\sum_{i = 1}^n x_i / n\\]\n\\[s = \\sqrt{ \\frac{\\sum_{i = 1}^n(x_i - \\bar{x})^2}{n-1}}\\]\nThe code below does the following:\n\nGenerates 1,000,000 random numbers drawn from a normal distribution\nThe mean of the normal distribution is 25 with a standard deviation of 3\nDeclares 5 samples will be taken\nCreates three vectors to be filled by the loop\nA sample of 500 is taken from the population\nThe sample number is set to the iteration of the loop\nThe sample average and sample standard deviation are estimated\nA tibble is created of all three variables\nA table of output is produced\n\n\nrm(list = ls())\n\nlibrary(dplyr)\nlibrary(kableExtra)\n\npopulation &lt;- rnorm(1000000, mean = 25, sd = 3) \n\nsamples &lt;-5 \nsamp_no &lt;- rep(0,samples)\nmean_vec &lt;- rep(0,samples)\nsd_vec &lt;- rep(0,samples)\n\nfor(i in 1:samples){\n  temp &lt;- sample(population, size = 500, replace = FALSE)\n  samp_no[i] &lt;- i\n  mean_vec[i] &lt;- mean(temp)\n  sd_vec[i] &lt;- sd(temp)\n}\n\nsamples &lt;- tibble(samp_no, mean_vec,sd_vec)\n\nkable(samples,\n      align       = \"ccc\", \n      digits      = 2,\n      caption     = \"Sample and Population Averages\",\n      col.names   = c(\"Sample\",\"Average\", \"SD\")) %&gt;%\nkable_styling(font_size = 10)\n\n\nSample and Population Averages\n\n\nSample\nAverage\nSD\n\n\n\n\n1\n24.91\n3.13\n\n\n2\n24.82\n3.06\n\n\n3\n25.02\n2.89\n\n\n4\n24.86\n3.19\n\n\n5\n24.91\n2.94\n\n\n\n\n\nAn illustrative exercise is to increase the sample size relative to the population and observe what happens to the estimates of the mean and standard deviation.",
    "crumbs": [
      "Sampling Distributions",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "mod-7-2-central.html",
    "href": "mod-7-2-central.html",
    "title": "33  Central Limit Theorem",
    "section": "",
    "text": "33.1 Unbiasedness\nAn estimator of a population parameter is unbiased if the expected value of the estimator is equal to the value of the population parameter.\nMore formally, the sample statistic \\(\\hat{\\theta}\\) is an unbiased estimator of the population parameter \\(\\theta\\) if:\n\\[E[\\hat{\\theta}] = \\theta\\]\nIn other words, an estimator is unbiased if it produces parameter estimates that are, on average, equal to the population parameter.\nAn estimator that is not unbiased is a biased estimator.\nThe bias of an estimator is equal to its expected value minus the value of the population parameter or:\n\\[\nBias(\\hat{\\theta}) = E[\\hat{\\theta}] - \\theta\n\\]\nIn other words, an estimator is unbiased if its bias is equal to zero.\nThe sample average is an unbiased estimator of the population mean as:\n\\[\nE[\\bar{x}] = \\mu\n\\]\nIt can be shown that the unadjusted sample variance is a biased estimator of the population variance or:\n\\[\nE[S_n^2] = E[\\frac{1}{n}\\sum_{1}^{n}(X_i - \\bar{X})^2] = \\frac{n-1}{n}\\sigma^2\n\\]\nIt can also be shown that the adjusted sample variance is an unbiased estimator of the population variance or:\n\\[\nE[s^2] = E[\\frac{1}{n-1}\\sum_1^n(X_i - \\bar{X})^2 ] = \\sigma^2\n\\]\nWhy? When the population mean is unknown, we must estimate it with the sample average. We lose a degree of freedom as we employ an estimate instead of a parameter. The number of degrees of freedom is equal to the number of sample observations minus the number of parameters to be estimated.",
    "crumbs": [
      "Sampling Distributions",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "mod-7-2-central.html#efficiency",
    "href": "mod-7-2-central.html#efficiency",
    "title": "33  Central Limit Theorem",
    "section": "33.2 Efficiency",
    "text": "33.2 Efficiency\nAn efficient estimator has the smallest possible variance among all unbiased estimators.\nMore formally, the sample statistic \\(\\hat{\\theta}\\) is relatively efficient to \\(\\hat{\\hat{\\theta}}\\) when\n\\[var(\\hat{\\theta}) &lt; var(\\hat{\\hat{\\theta}})\\]",
    "crumbs": [
      "Sampling Distributions",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "mod-7-2-central.html#consistency",
    "href": "mod-7-2-central.html#consistency",
    "title": "33  Central Limit Theorem",
    "section": "33.3 Consistency",
    "text": "33.3 Consistency\nAn estimator of a population parameter is consistent if it converges in probability to the true value of the parameter as the sample size tends to infinity.\nThe sample statistic \\(\\hat{\\theta}\\) is consistent when\n\\[plim_{n \\rightarrow \\infty} \\, \\hat{\\theta} \\rightarrow \\theta\\]\nIt can be shown, for example, that the sample mean converges almost surely to the mean, which implies that the sample mean converges in probability to the mean or:\n\\[\nplim_{n \\rightarrow \\infty} \\, \\bar{X} \\rightarrow \\mu\n\\]\nWhile the unadjusted variance estimator is biased and the adjusted variance estimator is unbiased, it can be shown that both estimators are consistent estimators of the population variance.\n\\[\nplim_{n \\rightarrow \\infty} \\, S^2 = \\frac{1}{n}\\sum_{1}^{n}(X_i - \\bar{X})^2] \\rightarrow \\sigma^2\n\\] \\[\nplim_{n \\rightarrow \\infty} \\, s^2 = \\frac{1}{n-1}\\sum_{1}^{n}(X_i - \\bar{X})^2] \\rightarrow \\sigma^2\n\\]",
    "crumbs": [
      "Sampling Distributions",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "mod-7-2-central.html#law-of-large-numbers",
    "href": "mod-7-2-central.html#law-of-large-numbers",
    "title": "33  Central Limit Theorem",
    "section": "33.4 Law of Large Numbers",
    "text": "33.4 Law of Large Numbers\nThe Law of Large Numbers states that the average of an experiment will converge to the expected value as more trials of the experiment are performed.\nLet \\(\\bar{x_n}\\) be the sample average across \\(n\\) trials.\nLet \\(E[x] = \\mu\\).\nThe law of large numbers states that the sample average converges in probability towards the expected value or:\n\\[\\bar{x_n} \\rightarrow^P \\mu\\]\n\\[lim_{n \\rightarrow \\infty} P(|\\bar{x}_n - \\mu| &lt; \\epsilon) = 1, \\quad \\epsilon &gt;0 \\]\nThe strong law of large numbers states that the sample average converges almost surely to the expected value or:\n\\[\\bar{x} \\rightarrow^{a.s} \\mu\\]\n\\[lim_{n \\rightarrow \\infty} \\, P(\\bar{x}_n = \\mu) = 1\\]",
    "crumbs": [
      "Sampling Distributions",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "mod-7-2-central.html#uniform-example-of-lln",
    "href": "mod-7-2-central.html#uniform-example-of-lln",
    "title": "33  Central Limit Theorem",
    "section": "33.5 Uniform Example of LLN",
    "text": "33.5 Uniform Example of LLN\nWe can demonstrate the large of large numbers of a uniform distribution.\nThe expected value of a uniform distribution is\n\\[E[x] = (a + b) / 2\\]\nGiven \\(a = 1\\) and \\(b = 10\\) then\n\\[E[x] = (1 + 10)/2 = 5.5\\]\nThe code below does the following\n\nConstructs a population of 100,000 random observations from a uniform distribution\nEstablishes two vectors, one for the number of trials, one for the sample averages\nPerforms a loop\nInside the loop, the trial number is equal to the loop counter\nTakes a sample of size i and estimates the sample average\nPlaces the vectors of trial numbers and averages in a tibble\nConstructs a plot\n\nAs the sample size increases, the sample averages tend toward the population mean.\n\nrm(list = ls())\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(kableExtra)\n\npopulation &lt;- runif(100000, min = 1, max = 10)\n\nntrials = 10000\ntrials &lt;- rep(0,ntrials)\navgs &lt;- rep(0,ntrials)\n\nfor(i in 1:ntrials){\n  trials[i] &lt;- i\n  avgs[i] &lt;- mean(sample(population, i, replace = FALSE))\n}\n\nmean_frame &lt;- tibble(trials,avgs)\n\nggplot(mean_frame,\n       aes(x = trials,\n       y = avgs)) +\ngeom_point() +\ntheme_minimal() +\nlabs(title = \"Large of Large Numbers Simulation\",\n     subtitle = \"Population of 100,000\",\n     y     = \"Sample Average\",\n     x     = \"Trial Number\")",
    "crumbs": [
      "Sampling Distributions",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "mod-7-2-central.html#normal-example-of-lln",
    "href": "mod-7-2-central.html#normal-example-of-lln",
    "title": "33  Central Limit Theorem",
    "section": "33.6 Normal Example of LLN",
    "text": "33.6 Normal Example of LLN\nWe can demonstrate the Law of Large Numbers using data drawn from a normal distribution.\nWe know the sample average is equal to:\n\\[\n\\bar{X} = \\frac{1}{n}\\sum_{i=1}^n x_i\n\\]\nWe also know that the sample average estimator is unbiased and consistent:\n\\[\nE[\\bar{X}] = \\mu\n\\]\n\\[\nplim_{n \\rightarrow \\infty} \\, \\bar{X} \\rightarrow \\mu\n\\]\nWe use the code chunk from the last example as a starting point. Here we modify the population so that it is drawn from a normal distribution and we also create a table to illustrate the differences between the first five observations and the last five observations.\n\nrm(list = ls())\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(kableExtra)\n\npopulation &lt;- rnorm(100000, mean = 25, sd = 10)\n\nntrials = 10000\ntrials &lt;- rep(0,ntrials)\navgs &lt;- rep(0,ntrials)\n\nfor(i in 1:ntrials){\n  trials[i] &lt;- i\n  avgs[i] &lt;- mean(sample(population, i, replace = FALSE))\n}\n\nmean_frame &lt;- tibble(trials,avgs)\n\nmean_frame_2 &lt;- mean_frame %&gt;%\n  filter(trials &lt;= 5 | trials &gt; ntrials - 5)\n\nggplot(mean_frame,\n       aes(x = trials,\n       y = avgs)) +\ngeom_point() +\ntheme_minimal() +\nlabs(title = \"Large of Large Numbers Simulation\",\n     subtitle = \"Population of 100,000\",\n     y     = \"Sample Average\",\n     x     = \"Trial Number\")\n\n\n\n\n\n\n\nkable(mean_frame_2,\n      align  = 'c',\n      digits = 3,\n      col.names = c('Obsveration','Sample average')) %&gt;%\nkable_styling()\n\n\n\n\nObsveration\nSample average\n\n\n\n\n1\n37.888\n\n\n2\n21.452\n\n\n3\n34.383\n\n\n4\n29.046\n\n\n5\n31.023\n\n\n9996\n24.930\n\n\n9997\n25.018\n\n\n9998\n25.023\n\n\n9999\n24.786\n\n\n10000\n24.955",
    "crumbs": [
      "Sampling Distributions",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "mod-7-2-central.html#central-limit-theorem",
    "href": "mod-7-2-central.html#central-limit-theorem",
    "title": "33  Central Limit Theorem",
    "section": "33.7 Central Limit Theorem",
    "text": "33.7 Central Limit Theorem\nThe central limit theorem states that when independent random variables are summed, that, in many situations, the normalized sum tends toward a normal distribution even if the original variables were not normally distributed.\nLet \\(\\{x_1, x_2, \\dots, x_n\\}\\) be a random sample of size \\(n\\).\nLet \\(\\{x_1, x_2, \\dots, x_n\\}\\) be a sequence identically and independently distributed random variables.\nLet \\(\\{x_1, x_2, \\dots, x_n\\}\\) have mean \\(\\mu\\) and finite variance \\(\\sigma^2\\).\n\\[E[x_1] = E[x_2] = \\dots = E[x_n] = \\mu\\]\n\\[var(x_1) = var(x_2) = \\dots = var(x_n) = \\sigma^2\\]\nThus, by the law of large numbers, the sample average of these random variables converges almost surely (and in probability) to the expected value \\(\\mu\\) as \\(n \\rightarrow \\infty\\).",
    "crumbs": [
      "Sampling Distributions",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "mod-7-2-central.html#central-limit-theorem-1",
    "href": "mod-7-2-central.html#central-limit-theorem-1",
    "title": "33  Central Limit Theorem",
    "section": "33.8 Central Limit Theorem",
    "text": "33.8 Central Limit Theorem\nSuppose \\(\\{x_1, x_2, \\dots, x_n\\}\\) is a sequence of i.i.d. random variables with \\(E[x_i] = \\mu\\) and \\(var[x_i] = \\sigma^2\\). Then, as \\(n\\) approaches infinity, the random variables \\(\\sqrt{n} \\, (\\bar{x} - \\mu)\\) converge in distribution to normal \\(N \\sim (0, \\sigma^2)\\).\n\\[lim_{n \\rightarrow \\infty} \\, \\sqrt{n}( \\bar{x} - \\mu) \\rightarrow^d N(0,\\sigma^2)\\]\nIf \\(\\sigma &gt;0\\), converge in distribution means that the cumulative distribution functions of \\(\\sqrt{n} \\, (\\bar{x} - \\mu)\\) converge pointwise to the CDF of \\(N(0,\\sigma^2)\\) for every real number \\(z\\).\nTherefore, it can be shown the sample average \\(\\bar{x}\\) is such that\n\\[lim_{n \\rightarrow \\infty} \\, \\frac{\\sqrt{n}}{\\sigma}( \\bar{x} - \\mu) \\rightarrow^d N(0,1)\\]\nIn other words, if a random sample of size \\(n\\) is selected from any population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), then when \\(n\\) is sufficiently large\n\\[\\bar{x} \\sim N(\\mu, \\sigma/\\sqrt{n})\\]",
    "crumbs": [
      "Sampling Distributions",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "mod-7-2-central.html#sampling-distributions",
    "href": "mod-7-2-central.html#sampling-distributions",
    "title": "33  Central Limit Theorem",
    "section": "33.9 Sampling Distributions",
    "text": "33.9 Sampling Distributions\nThe sampling distribution of the sample average estimator is the probability distribution of all possible values of the sample average estimator.\nRecall that a collection of random variables is identically and independently distributed if each random variable has the same probability distribution as the others and all are mutually independent.\nFirst, assume that the random variables are identically and independently distributed or:\n\\[\nx_1, x_2, \\dots, x_n \\sim i.i.d\n\\]\nNote that we have not assumed the underlying distribution, merely that the distribution of each random variable is the same.\nSecond, assume that the mean and variance of each random variable is:\n\\[\n\\mu_i, \\quad \\sigma_i^2 \\quad \\forall i\n\\]\nThird, the expected value of the sample averages is:\n\\[E(\\bar{x}) = E(\\frac{1}{n} \\sum_{i=1}^n x_i) = \\frac{1}{n} \\, E(\\sum_{i=1}^n x_i) = \\frac{1}{n} \\, \\sum_{i=1}^n E(x_i)\\]\nGiven that \\(x_i\\) are i.i.d. with mean \\(\\mu_x\\), then:\n\\[E(\\bar{x}) =  \\frac{1}{n} \\, \\sum_{i=1}^n E(x_i) =  \\frac{1}{n} \\, \\sum_{i=1}^n \\mu_x = \\frac{1}{n} \\times n \\times \\mu_x = \\mu_x\\]\nThe variance of the sample averages is:\n\\[var(\\bar{x}) = var(\\frac{1}{n} \\sum_{i=1}^n x_i)\\]\n\\[var(\\bar{x}) = \\frac{1}{n^2} \\sum_{i = 1}^{n} \\, var(x_i) + \\frac{1}{n^2} \\sum_{i=1}^{n} \\sum_{j = 1}^{n} cov(x_i, x_j) \\quad \\forall i \\ne j\\]\nSince the covariance between any dissimilar random variables is assumed to be zero, the variance of the sample averages is:\n\\[var(\\bar{x}) = \\frac{1}{n^2} \\sum_{i = 1}^{n} \\, var(x_i) = \\frac{1}{n^2} \\sum_{i = 1}^{n} \\, \\sigma_x^2 = \\frac{1}{n^2} \\times n \\times \\sigma_x^2 = \\frac{\\sigma_x^2}{n}\\]The variance of the sample average estimator is decreasing in the number of observations.\nIn other words, as the number of observations in repeated sampling increases, the variance of the distribution of the sample average estimator declines relatively to distributions with smaller sample sizes..",
    "crumbs": [
      "Sampling Distributions",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "mod-7-2-central.html#clt-example",
    "href": "mod-7-2-central.html#clt-example",
    "title": "33  Central Limit Theorem",
    "section": "33.10 CLT Example",
    "text": "33.10 CLT Example\nIn the following code below, we demonstrate the Law of Large Numbers and the Central Limit Theorem.\nWe would expect that, given repeated sampling, that the sample average estimates would be, on average equal to the population mean.\nWe would also expect that, given repeated sampling, that as sample size increased, the variance of the distribution of sample average estimates would decrease.\nWe observe that as sample size increases, the variance of the distribution of the sample average estimates declines. Each of the distributions is centered at the population mean.\nThe code completes the following tasks:\n\nThe population of 100,000 observations is drawn from a standard normal distribution\nSample sizes are equal to 1, 10, 100, and 1000\nThe number of repetitions is 1,000\nIn other words, we will take 1,000 samples of size 1, then 1,000 samples of size 10 and so on\nWe can two lists in which we can populate data\nWe have two loops. The outer loop loops over sample sizes. The inner loop over repetitions.\nIn the inner loop, the sample size is set by the outer loop index (1, 10, 100, 1000)\nThe sample average is estimated\nThe sample size is assigned\nA data frame is created containing sample average and sample size\nThe data frame is put into a list indexed by the inner loop counter\nAfter the inner loop completes, all the data frames are put in a loop indexed by the outer loop\nAfter the outer loop completes, all the data frames are row bound\nThe plot illustrates the measure of central tendency and variance of each distribution\n\n\nrm(list = ls())\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(kableExtra)\n\npopulation &lt;- tibble(x = rnorm(100000, mean = 0, sd = 1))\nsample_size &lt;- c(1, 10, 100, 1000)\nnreps &lt;- 1000\n\nlist_1 &lt;- list()\nlist_2 &lt;- list()\n\nfor(j in sample_size){\n\nfor(i in 1:nreps){\n  samp &lt;- slice_sample(population, n = j)\n  samp_avg &lt;- colMeans(samp)\n  sample_size &lt;- as.numeric(j)\n  temp &lt;- data.frame(samp_avg, sample_size)\n  list_1[[i]] &lt;- temp\n\n}\n  list_2[[j]] &lt;- bind_rows(list_1)\n}\n\navg_final &lt;- bind_rows(list_2)\n\nggplot(data = avg_final,\n       aes(x = samp_avg,\n           fill = as.character(sample_size))) +\ngeom_density(stat = \"density\", alpha = 0.5, linewidth = 0.1) +\ntheme(legend.position =\"bottom\",\n      legend.title    = element_blank(),\n      plot.caption    = element_text(hjust=0),\n      axis.title.x    = element_text(face = \"bold\"), \n      axis.text.x     = element_text(face = \"bold\"),\n      axis.text.y     = element_text(face = \"bold\"),\n      axis.title.y    = element_text(face=\"bold\")) +\nlabs(x = \"x\",\n     y = \"Density\",\n     title = \"Distribution of Sample Averages\",\n     subtitle = \"True Mean = 0\")",
    "crumbs": [
      "Sampling Distributions",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "mod-7-3-interval.html",
    "href": "mod-7-3-interval.html",
    "title": "34  Interval Estimation",
    "section": "",
    "text": "34.1 Standard Error\nThe standard error (SE) of a statistic is the standard deviation of its sampling distribution.\nAssume that \\(x_1, x_2, \\dots , x_n\\) are i.i.d with mean \\(\\mu_x\\) and variance \\(\\sigma_x^2\\).\nWe have determined that\n\\[var(\\bar{x}) = \\frac{\\sigma_x^2}{n}\\]\nFor an infinite population, the standard error of the mean \\(\\bar{x}\\) is:\n\\[se(\\bar{x}) = \\sigma_{\\bar{x}} = \\frac{\\sigma_x}{\\sqrt{n}}\\]\nSince the standard deviation of the population \\(\\sigma_x\\) is typically unknown, we use the sample standard deviation \\(s\\) or\n\\[\\hat{\\sigma_\\bar{x}} = s(\\bar{x}) = \\frac{s_x}{\\sqrt{n}}\\]",
    "crumbs": [
      "Sampling Distributions",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Interval Estimation</span>"
    ]
  },
  {
    "objectID": "mod-7-3-interval.html#finite-population",
    "href": "mod-7-3-interval.html#finite-population",
    "title": "34  Interval Estimation",
    "section": "34.2 Finite Population",
    "text": "34.2 Finite Population\nWhat if the population from which the samples are drawn is not infinite?\nThe Finite Population Correction Factor is used when you sample without replacement from more than 5% of a finite population.\nWhy do we need to correct the standard error when the sampled population is “small” and the sample of the finite population is “large” relative to the population?\nIt can be shown that when you sample without replacement from a finite population and the sample size exceeds 5% of the population, the Central Limit Theorem will no longer hold. The Finite Population Correction Factor captures the difference between sampling without replacement and sampling with replacement.\nFor example, if conduct a telephone survey in a large city with a population of 1 million, then surveying 1,000 residents without replacement will still leave a large population to be sampled. On the other hand, if you conduct the same survey in a town of 5,000 residents, you are sampling a significant proportion of the population.\nIn finite populations, the standard error of the mean is:\n\\[\\sigma(\\bar{x}) = \\sqrt{\\frac{(N-n)}{(N-1)}} \\bigg(\\frac{\\sigma}{\\sqrt{n}} \\bigg)\\]\n\\[s(\\bar{x}) = \\sqrt{\\frac{(N-n)}{(N-1)}} \\bigg(\\frac{s}{\\sqrt{n}} \\bigg)\\]\nIn the code chunk below, we demonstrate the FPC and how the FPC corrects the standard error as the sample size rises as a proportion of a finite (“small”) population.\n\nrm(list = ls())\n\nlibrary(dplyr)\nlibrary(kableExtra)\n\nN &lt;- 10000\nsigma &lt;- 5\n\n# Define sample sizes\nsample_sizes &lt;- c(1, 10, 50, 100, 500, 1000, 5000, 8000)\n\n# Calculate finite population correction factor using a loop or vectorized method\n\nfpcf &lt;- sqrt((N - sample_sizes) / (N - 1))\n\n# Combine into a table (data frame)\nfpc_table &lt;- tibble(sample_size = sample_sizes,\n                    fpc = sqrt((N - sample_sizes) / (N - 1)),\n                    se  = sigma/sqrt(sample_sizes),\n                    se_fpc = fpc*se)\n\nkable(fpc_table,\n      align = 'c',\n      digits = 3,\n      col.names = c('Sample Size', 'FPC', 'SE', 'FPC*SE')) %&gt;%\nkable_classic()\n\n\n\n\nSample Size\nFPC\nSE\nFPC*SE\n\n\n\n\n1\n1.000\n5.000\n5.000\n\n\n10\n1.000\n1.581\n1.580\n\n\n50\n0.998\n0.707\n0.705\n\n\n100\n0.995\n0.500\n0.498\n\n\n500\n0.975\n0.224\n0.218\n\n\n1000\n0.949\n0.158\n0.150\n\n\n5000\n0.707\n0.071\n0.050\n\n\n8000\n0.447\n0.056\n0.025",
    "crumbs": [
      "Sampling Distributions",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Interval Estimation</span>"
    ]
  },
  {
    "objectID": "mod-7-3-interval.html#z-score-example",
    "href": "mod-7-3-interval.html#z-score-example",
    "title": "34  Interval Estimation",
    "section": "34.3 Z-Score Example",
    "text": "34.3 Z-Score Example\nAssume that individuals are distributed in a population so that the ages of the individuals are normally distributed with a mean age of 27.0 and a standard deviation of 12.0. If we define \\(X\\) as a random variable representing age, then\n\\[\nX \\sim N(\\mu, \\sigma^2) \\rightarrow X \\sim N(27, 144)\n\\]\nWhat is the probability that a single randomly selected individual is less than 30 years old?\n\\[P(X \\le 30) = P(Z \\le \\frac{30 - 27}{12}) = P(Z \\le 0.25) = 0.5987\\]\nWe can easily demonstrate this problem using the code chunk below.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Sampling Distributions",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Interval Estimation</span>"
    ]
  },
  {
    "objectID": "mod-7-3-interval.html#standard-error-example",
    "href": "mod-7-3-interval.html#standard-error-example",
    "title": "34  Interval Estimation",
    "section": "34.4 Standard Error Example",
    "text": "34.4 Standard Error Example\nAssume that we now draw multiple samples from the population.\nThe sample size of each trial is 36 and we repeatedly sample the population.\nGiven the population mean is 27, the population variance is 144, then:\n\\[\nE[\\bar{x}] = \\mu = 27\n\\]\n\\[\nvar(\\bar{x}) = \\frac{\\sigma^2}{n} = \\frac{144}{36} = 4\n\\]\n\\[\nsd(\\bar{x}) = \\frac{\\sigma}{\\sqrt{n}} = \\frac{12}{6} = 2\n\\]\nFrom the Central Limit Theorem, the sampling distribution of the sample averages is:\n\\[\\bar{x} \\sim N(\\mu, \\frac{\\sigma^2}{n})\\] \\[\\bar{x} \\sim N(27, 4)\\]\nWhat is the probability that the average age of a single sample of randomly selected individuals is less than 30 years given that the sample size is 36 observations?\n\\[P(\\bar{x} \\le 30)  = P(Z \\le \\frac{30-27}{2}) = P(Z \\le 1.5) = 0.9332\\]\nThe code chunk below does the following:\n\nDeclares the variables of interest\nCreates a tibble with the sample average equal to 30 and estimates the probability that the one observes a sample average of less than or equal to 30 with a mean of 27 and a standard error of 2\nProduces a table of the tibble\nCreates a tibble where the variable \\(x\\) is defined on the interval \\([20, 34]\\) and creates 200 observations for \\(x\\) along this interview and then estimates the associated probabilities for \\(x\\) given a mean of 27 and a standard error of 2\nCreates a tibble that only contains observations where \\(x\\) is less than or equal to 30\nCreates a density plot of the tibble to represent the distribution of sample averages\nShades the distribution to represent \\(P(\\bar{x} \\le 30)\\)\n\n\nrm(list = ls())\n\nmu &lt;- 27         \nsigma &lt;- 12       \nn &lt;- 36          \n\nmean_xbar &lt;- mu\nsd_xbar   &lt;- sigma / sqrt(n)\n\np_xbar &lt;- tibble(x_bar = 30,\n                 p_x_bar = pnorm(30, mean_xbar, sd_xbar))\n\nkable(p_xbar,\n      align = 'c',\n      digits = 4,\n      col.names = c('X-Bar','P(X-Bar &lt;= 30)'))\n\ndf_xbar &lt;- tibble(x = seq(20, 34, length = 200),\n                  y = dnorm(x, mean = mean_xbar, sd = sd_xbar))\n\nshade_df &lt;- df_xbar %&gt;% filter(x &lt;= 30)\n\nggplot(df_xbar, \n       aes(x = x, \n           y = y)) +\ngeom_line(color = \"blue\", linewidth = 1.2) +\ngeom_area(data = shade_df, \n          aes(x = x, \n              y = y), \n          fill = \"red\", \n          alpha = 0.5) +\ngeom_vline(xintercept = 30, \n           linetype = \"dashed\", \n           color = \"darkblue\") +\ntheme_minimal(base_size = 12) +\nlabs(title    = \"Sampling Distribution of X-Bar\",\n     subtitle = \"Mean = 27, Sigma = 12, N= 36\",\n     x        = \"X-Bar\",\n     y        = \"Density\") \n\n\n\n\n\n\n\n\n\n\n\nX-Bar\nP(X-Bar &lt;= 30)\n\n\n\n\n30\n0.9332",
    "crumbs": [
      "Sampling Distributions",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Interval Estimation</span>"
    ]
  },
  {
    "objectID": "mod-7-3-interval.html#standard-error-example-1",
    "href": "mod-7-3-interval.html#standard-error-example-1",
    "title": "34  Interval Estimation",
    "section": "34.5 Standard Error Example",
    "text": "34.5 Standard Error Example\nAs part of your research, you have obtained a sample of 3,600 individuals. Using this data, you have estimated the average income of the sample and the sample standard deviation of income.\n\\[\n\\bar{x} = 52,500\n\\]\n\\[\ns = 2,750\n\\]\nWhat is the standard error of the sample estimator in this example?\n\\[\nse(\\bar{x}) = \\frac{s}{\\sqrt{n}} = \\frac{2,750}{\\sqrt{3,600}} = 45.83\n\\]\nWhat is the probability, in repeated samples of 3,600, of drawing an individual with an income of 47,500 or less?\n\\[\nP(X \\le 47,500) = P(Z \\le \\frac{x - \\mu}{\\sigma}) = P(Z \\le \\frac{47500-52500}{2750}) = P(Z \\le -1.818)\n\\]\n\\[\nP(Z \\le -1.818) = 0.0345\n\\]\nRemember, the distribution of sample averages is more “tightly clustered” around the population mean. We can observe this by noting the sample standard deviation is 2,750 but the standard error of the sample averages is 45.83. Our samples are relatively ‘large’ so the variance of the sample averages is much lower than the variance of the random variable.\nFor example, what is the probability, with a sample size of 3,600, sample average of 52,500, and a sample standard error of 45.83 of drawing a sample with a sample average less than or equal to 52,400?\n\\[\nP(\\bar{x} \\le 52,400) = P(Z \\le \\frac{52400-52500}{45.83}) = 0.0145\n\\]\nLet’s say that we change the sample size from 3,600 to 600. What impact does this have on the probability of observing a sample average of 52,400 or less?\nGiven the change in the sample size, we re-estimate the standard error and obtain a new probability of observing a sample with a sample average of 52,400.\n\\[\nse(\\bar{x}) = \\frac{s}{\\sqrt{n}} = \\frac{2,750}{\\sqrt{600}} = 112.27\n\\]\n\\[\nP(\\bar{x} \\le 52,400) = P(Z \\le \\frac{52400-52500}{112.27}) = 0.1865\n\\]\nIn the code chunk below, we create simulated population of 500,000 observations and then take samples of 60, 600, and 3,600 from the population. We then estimate the probability of observing a sample average of 52,400 for each of the sample sizes.\nAs one would expect, given the Central Limit Theorem, as you increase the sample size, the standard error of the sample average estimator declines, and the probability of observing a sample with an average income of 52,400 declines.\n\nrm(list = ls())\n\nset.seed(1234)\n\nlibrary(dplyr)\nlibrary(kableExtra)\n\npopulation &lt;- tibble(income = rnorm(500000, 52500, 2750))\n\nsample_1 &lt;- population %&gt;% \n            slice_sample(n = 3600, replace = FALSE) %&gt;%\n            summarize(average = mean(income),\n                      sd      = sd(income),\n                      se      = sd(income)/sqrt(6000)) %&gt;%\n            mutate(p_x_bar = pnorm(52400, 52500, se),\n                   n       = 6000) %&gt;%\n            select(n, everything())\n\nsample_2 &lt;- population %&gt;% \n            slice_sample(n = 600, replace = FALSE) %&gt;%\n            summarize(average = mean(income),\n                      sd      = sd(income),\n                      se      = sd(income)/sqrt(600)) %&gt;%\n            mutate(p_x_bar = pnorm(52400, 52500, se),\n                   n       = 600) %&gt;%\n            select(n, everything())\n\nsample_3 &lt;- population %&gt;% \n            slice_sample(n = 60, replace = FALSE) %&gt;%\n            summarize(average = mean(income),\n                      sd      = sd(income),\n                      se      = sd(income)/sqrt(60)) %&gt;%\n            mutate(p_x_bar = pnorm(52400, 52500, se),\n                   n       = 60) %&gt;%\n            select(n, everything())\n\noutput &lt;- sample_1 %&gt;%\n          rbind(sample_2) %&gt;%\n          rbind(sample_3) %&gt;%\n          arrange(n)\n\nkable(output,\n      align = 'c',\n      digits = 3,\n      col.names = c('n', 'X-bar', 'SD(X)', 'SE(X-bar)','P(X-bar &lt;= 52400)')) %&gt;%\nkable_classic()\n\n\n\n\nn\nX-bar\nSD(X)\nSE(X-bar)\nP(X-bar &lt;= 52400)\n\n\n\n\n60\n52657.51\n2804.182\n362.018\n0.391\n\n\n600\n52676.70\n2841.645\n116.010\n0.194\n\n\n6000\n52440.07\n2763.601\n35.678\n0.003",
    "crumbs": [
      "Sampling Distributions",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Interval Estimation</span>"
    ]
  },
  {
    "objectID": "mod-7-3-interval.html#interval-estimation",
    "href": "mod-7-3-interval.html#interval-estimation",
    "title": "34  Interval Estimation",
    "section": "34.6 Interval Estimation",
    "text": "34.6 Interval Estimation\nAn estimator generates a point estimate of a population parameter of interest.\nHowever, the point estimate is unlikely to be exactly equal to the population parameter of interest.\nAn interval estimate provides information on how “close” the point estimate is to the population parameter.\nThe margin of error is often added and subtracted from the point estimate to generate the interval estimate or:\n\\[\\bar{x} \\pm \\, \\text{m.o.e}\\]\nThe sampling distribution of \\(\\bar{x}\\) is the basis for the interval estimates.",
    "crumbs": [
      "Sampling Distributions",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Interval Estimation</span>"
    ]
  },
  {
    "objectID": "mod-7-3-interval.html#statistical-significance",
    "href": "mod-7-3-interval.html#statistical-significance",
    "title": "34  Interval Estimation",
    "section": "34.7 Statistical Significance",
    "text": "34.7 Statistical Significance\nFirst, we define the level of statistical significance as equal to the probability of incorrectly rejecting the null hypothesis of interest. In other words, we define the level of statistical significance as the likelihood of a false positive.\nFor example, assume that we are investigating whether a drug statistically reduces the likelihood of cancer. The null hypothesis is that the drug has not statistically discernible influence on the likelihood of cancer (or increases the likelihood of having cancer).\nThe alternative hypothesis is that the drug reduces the likelihood of having cancer.\nA false positive would be determining the drug reduces the likelihood of having cancer when it has no impact (or, even worse, increases the likelihood of having cancer).\nSo, if we set the level statistical significance to 0.05, we are saying there is a 5% change of incorrectly rejecting the null hypothesis.\nAn increase in the level of statistical significance lowers the probability of a Type 1 error.\nLet \\(\\alpha\\) be equal to the probability of making a Type 1 error. As we lower \\(\\alpha\\) from 0.10 to 0.05 to 0.01, we are increasing statistical significance from 10% to 5% to 1%.",
    "crumbs": [
      "Sampling Distributions",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Interval Estimation</span>"
    ]
  },
  {
    "objectID": "mod-7-3-interval.html#interval-estimation-1",
    "href": "mod-7-3-interval.html#interval-estimation-1",
    "title": "34  Interval Estimation",
    "section": "34.8 Interval Estimation",
    "text": "34.8 Interval Estimation\nAssume that the population variance is known.\nLet \\(\\alpha\\) be the level of statistical significance or the probability of making a Type 1 error.\nThe confidence interval of the interval estimate is:\n\\[\n(1 - \\alpha)\n\\]\nThe interval estimate for the sample average when \\(\\sigma^2\\) is known is:\n\\[\\bar{x} \\pm \\, z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\]\nLet \\(\\sigma = 20\\) and \\(n = 100\\) and \\(\\alpha = 0.05\\) so \\(\\alpha/2 = 0.025\\)\n\\[\\sigma(\\bar{x}) = \\frac{\\sigma_x}{\\sqrt{n}} = \\frac{20}{\\sqrt{100}} = 2\\]\nGiven that \\(1 - \\alpha/2 = 0.975\\) then \\(Z(0.975)=\\)\n\\[\\bar{x} \\pm \\, z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} = \\bar{x} \\pm (1.96 \\times 2)  = \\bar{x} \\pm 3.92\\]",
    "crumbs": [
      "Sampling Distributions",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Interval Estimation</span>"
    ]
  },
  {
    "objectID": "mod-7-3-interval.html#interval-estimation-in-r",
    "href": "mod-7-3-interval.html#interval-estimation-in-r",
    "title": "34  Interval Estimation",
    "section": "34.9 Interval Estimation in R",
    "text": "34.9 Interval Estimation in R\nWe demonstrate how to estimate confidence intervals in R in the code chunk below.\n\nThe sample size is set to 500\nThree significance levels are used, 10%, 5%, and 1%\nWe use \\(\\alpha/2\\) and qnorm to find the appropriate Z value for each interval\nWe create a population of 100,000 observations with a known mean and standard deviation\nWe randomly sample the population and estimate descriptive statistics\nWe estimate the lower and upper bounds of each confidence internal\nWe produce a table of results\n\n\nrm(list = ls())\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(kableExtra)\n\nsample_size = 500\n\nalpha_1 = 0.10\nalpha_2 = 0.05\nalpha_3 = 0.01\n\nci_1 &lt;- 1 - alpha_1/2\nci_2 &lt;- 1 - alpha_2/2\nci_3 &lt;- 1 - alpha_3/3\n\nz_ci_1 &lt;- qnorm(ci_1, 0, 1)\nz_ci_2 &lt;- qnorm(ci_2, 0, 1)\nz_ci_3 &lt;- qnorm(ci_3, 0, 1)\n\npopulation &lt;- tibble(income = rnorm(100000, mean = 52500, sd = 2750))\n\nsample &lt;- population %&gt;%\n          slice_sample(n = sample_size) %&gt;%\n          summarize(samp_avg = mean(income),\n                    samp_sd  = sd(income),\n                    samp_se  = sd(income)/sqrt(sample_size))\n\nmoe &lt;- sample %&gt;%\n       mutate(lower_1 = samp_avg - z_ci_1 * samp_se,\n              upper_1 = samp_avg + z_ci_1 * samp_se,\n              lower_2 = samp_avg - z_ci_2 * samp_se,\n              upper_2 = samp_avg + z_ci_2 * samp_se,\n              lower_3 = samp_avg - z_ci_3 * samp_se,\n              upper_3 = samp_avg + z_ci_3 * samp_se)\n\n\nkable(moe,\n      align       = \"c\",\n      digits      = 2,\n      caption     = \"Confidence Intervals\",\n      col.names   = c(\"Sample Average\", \n                      \"Sample SD\",\n                      \"Sample Avg SE\",\n                      \"Lower Bound (10%)\",\n                      \"Upper Bound (10%)\",\n                      \"Lower Bound (5%)\",\n                      \"Upper Bound (5%)\",\n                      \"Lower Bound (1%)\",\n                      \"Upper Bound (1%)\")) %&gt;%\nkable_styling(font_size = 12)\n\n\nConfidence Intervals\n\n\nSample Average\nSample SD\nSample Avg SE\nLower Bound (10%)\nUpper Bound (10%)\nLower Bound (5%)\nUpper Bound (5%)\nLower Bound (1%)\nUpper Bound (1%)\n\n\n\n\n52607.96\n2861.21\n127.96\n52397.49\n52818.43\n52357.17\n52858.75\n52260.81\n52955.12",
    "crumbs": [
      "Sampling Distributions",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Interval Estimation</span>"
    ]
  },
  {
    "objectID": "mod-7-3-interval.html#interval-estimation---mean",
    "href": "mod-7-3-interval.html#interval-estimation---mean",
    "title": "34  Interval Estimation",
    "section": "34.10 Interval Estimation - Mean",
    "text": "34.10 Interval Estimation - Mean\nWhile we would prefer to know the population variance, in all likelihood, the true population variance is not know.\nWe, however, do have unbiased estimators of the population mean and variance.\n\\[\nE[\\bar{x}] = \\mu\n\\]\n\\[\nE[var(\\bar{x})] = E[\\frac{s}{\\sqrt{n}}] = \\frac{\\sigma}{\\sqrt{n}}\n\\]\nLet \\(s^2\\) be the unbiased sample variance.\nSince \\(\\sigma^2\\) is unknown, then replacing \\(\\sigma^2\\) with its unbiased estimate \\(s^2\\) yields the t-statistic or:\n\\[t = \\frac{\\bar{x} - \\mu}{s/\\sqrt{n}}\\]",
    "crumbs": [
      "Sampling Distributions",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Interval Estimation</span>"
    ]
  },
  {
    "objectID": "mod-7-3-interval.html#t-distribution",
    "href": "mod-7-3-interval.html#t-distribution",
    "title": "34  Interval Estimation",
    "section": "34.11 T-Distribution",
    "text": "34.11 T-Distribution\nThe t-statistic is:\n\\[t = \\frac{\\bar{x} - \\mu}{s/\\sqrt{n}}\\]\nRearrange the t-statistic to obtain:\n\\[T_{n-1} = \\frac{\\sqrt{n} (\\bar{x} - \\mu)}{s}\\] The numerator is a standard normal random variable.\nThe denominator follows a chi-squared distribution with \\((n - 1)\\) degrees of freedom.\n\\[s^2 = \\frac{1}{n-1} \\, \\sum_{i = 1}^{n} (x_i - \\bar{x})^2\\]\nThe numerator and denominator are independent.",
    "crumbs": [
      "Sampling Distributions",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Interval Estimation</span>"
    ]
  },
  {
    "objectID": "mod-7-3-interval.html#t-distribution-plot",
    "href": "mod-7-3-interval.html#t-distribution-plot",
    "title": "34  Interval Estimation",
    "section": "34.12 T Distribution Plot",
    "text": "34.12 T Distribution Plot\n\nrm(list = ls())\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(kableExtra)\n\nggplot(data.frame(x = c(-5, 5)), \n       aes(x = x)) +\nstat_function(fun = dt, \n              args = list(df = 1),\n              linewidth = 1.2,\n              color = \"dark blue\") +\nstat_function(fun = dnorm, \n              args = list(mean = 0, sd = 1),\n              linewidth = 1.2,\n              color = \"red\") +\ntheme_minimal() +\nlabs(x = \"x\",\n     y = \"P(x)\",\n     title = \"Standard Normal and T Distribution\",\n     subtitle = \"1 degree of freedom\")\n\n\n\n\n\n\n\n\n\nrm(list = ls())\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(kableExtra)\n\nlibrary(ggplot2)\n\nggplot(data.frame(x = c(-5, 5)), \n       aes(x = x)) +\nstat_function(fun = dt, \n              args = list(df = 10),\n              linewidth = 1.2,\n              color = \"dark blue\") +\nstat_function(fun = dnorm, \n              args = list(mean = 0, sd = 1),\n              linewidth = 1.2,\n              color = \"red\") +\ntheme_minimal() +\nlabs(x = \"x\",\n     y = \"P(x)\",\n     title = \"Standard Normal and T-Distribution\",\n     subtitle = \"10 degrees of freedom\")",
    "crumbs": [
      "Sampling Distributions",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Interval Estimation</span>"
    ]
  },
  {
    "objectID": "mod-7-3-interval.html#interval-estimation---mean-1",
    "href": "mod-7-3-interval.html#interval-estimation---mean-1",
    "title": "34  Interval Estimation",
    "section": "34.13 Interval Estimation - Mean",
    "text": "34.13 Interval Estimation - Mean\nAssume that \\(\\sigma\\) is unknown and \\(s\\) is an unbiased estimator.\nLet \\((1-\\alpha)\\) be the confidence coefficient.\nThe interval estimate of the population mean is:\n\\[\\bar{x} \\pm \\, t_{\\alpha/2} \\, \\frac{s}{\\sqrt{n}}\\]\nLet n = 70, the sample average is 9,312, sample standard deviation is 4,007, and \\(\\alpha = 0.05\\).\nWe can use the qt function or a t-table to find the appropriate t-statistic.\nThe interval estimate is thus:\n\\[\\bar{x} \\pm \\, t_{\\alpha/2} \\, \\frac{s}{\\sqrt{n}} = 9312 \\pm 1.994945 \\times \\frac{4007}{\\sqrt{70}} = 9312 \\pm 955.4352\\]\n\nrm(list = ls())\n\nalpha = 0.05\nn = 70\nxbar = 9312\ns = 4007\n\nt_stat &lt;- qt(alpha/2,n-1, lower.tail = FALSE)\n\nmoe &lt;- tibble(xbar = xbar,\n              se = s/sqrt(n),\n              t_stat = t_stat,\n              lower = xbar - t_stat*se,\n              upper = xbar + t_stat*se)\n\nkable(moe,\n      align = 'c',\n      digits = 3,\n      col.names = c('X-Bar','SE(X-Bar)','T-Statistic',\n                    'Lower Bound', 'Upper Bound')) %&gt;%\nkable_classic()\n\n\n\n\nX-Bar\nSE(X-Bar)\nT-Statistic\nLower Bound\nUpper Bound\n\n\n\n\n9312\n478.928\n1.995\n8356.565\n10267.43",
    "crumbs": [
      "Sampling Distributions",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Interval Estimation</span>"
    ]
  },
  {
    "objectID": "mod-7-3-interval.html#r-example",
    "href": "mod-7-3-interval.html#r-example",
    "title": "34  Interval Estimation",
    "section": "34.14 R Example",
    "text": "34.14 R Example\nHere we have a randomly generated population with \\(\\mu = 50,000\\) and \\(\\sigma = 9700\\).\nAssume we don’t know \\(\\mu\\) or \\(\\sigma\\) but we can sample the population.\nWe can construct a 90% confidence interval using the sample average and the t-statistic.\nWe can vary \\(\\alpha\\) and \\(n\\) to observe how changing the confidence interval and sample size affects the confidence interval.\n\nrm(list = ls())\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(kableExtra)\n\npop = 100000\nmu = 50000\nsigma = 9700\nnsample &lt;- 500\nalpha &lt;- 0.10\n\nci &lt;- 1 - (alpha/2)\nt_ci &lt;- qt(ci, nsample - 1)\n\npopulation &lt;- data.frame(x = rnorm(pop, mu, sigma))\n\nsamples &lt;- population %&gt;%\n  slice_sample(n = nsample) %&gt;%\n  summarize(samp_avg = mean(x),\n            samp_sd  = sd(x)) %&gt;%\n  mutate(std_err = samp_sd/sqrt(nsample),\n         moe     = t_ci*std_err) %&gt;%\n  mutate(lower_ci = samp_avg - moe,\n         upper_ci = samp_avg + moe)\n\nkable(samples,\n      align       = \"cccccc\", \n      caption     = \"Confidence Interval\",\n      col.names   = c(\"Average\", \"Std Dev\", \n                      \"Std Error\", \"MOE\",\n                      \"Lower Bound\", \"Upper Bound\")) %&gt;%\n  kable_styling(font_size = 12)\n\n\nConfidence Interval\n\n\nAverage\nStd Dev\nStd Error\nMOE\nLower Bound\nUpper Bound\n\n\n\n\n50519.08\n9713.154\n434.3854\n715.8294\n49803.25\n51234.91",
    "crumbs": [
      "Sampling Distributions",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Interval Estimation</span>"
    ]
  },
  {
    "objectID": "mod-7-3-interval.html#wrapping-up",
    "href": "mod-7-3-interval.html#wrapping-up",
    "title": "34  Interval Estimation",
    "section": "34.15 Wrapping Up",
    "text": "34.15 Wrapping Up\nIn this module, we have explored how samples are drawn from a population and investigated how sampling variability occurs when estimating population parameters. We have used data to explore the shape and dispersion of sampling distributions, illustrating the relationship between estimators and the population characteristics of interest.\nWe explored the Law of Large Numbers and Central Limit Theorem. These play a central role in inferential statistics. We noted that, regardless of the underlying population distribution, the sample distribution of the sample averages approaches normality as sample size increases. We estimated the standard error and applied these concepts in a variety of examples.",
    "crumbs": [
      "Sampling Distributions",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Interval Estimation</span>"
    ]
  },
  {
    "objectID": "mod-8-0-overview.html",
    "href": "mod-8-0-overview.html",
    "title": "35  Module 8 Overview",
    "section": "",
    "text": "35.1 Introduction\nWelcome to Module 8 of ECON 700.\nThis final module integrates the principles of statistical inference, emphasizing how sample evidence supports or refutes claims about population parameters. Students learn how to formulate and test hypotheses regarding population means, proportions, and variances. The module introduces the logic of null and alternative hypotheses, Type I and Type II errors, and the role of significance levels (α) in decision-making. Through practical examples and R-based visualizations, learners explore one-tailed and two-tailed tests using both the z and t distributions, building a strong understanding of how to determine rejection regions, calculate p-values, and interpret statistical significance.\nThe module extends these concepts to inferences involving two populations and variability. Students conduct hypothesis tests for the difference between two means and proportions and explore the chi-square \\(\\chi^2\\) and F-distributions for analyzing population variances. By connecting hypothesis testing to real-world economic and data-driven applications, students gain experience in applying inference to managerial decisions and policy evaluation. This capstone module consolidates key statistical methods, preparing students to interpret and evaluate empirical evidence in research and professional contexts.",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Module 8 Overview</span>"
    ]
  },
  {
    "objectID": "mod-8-0-overview.html#learning-objectives",
    "href": "mod-8-0-overview.html#learning-objectives",
    "title": "35  Module 8 Overview",
    "section": "35.2 Learning Objectives",
    "text": "35.2 Learning Objectives\nBy the end of this module, you should be able to:\n\nMLO 1: Formulate null and alternative hypotheses and explain the logic of statistical decision-making. (CLO 1, 2, 3, 4)\nMLO 2: Differentiate between Type I and Type II errors and interpret the role of significance levels in hypothesis testing. (CLO 1, 2, 3, 4)\nMLO 3: Conduct hypothesis tests for population means and proportions using the z and t distributions in R. (CLO 1, 2, 3, 4, 5)\nMLO 4: Apply hypothesis testing to compare two population means or proportions and interpret p-values and confidence levels. (CLO 1, 2, 3, 4, 5)\nMLO 5: Evaluate population variances using the chi-square and F-distributions to test assumptions about variability. (CLO 1, 2, 3, 4, 5)\nMLO 6: Integrate hypothesis testing results into real-world economic and data analysis contexts to draw valid inferential conclusions. (CLO 1, 2, 3, 4, 5)",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Module 8 Overview</span>"
    ]
  },
  {
    "objectID": "mod-8-0-overview.html#required-texts",
    "href": "mod-8-0-overview.html#required-texts",
    "title": "35  Module 8 Overview",
    "section": "35.3 Required Texts",
    "text": "35.3 Required Texts\nThe following textbooks are available for free. Please select the links provided to access.\n\nBarbara Illowsky and Susan Dean. Introductory Statistics 2e. OpenStax. Chapter 9-11.\nRafael Irizarry. (2025). Introduction to Data Science: Data Wrangling and Visualization with R.\nHadley Wickham, Mine Cetinkaya-Rundel, and Garrett Grolemund. (2025) R for Data Science.\nOnline R documentation\nRStudio Cheat Sheets",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Module 8 Overview</span>"
    ]
  },
  {
    "objectID": "mod-8-0-overview.html#activities",
    "href": "mod-8-0-overview.html#activities",
    "title": "35  Module 8 Overview",
    "section": "35.4 Activities",
    "text": "35.4 Activities\n\nComplete the hands-on coding exercises embedded in each lesson (MLO 3-6)\n\nComplete the weekly class assignment (MLO 1 - MLO 6)\n\nParticipate in the discussion forum (MLO 3 - MLO 6)\n\nTake the weekly knowledge quiz (MLO 1 - MLO 6)",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Module 8 Overview</span>"
    ]
  },
  {
    "objectID": "mod-8-0-overview.html#lessons-in-this-module",
    "href": "mod-8-0-overview.html#lessons-in-this-module",
    "title": "35  Module 8 Overview",
    "section": "35.5 Lessons in this Module",
    "text": "35.5 Lessons in this Module\n\n8.1 – Hypothesis Testing\n8.2 – Hypothesis Testing: Means\n8.3 – Hypothesis Testing: Variance\n\n\nNext: Start with Hypothesis Testing.",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Module 8 Overview</span>"
    ]
  },
  {
    "objectID": "mod-8-1-hypothesis.html",
    "href": "mod-8-1-hypothesis.html",
    "title": "36  Hypothesis Testing",
    "section": "",
    "text": "36.1 Terminology\nHypothesis testing is a process by which a statement about the value of a population parameter of interest is rejected or not rejected.\nA null hypothesis or \\(H_0\\) is statement about a population parameter and a specific value or there is no difference between two parameters of interest.\nAn alternative hypothesis or \\(H_a\\) is a statement that states the existence of a difference between a population parameter and a specific value or states that there is a difference between two parameters of interest.\nWe use data to test whether to reject or fail to reject the null hypothesis.\nWe never accept the null hypothesis nor do we accept the alternative.\nWe either have sufficient empirical evidence to reject \\(H_0\\) or we have insufficient evidence to reject \\(H_0\\).",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "mod-8-1-hypothesis.html#two-tailed-hypotheses",
    "href": "mod-8-1-hypothesis.html#two-tailed-hypotheses",
    "title": "36  Hypothesis Testing",
    "section": "36.2 Two-Tailed Hypotheses",
    "text": "36.2 Two-Tailed Hypotheses\n\\(H_0\\) will always have an equality as part of the hypothesis.\n\\(H_a\\) will not have an equality as part of the hypothesis.\nA two-tailed hypothesis can be stated that the population parameter is equal to a value.\n\\[H_0 : \\mu = k\\]\n\\[H_a: \\mu \\ne k\\]\nWe will reject the null hypothesis if we have sufficient empirical evidence that \\(\\mu\\) is sufficiently different from \\(k\\).\nA two-tailed hypothesis can also be stated that two parameters are equal to each other under the null.\n\\[H_0: \\mu_1 = \\mu_2 \\implies H_0: \\mu_1 - \\mu_2 = 0\\]\n\\[H_a: \\mu_1 \\ne \\mu_2 \\implies H_a: \\mu_1 - \\mu_2 \\ne 0\\]",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "mod-8-1-hypothesis.html#one-tailed-hypotheses",
    "href": "mod-8-1-hypothesis.html#one-tailed-hypotheses",
    "title": "36  Hypothesis Testing",
    "section": "36.3 One-Tailed Hypotheses",
    "text": "36.3 One-Tailed Hypotheses\n\\(H_0\\) will always have an equality as part of the hypothesis.\n\\(H_a\\) will not have an equality as part of the hypothesis.\nFor a one-tailed hypothesis, the population parameter under \\(H_0\\) may be less than or equal to a value of interest.\n\\[H_0 : \\mu \\le k\\]\n\\[H_a: \\mu &gt; k\\]\nFor a one-tailed hypothesis, the population parameter under \\(H_0\\) may be greater than or equal to a value of interest.\n\\[H_0 : \\mu \\ge k\\]\n\\[H_a: \\mu &lt; k\\]\nA one-tailed hypothesis can also be stated in terms of two parameters.\n\\[H_0: \\mu_1 \\le \\mu_2 \\implies H_0: \\mu_1 - \\mu_2 \\le 0\\]\n\\[H_a: \\mu_1 &gt; \\mu_2 \\implies H_a: \\mu_1 - \\mu_2 &gt; 0\\]",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "mod-8-1-hypothesis.html#hypothesis-example",
    "href": "mod-8-1-hypothesis.html#hypothesis-example",
    "title": "36  Hypothesis Testing",
    "section": "36.4 Hypothesis Example",
    "text": "36.4 Hypothesis Example\nAssume that mean sales volume is 14 cars a month. A manager implements a new incentive plan to increase sales and collects data over a six-month period. What are the null and alternative hypotheses?\nIf the manager want to examine whether mean sales volume increased and let \\(\\mu_1\\) be the mean sales volume prior to the implementation of the plan and \\(\\mu_2\\) be the mean sales volume after the implementation of the plan.\nIn other words, since the manager is really interested if sales increased \\(\\mu_2 &gt; \\mu_1\\), we can form a one-tailed hypothesis.\n\\[H_0: \\mu_1 \\ge \\mu_2 \\implies H_0: \\mu_1 - \\mu_2 \\ge 0\\]\n\\[H_a: \\mu_1 &lt; \\mu_2 \\implies H_a: \\mu_1 - \\mu_2 &lt; 0\\]\nMore specifically:\n\\[H_0: \\mu_{old} \\ge \\mu_{new} \\implies H_0: \\mu_{old} - \\mu_{new} \\ge 0\\]\n\\[H_a: \\mu_{old} &lt; \\mu_{new} \\implies H_a: \\mu_{old} - \\mu_{new} &lt; 0\\]",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "mod-8-1-hypothesis.html#example",
    "href": "mod-8-1-hypothesis.html#example",
    "title": "36  Hypothesis Testing",
    "section": "36.5 Example",
    "text": "36.5 Example\nAssume that mean filling weight on a production line is 32 ounces/container. A new machine is installed and a sample of cartons is gathered and weighed. What is the hypothesis to determine whether the new machine impacted mean filling weight?\nSince the question is whether an impact (positive or negative) has occurred, we can develop a two-tailed hypothesis\n\\[H_0: \\mu_1 = \\mu_2 \\implies H_0: \\mu_1 - \\mu_2 = 0\\]\n\\[H_a: \\mu_1 \\ne \\mu_2 \\implies H_a: \\mu_1 - \\mu_2 \\ne 0\\]More specifically,\n\\[H_0: \\mu_{old} = \\mu_{new} \\implies H_0: \\mu_{old} - \\mu_{new} = 0\\]\n\\[H_a: \\mu_{old} \\ne \\mu_{new} \\implies H_a: \\mu_{old} - \\mu_{new} \\ne 0\\]Note that you would reject the null hypothesis if there was sufficient empirical evidence that the new mean weight was significantly less or significantly more than the previous weight.",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "mod-8-1-hypothesis.html#type-i-and-type-ii-errors",
    "href": "mod-8-1-hypothesis.html#type-i-and-type-ii-errors",
    "title": "36  Hypothesis Testing",
    "section": "36.6 Type I and Type II Errors",
    "text": "36.6 Type I and Type II Errors\nSince we are using sample data to make inferences about population parameters, our estimates will not be exactly equal to the population parameter(s) of interest.\nA Type I error occurs when when we reject \\(H_0\\) but \\(H_0\\) should not be rejected.\nA Type II errors occurs when we do not reject \\(H_0\\) but we should reject \\(H_0\\)\nFrom a hypothesis testing perspective, we focus on incorrectly rejecting \\(H_0\\), that is, concluding that there is sufficient empirical evidence to reject the null hypothesis.\nThe probability of making a Type I error is called the level of significance.\nWe denote the level of significance as \\(\\alpha\\).\nIf \\(\\alpha = 0.05\\), then we want to know whether we can reject \\(H_0\\) at the 5% level of significance.\nThe greater the consequences of a Type I error, the higher the level of significance to reduce the probability of incorrectly rejecting \\(H_0\\) when, in fact, it should not be rejected.",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "mod-8-1-hypothesis.html#type-i-error-discussion",
    "href": "mod-8-1-hypothesis.html#type-i-error-discussion",
    "title": "36  Hypothesis Testing",
    "section": "36.7 Type I Error Discussion",
    "text": "36.7 Type I Error Discussion\nA Type 1 error occurs when you reject the null hypothesis when you should not reject the null hypothesis, in other words, you conclude a statistically significant result exists when it does not exist.\nA Type 1 error is a false positive error.\nLet the significance level be 10%, that is, \\(\\alpha = 0.10\\).\nThis means that we have a 10% chance of incorrectly rejecting the null hypothesis.\nA higher level of significance means that \\(\\alpha\\) is smaller.\nLet the significance level be 1%, that is, \\(\\alpha = 0.01\\).\nThis means that we now have a 1% chance of incorrectly rejecting the null hypothesis.\nYou conduct a clinical study comparing the mean recovery time of patients who received a new drug versus patients who received an old drug. You have decided to conduct a two-tailed hypothesis test comparing the mean recovery times. You have set your level of significance to 5%.\n\\[\nH_0: \\mu_{old} = \\mu_{new}\n\\]\n\\[\nH_A: \\mu_{old} \\ne \\mu_{new}\n\\]\nIf you obtain a p-value from your test statistic of 0.07, you would fail to reject the null hypothesis as the p-value of the test is greater than the level of significance.\nNow assume you obtain new data and you obtain a p-value of 0.03. Your p-value is less than the level of significance, so you would reject the null hypothesis. You should note that the p-value of 0.03 indicates that the probability of a Type 1 error is 3%, that is, the probability of a Type 1 error is smaller than the level of significance and non-zero.",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "mod-8-1-hypothesis.html#type-ii-error-discussion",
    "href": "mod-8-1-hypothesis.html#type-ii-error-discussion",
    "title": "36  Hypothesis Testing",
    "section": "36.8 Type II Error Discussion",
    "text": "36.8 Type II Error Discussion\nA Type II error occurs when you fail to reject the null hypothesis when, in fact, it should be rejected.\nA Type II error is a false negative error.\nAn example of a Type II error is when a doctor uses a test to screen for a disease and the test returns a negative result (no disease) but the patient actually has the disease.\nThe probability of a Type II error is denoted by \\(\\beta\\).\nThere is a trade off between Type I and Type II errors.\nAs you decrease the probability of one type of error, you increase the probability of the other type of error.\nFor example, you are examining whether a new type of surgery improves outcomes for patients. The surgery is costly and invasive. As such, you want to only reject the null hypothesis of no impact with a high degree of confidence and so you set the significance level of the test to 1%.\nA significance level of 1% is “more stringent” than a significance level of 5% and this means you require more evidence to reject the null hypothesis.\nIn other words, by increasing the significance level to 1%, you reduce the likelihood of false positive but you also increase the likelihood of a false negative.\nThe graphic below highlights the trade off between Type I and Type II errors.\nAs you increase the level of significance, \\(\\alpha\\), you move the threshold of the test to the right, reducing the likelihood of a Type I error but increasing the likelihood of a Type II error.\nSince we are typically most concerned about a false positive, we focus on Type I errors rather than Type II errors, hence our focus on the level of statistical significance which is equal to the likelihood of making a Type I error.\n\nImage Source: http://grasshopper.com/blog/the-errors-of-ab-testing-your-conclusions-can-make-things-worse/",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "mod-8-2-means.html",
    "href": "mod-8-2-means.html",
    "title": "37  Hypothesis Testing: Means",
    "section": "",
    "text": "37.1 Hypothesis Testing\nLet \\(n\\) be the sample size used to estimate \\(\\bar{x}\\) and \\(\\sigma\\) the population standard deviation for \\(x\\).\nLet \\(\\mu_0\\) be the hypothesized value of the population mean under \\(H_0\\) such that:\nThe test-statistic for testing hypotheses regarding the sample mean when \\(\\sigma\\) is known is:\n\\[z = \\frac{\\bar{x} - \\mu_0}{\\sigma/\\sqrt{n}}\\]\nThe value of \\(z\\) is known as the test statistic.\nGiven a level of significance, \\(\\alpha\\), we can obtain the critical value of the test statistic.\nIf it is a one-tailed test, then we use \\(\\alpha\\) to obtain the test statistic.\nIf it is a two-tailed test, then we use \\(\\alpha/2\\) to obtain the test statistic.",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Hypothesis Testing: Means</span>"
    ]
  },
  {
    "objectID": "mod-8-2-means.html#one-tailed-hypothesis-test",
    "href": "mod-8-2-means.html#one-tailed-hypothesis-test",
    "title": "37  Hypothesis Testing: Means",
    "section": "37.2 One-Tailed Hypothesis Test",
    "text": "37.2 One-Tailed Hypothesis Test\nWe hypothesize that the population mean is less than than 42 and let \\(\\alpha = 0.10\\).\n\\[H_0: \\mu_1 \\ge 42\\]\n\\[H_a: \\mu &lt; 42\\]\nOur rejection region is in the left tail of the standard normal distribution or:\n\\[\\text{If} \\; z \\le z_c \\implies \\text{reject} \\; H_0\\]\n\\[\\text{If} \\; z &gt; z_c \\implies \\text{fail to reject} \\; H_0\\]\nGiven \\(\\alpha = 0.10\\) and a left-tail test, we need to find \\(z_{\\alpha} = z_{0.10}\\).\nThe critical value of the test statistic can be found with a standard normal table or using R as in the code chunk below.\nGiven the \\(\\alpha = 0.10\\) and the rejection reject is in the left tail, we use the qnorm function to find the critical value of the test statistic.\nThe critical value of the test statistic is -1.282.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nFor this one-tailed hypothesis,\n\\[\\text{If} \\; z \\le -1.28 \\implies \\text{reject} \\; H_0\\] \\[\\text{If} \\; z &gt; -1.28 \\implies \\text{fail to reject} \\; H_0\\]\nIf the estimated test statistic is less than or equal to -1.282, we reject the null hypothesis. If the estimated test statistic is greater than -1.282, then we fail to reject the null hypothesis.\nThe code chunk below graphically depicts the rejection region for this one-tailed hypothesis test.\n\nrm(list = ls())\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(kableExtra)\n\ndf &lt;- data.frame(x = seq(-4, 4, length.out = 100)) %&gt;%\n          mutate(y = dnorm(x))\n\nggplot(df, \n       aes(x = x,\n           y = y)) +\ngeom_area(fill = 'sky blue') +\ngeom_ribbon(data = df %&gt;% filter(x &lt;= -1.28), \n            aes(ymax = y), ymin = 0, fill= \"red\", alpha = 0.5) +\ngeom_vline(aes(xintercept = -1.28), linetype = 'dashed', linewidth = 1.1) +\ntheme_minimal() +\nlabs(x = \"Z\",\n     y = \"P(Z)\",\n     title = \"Standard Normal Distribution\",\n     subtitle = \"One Tailed Rejection Region for Alpha = 0.10 is Z &lt;= -1.28\")",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Hypothesis Testing: Means</span>"
    ]
  },
  {
    "objectID": "mod-8-2-means.html#one-tailed-hypothesis-test-1",
    "href": "mod-8-2-means.html#one-tailed-hypothesis-test-1",
    "title": "37  Hypothesis Testing: Means",
    "section": "37.3 One-Tailed Hypothesis Test",
    "text": "37.3 One-Tailed Hypothesis Test\nWe hypothesize that the population mean is greater than 42 and let \\(\\alpha = 0.05\\).\n\\[H_0: \\mu \\le 42\\]\n\\[H_a: \\mu &gt; 42\\]\nOur rejection region is in the right tail of the standard normal distribution or:\n\\[\\text{If} \\; z \\ge z_c \\implies \\text{reject} \\; H_0\\]\n\\[\\text{If} \\; z &lt; z_c \\implies \\text{fail to reject} \\; H_0\\]\nGiven \\(\\alpha = 0.05\\) and a right-tail test, we need to find \\(z_{(1-\\alpha)} = z_{0.95}\\).\nGiven the \\(\\alpha = 0.05\\) and the rejection reject is in the left tail, we can use the qnorm function to find the critical value of the test statistic.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nFor this one-tailed hypothesis,\n\\[\\text{If} \\; z \\ge 1.65 \\implies \\text{reject} \\; H_0\\]\n\\[\\text{If} \\; z &lt; 1.65 \\implies \\text{fail to reject} \\; H_0\\]If the estimated test statistic is greater than or equal to 1.65, then we reject the null hypothesis. If the estimated test statistic is less than 1.65, we fail to reject the null hypothesis.\nThe code chunk below illustrates the rejection region in question\n\nrm(list = ls())\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(kableExtra)\n\ndf &lt;- data.frame(x = seq(-4, 4, length.out = 100)) %&gt;%\n          mutate(y = dnorm(x))\n\nggplot(df, \n       aes(x = x,\n           y = y)) +\ngeom_area(fill = 'sky blue') +\ngeom_ribbon(data = df %&gt;% filter(x &gt;= 1.65), \n            aes(ymax = y), ymin = 0, fill= \"red\", alpha = 0.5) +\ngeom_vline(aes(xintercept = 1.65), linetype = 'dashed', linewidth = 1.1) +\ntheme_minimal() +\nlabs(x = \"Z\",\n     y = \"P(Z)\",\n     title = \"Standard Normal Distribution\",\n     subtitle = \"One Tailed Rejection Region for Alpha = 0.05 is Z &gt;= 1.65\")",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Hypothesis Testing: Means</span>"
    ]
  },
  {
    "objectID": "mod-8-2-means.html#two-tailed-hypothesis-test",
    "href": "mod-8-2-means.html#two-tailed-hypothesis-test",
    "title": "37  Hypothesis Testing: Means",
    "section": "37.4 Two-Tailed Hypothesis Test",
    "text": "37.4 Two-Tailed Hypothesis Test\nWe hypothesize that the population mean is not equal to 42 or:\n\\[H_0: \\mu = 42\\]\n\\[H_a: \\mu \\ne 42\\]\nThis is a two-tailed hypothesis test.\nThe rejection region lies in each tail of the distribution.\nWhy? Our null hypothesis is that the mean is equal to 42. If find evidence that the mean is sufficiently different from 42, in either direction, we will reject the null hypothesis.\nWe need to find the critical values of \\(z\\) for \\(\\alpha/2\\) and \\(1-\\alpha/2\\).\nLet \\(\\alpha = 0.05\\) so \\(\\alpha/2 = 0.025\\) and \\(1-\\alpha/2 = 0.975\\).\nThe critical values of the test statistic can be found using R as before. Since the standard normal is symmetrically distributed around the mean of zero, the critical values are -1.96 and 1.96.\n\nrm(list = ls())\n\nqnorm(0.025,0,1)\n\n[1] -1.959964\n\nqnorm(0.975,0,1)\n\n[1] 1.959964\n\n\nFor this two-tailed hypothesis,\n\\[\\text{If} \\; z \\le -1.96 \\implies \\text{reject} \\; H_0\\]\n\\[\\text{If} \\; z \\ge 1.96 \\implies \\text{reject} \\; H_0\\]\n\\[\\text{If} \\; -1.95 &lt; z &lt; 1.96 \\implies \\text{fail to reject} \\; H_0\\]\nIf the estimated test statistic is less than or equal to -1.96 or less than or equal to 1.96, then we reject the null hypothesis.\nThe code chunk below illustrates the rejection regions for the hypothesis test.\n\nrm(list = ls())\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(kableExtra)\n\ndf &lt;- data.frame(x = seq(-3, 3, length.out = 100), by = 0.1) %&gt;%\n          mutate(y = dnorm(x))\n\nggplot(df, \n       aes(x = x,\n           y = y)) +\ngeom_area(fill = 'sky blue') +\ngeom_ribbon(data = subset(df, x &lt;= -1.96), aes(ymax = y), \n            ymin = 0, fill= \"red\", alpha = 0.5) +\ngeom_ribbon(data = subset(df, x &gt;= 1.96), aes(ymax = y), \n            ymin = 0, fill= \"red\", alpha = 0.5) +\ngeom_vline(aes(xintercept = -1.96), linetype = 'dashed') +\ngeom_vline(aes(xintercept = 1.96), linetype = 'dashed') +\ntheme_minimal() +\nlabs(x = \"Z\",\n     y = \"P(Z)\",\n     title = \"Standard Normal - Two Tailed Hypothesis Test\",\n     subtitle = \"Alpha = 0.05 and Z &lt;= -1.96 and Z =&gt; 1.96\")",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Hypothesis Testing: Means</span>"
    ]
  },
  {
    "objectID": "mod-8-2-means.html#one-tailed-example",
    "href": "mod-8-2-means.html#one-tailed-example",
    "title": "37  Hypothesis Testing: Means",
    "section": "37.5 One-Tailed Example",
    "text": "37.5 One-Tailed Example\nWe have collected a sample of 50 individuals and assume that the 50 individuals are representative of the population. The individuals take a standard test after receiving training and the average of their scores was 19.4. We know that, in general, the mean score on the test was 20.0.\nDid the test lower the average test score? Test this hypothesis at a 5% significance level.\nConsider the following hypothesis test with \\(\\alpha = 0.05\\).\n\\[H_0: \\mu \\ge 20\\]\n\\[H_a: \\mu &lt; 20\\]\nThe critical value of the test statistic for \\(\\alpha = 0.05\\) is -1.65.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nLet\n\\[\nn = 50\n\\]\n\\[\n\\bar{x} = 50\n\\]\n\\[\n\\sigma = 2\n\\]\nGiven the known population variance and standard deviation, we can estimate the Z-score for the average score of the 50 individuals or:\n\\[z =  \\frac{\\bar{x} - \\mu_0}{\\sigma/\\sqrt{n}} = \\frac{19.4 - 20}{2/\\sqrt{50}} = -2.12132\\]\nWe reject \\(H_0\\) at the 5% level of statistical significance as\n\\[z = -2.12 &lt; -1.65 = z_c\\]\nWe can also use pnorm to find \\(P(z \\le -2.12132) =\\) 0.0169 and reject \\(H_0\\) since:\n\\[P(z \\le -2.12132) = 0.017 &lt; 0.05 = P(z &lt; 1.65)\\]\n\nrm(list = ls())\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(kableExtra)\n\nxbar = 19.4\nmu = 20\nn = 50\nsigma = 2\n\ndf &lt;- tibble(crit_z &lt;- qnorm(0.05,0,1),\n             z &lt;- (xbar-mu)/(sigma/sqrt(n)),\n             pnorm(z, 0, 1))\n\nkable(df,\n      digits = 3,\n      align = 'c',\n      col.names = c('Critical Z', 'Estimated Z', 'P(Z &lt;= z)')) %&gt;%\nkable_classic()\n\n\n\n\nCritical Z\nEstimated Z\nP(Z &lt;= z)\n\n\n\n\n-1.645\n-2.121\n0.017",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Hypothesis Testing: Means</span>"
    ]
  },
  {
    "objectID": "mod-8-2-means.html#one-tailed-example-1",
    "href": "mod-8-2-means.html#one-tailed-example-1",
    "title": "37  Hypothesis Testing: Means",
    "section": "37.6 One-Tailed Example",
    "text": "37.6 One-Tailed Example\nConsider the following hypothesis test with \\(\\alpha = 0.10\\).\n\\[H_0: \\mu \\le 30\\]\n\\[H_a: \\mu &gt; 30\\]\nLet \\(n = 90\\), \\(\\bar{x} = 31.7\\), and \\(\\sigma = 12.5\\), then\n\\[z =  \\frac{\\bar{x} - \\mu_0}{\\sigma/\\sqrt{n}} = \\frac{31.2 - 30}{12.5/\\sqrt{90}} = 0.91\\]\nThe critical value of the test statistic for \\(1 - \\alpha = 0.90\\) is 1.282.\nWe fail to reject \\(H_0\\) at the 10% level of statistical significance as:\n\\[z = 0.91 &lt; 1.28 = z_c\\]\nWe can also use pnorm to find \\(P(z \\le 0.91) =\\) 0.819 and fail to reject \\(H_0\\) since:\n\\[P(z \\le 0.91) = 0.812 &lt; 0.90 = P(z &lt; 1.28)\\]\n\nrm(list = ls())\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(kableExtra)\n\nxbar = 31.2\nmu = 30\nsigma = 12.5\nn = 90\n\ndf &lt;- tibble(crit_z &lt;- qnorm(0.90,0,1),\n             z &lt;- (xbar-mu)/(sigma/sqrt(n)),\n             pnorm(z, 0, 1))\n\nkable(df,\n      digits = 3,\n      align = 'c',\n      col.names = c('Critical Z', 'Estimated Z', 'P(Z &lt;= z)')) %&gt;%\nkable_classic()\n\n\n\n\nCritical Z\nEstimated Z\nP(Z &lt;= z)\n\n\n\n\n1.282\n0.911\n0.819",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Hypothesis Testing: Means</span>"
    ]
  },
  {
    "objectID": "mod-8-2-means.html#hypothesis-test-in-r",
    "href": "mod-8-2-means.html#hypothesis-test-in-r",
    "title": "37  Hypothesis Testing: Means",
    "section": "37.7 Hypothesis Test in R",
    "text": "37.7 Hypothesis Test in R\nAssume that we observe pressure readings for a pipe.\nWe want to know whether pressure is higher than 127.9.\nWe know that \\(\\sigma = 8\\) and we set \\(\\alpha = 0.05\\).\nWe find that \\(z = 0.693\\) and that \\(z_c = 1.65\\).\nWe find that \\(P(z \\ge 0.693) = 0.2443\\).\nWe fail to reject \\(H_0: \\mu \\le 127.9\\) at the 5% level of significance.\nWe would also fail to reject the null at the 10% level of significance.\n\nrm(list = ls())\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(kableExtra)\n\nmu = 127.9\nsigma = 8\nalpha = 0.05\n\npressure &lt;- c(110, 132, 138, 98, 122, 190, 92, 105, 104, 148, \n              83, 193, 132, 114, 192, 183, 100, 99, 111, 123, \n              130, 194, 141, 138, 98, 94, 97, 128, 105, 108, \n              193, 85, 80, 132, 134, 148, 192)\n\nxbar = mean(pressure)\n\nz = (mean(pressure) - mu)/(sigma/sqrt(length(pressure)))\n\nz_crit &lt;- qnorm((1-alpha),0,1)\np_z &lt;- 1-pnorm(z,0,1)\n\ntest &lt;- data.frame(mu, xbar, alpha, z, z_crit, p_z)\n\nkable(test,\n      align     = \"c\",\n      digits    = 3,\n      caption   = \"Hypothesis Test\",\n      col.names = c(\"Hypothesized Mean\", \"Sample Avg\", \n                    \"Sig. Level\", \"Z\", \"Z(c)\",\n                    \"1-CDF(Z)\")) %&gt;%\nkable_styling(font_size = 11)\n\n\nHypothesis Test\n\n\nHypothesized Mean\nSample Avg\nSig. Level\nZ\nZ(c)\n1-CDF(Z)\n\n\n\n\n127.9\n128.811\n0.05\n0.693\n1.645\n0.244",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Hypothesis Testing: Means</span>"
    ]
  },
  {
    "objectID": "mod-8-2-means.html#hypothesis-test-in-r-1",
    "href": "mod-8-2-means.html#hypothesis-test-in-r-1",
    "title": "37  Hypothesis Testing: Means",
    "section": "37.8 Hypothesis Test in R",
    "text": "37.8 Hypothesis Test in R\nWe observe pressure readings for a pipe.\nWe hypothesize that \\(\\mu = 127.3\\).\nLet \\(\\sigma = 10\\) and \\(\\alpha = 0.1\\).\nTwo-tailed hypothesis: Find \\(\\alpha/2\\) and \\(1-\\alpha/2\\) critical values.\n\\(z_c = -1.65\\) and \\(z_c = 1.65\\)\nWe find \\(z = 2.66\\).\nWe reject \\(H_0\\) at 5% as \\(z &gt; z_c\\).\nWe also reject \\(H_0\\) at 1% as \\(P(Z \\ge z) = .004 &gt; 0.005 = \\alpha/2\\).\n\nrm(list = ls())\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(kableExtra)\n\nmu = 127.3\nsigma = 10\nalpha = 0.1\n\npressure &lt;- c(110, 132, 138, 98, 122, 190, 92, 105, 104, 148, \n              83, 193, 132, 114, 192, 183, 100, 99, 111, 123, \n              130, 194, 141, 138, 98, 94, 97, 128, 105, 108, \n              193, 85, 80, 132, 134, 148, 192, 218, 113, \n              138, 83, 103, 109, 183, 148, 153, 135, 146)\n\nxbar = mean(pressure)\n\nz = (mean(pressure) - mu)/(sigma/sqrt(length(pressure)))\n\nz_crit_left &lt;- qnorm((alpha/2),0,1)\nz_crit_right &lt;- qnorm(1 - (alpha/2),0,1)\np_z &lt;- pnorm(z,0,1, lower.tail = FALSE)\n\ntest &lt;- data.frame(mu, xbar, alpha, z, z_crit_right, p_z)\n\nkable(test,\n      align     = \"cccccc\",\n      caption   = \"Hypothesis Test\",\n      col.names = c(\"Hypothesized Mean\", \"Sample Avg\", \n                    \"Sig. Level\", \"Z\", \"Z(c) at 5%\",\n                    \"1-CDF(Z)\")) %&gt;%\n  kable_styling(font_size = 11)\n\n\nHypothesis Test\n\n\nHypothesized Mean\nSample Avg\nSig. Level\nZ\nZ(c) at 5%\n1-CDF(Z)\n\n\n\n\n127.3\n131.1458\n0.1\n2.664471\n1.644854\n0.0038555",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Hypothesis Testing: Means</span>"
    ]
  },
  {
    "objectID": "mod-8-2-means.html#unknown-variance",
    "href": "mod-8-2-means.html#unknown-variance",
    "title": "37  Hypothesis Testing: Means",
    "section": "37.9 Unknown Variance",
    "text": "37.9 Unknown Variance\nIf \\(\\sigma\\) is unknown, the test statistic has a t distribution with \\(n-1\\) degrees of freedom.\n\\[t = \\frac{\\bar{x} - \\mu}{s/\\sqrt{n}}\\]\nConsider the following hypothesis test:\n\\[H_0: \\mu \\le 12\\]\n\\[H_a: \\mu &gt; 12\\]\nIf \\(n = 25\\), \\(\\bar{x} = 14\\), and \\(s = 4.32\\), then\n\\[t_{n-1} = \\frac{\\bar{x} - \\mu}{s/\\sqrt{n}} = \\frac{14 - 12}{4.32/\\sqrt{25}} = 2.32\\]\nLet \\(\\alpha = 0.5\\) thus the critical value of the t-statistic is 1.7108821.\nWe reject the null hypothesis at the 5% level of significance since:\n\\[t = 2.32 &gt; 1.71 = t_c^{\\alpha = 0.05}\\]",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Hypothesis Testing: Means</span>"
    ]
  },
  {
    "objectID": "mod-8-2-means.html#hypothesis-testing-1",
    "href": "mod-8-2-means.html#hypothesis-testing-1",
    "title": "37  Hypothesis Testing: Means",
    "section": "37.10 Hypothesis Testing",
    "text": "37.10 Hypothesis Testing\nLet \\(n = 50\\), \\(\\bar{x} = 38.7\\), \\(s = 4.58\\), and \\(\\alpha = 0.1\\) and\n\\[H_0: \\mu = 39.9\\]\n\\[H_0: \\mu \\ne 39.9\\]\nThe test statistic is equal to:\n\\[t_{n-1} = \\frac{\\bar{x} - \\mu}{s/\\sqrt{n}} = \\frac{38.7 - 39.9}{4.58/\\sqrt{50}} = -1.85\\]\nThe critical value of the test-statistic is 1.6765509.\nWe reject the null hypothesis at the 10% level of significance since:\n\\[t = -1.85 &lt; -1.68 = t_c^{\\alpha/2 = 0.05}\\]\nHowever, we fail to reject at the 5% level of significance since:\n\\[t = -1.85 &gt; -2.01 = t_c^{\\alpha/2 = 0.025}\\]",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Hypothesis Testing: Means</span>"
    ]
  },
  {
    "objectID": "mod-8-2-means.html#standard-normal-and-t-distribution",
    "href": "mod-8-2-means.html#standard-normal-and-t-distribution",
    "title": "37  Hypothesis Testing: Means",
    "section": "37.11 Standard Normal and T Distribution",
    "text": "37.11 Standard Normal and T Distribution\n\nrm(list = ls())\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(kableExtra)\n\ndf &lt;- data.frame(x = seq(-5, 5, length.out = 100), by = 0.1) %&gt;%\n          mutate(y = dt(x, 1))\n\nggplot(df,\n       aes(x = x,\n           y = y,\n           color = \"red\")) +\n  geom_line(linewidth = 1.2) +\n  stat_function(fun = dnorm,\n                args = list(mean = 0, sd = 1),\n                color = \"dark blue\",\n                size = 1.2) +\n  theme_minimal() +\n  theme(legend.position = \"\") +\n  labs(title = \"Standard Normal and T-Distribution\",\n       subtitle = \"1 Degree of Freedom for T-Distribution\",\n       y = \"P(X)\",\n       x = \"X\")",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Hypothesis Testing: Means</span>"
    ]
  },
  {
    "objectID": "mod-8-2-means.html#rejection-regions",
    "href": "mod-8-2-means.html#rejection-regions",
    "title": "37  Hypothesis Testing: Means",
    "section": "37.12 Rejection Regions",
    "text": "37.12 Rejection Regions\n\nrm(list = ls())\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(kableExtra)\n\ndf &lt;- data.frame(x = seq(-3, 3, length.out = 100), by = 0.1) %&gt;%\n          mutate(y = dt(x, 24))\n\nggplot(df, \n       aes(x = x,\n           y = y)) +\ngeom_area(fill = 'sky blue') +\ngeom_ribbon(data = subset(df, x &gt;= 1.71),\n            aes(ymax = y), \n            ymin = 0, fill= \"red\", alpha = 0.5) +\ngeom_vline(aes(xintercept = 1.71), \n           linetype = 'dashed', size = 1.1) +\ntheme_minimal() +\nlabs(x = \"X\",\n     y = \"P(X)\",\n     title = \"T Distribution and One-Tailed Hypothesis Test\",\n     subtitle = \"Alpha = 0.05, 24 DF, and Critical T =&gt; 1.71\")",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Hypothesis Testing: Means</span>"
    ]
  },
  {
    "objectID": "mod-8-2-means.html#hypothesis-test-in-r-2",
    "href": "mod-8-2-means.html#hypothesis-test-in-r-2",
    "title": "37  Hypothesis Testing: Means",
    "section": "37.13 Hypothesis Test in R",
    "text": "37.13 Hypothesis Test in R\nWe observe pressure readings.\nWe hypothesize that \\(\\mu = 127.3\\).\n\\(n= 48\\), \\(s = 35.3\\), \\(\\alpha = 0.01\\)\nFind \\(\\alpha/2\\) and \\(1-\\alpha/2\\) critical values.\n\\(t_c = -2.69\\) and \\(t_c = 2.69\\)\n\\(t = 2.37\\).\nWe fail to reject \\(H_0\\) as \\(t &lt; t_c\\).\nWe fail to reject \\(H_0\\) as \\(P(T &gt; t) = .011 &gt; 0.005 = \\alpha/2\\).\nWe can, however, reject \\(H_0\\) at 5%.\n\nrm(list = ls())\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(kableExtra)\n\nmu = 127.3\nalpha = 0.01\n\npressure &lt;- c(90, 76, 98, 108, 102, 100, 92, 105, 104, 148, \n              83, 93, 132, 114, 92, 103, 100, 99, 111, 123, \n              130, 194, 141, 138, 98, 94, 97, 128, 105, 108, \n              193, 85, 80, 132, 134, 148, 192, 218, 113, \n              138, 83, 103, 109, 113, 138, 103, 105, 106)\n\nxbar = mean(pressure)\nsd = sd(pressure)\n\nt = (mean(pressure) - mu)/(sd/sqrt(length(pressure)))\n\nt_crit_left &lt;- qt((alpha/2),length(pressure) - 1)\nt_crit_right &lt;- qt(1 - (alpha/2), length(pressure - 1))\n\nt_p &lt;- pt(t, length(pressure -1))\n\ntest &lt;- data.frame(mu, xbar, alpha, t, t_crit_left, t_p)\n\nkable(test,\n      align     = \"c\",\n      digits    = 3,\n      caption   = \"Hypothesis Test\",\n      col.names = c(\"Hypothesized Mean\", \"Sample Avg\", \n                    \"Sig. Level\", \"T\", \"T(c) at 1%\",\n                    \"CDF(T)\")) %&gt;%\n  kable_styling(font_size = 11)\n\n\nHypothesis Test\n\n\nHypothesized Mean\nSample Avg\nSig. Level\nT\nT(c) at 1%\nCDF(T)\n\n\n\n\n127.3\n116.646\n0.01\n-2.366\n-2.685\n0.011",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Hypothesis Testing: Means</span>"
    ]
  },
  {
    "objectID": "mod-8-3-variance.html",
    "href": "mod-8-3-variance.html",
    "title": "38  Hypothesis Testing: Variance",
    "section": "",
    "text": "38.1 Difference in Means\nLet \\(\\mu_1\\) denote the mean of population 1.\nlet \\(\\mu_2\\) denote the mean of population 2.\nAssume that we have two independent random samples that produce \\(\\bar{x_1}\\) and \\(\\bar{x_2}\\).\nThe point estimator for the difference between \\(\\mu_1\\) and \\(\\mu_2\\) is:\n\\[\\bar{x}_1- \\bar{x}_2\\]\nIf \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\) are known, the standard error of the point estimator is:\n\\[\\sigma(\\bar{x}_1 -\\bar{x}_2) = \\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}\\] The interval estimate for the difference between two population means when the population variances are known:\n\\[(\\bar{x}_1 - \\bar{x}_2) \\pm z_{\\alpha/2} \\times \\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}\\]",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Hypothesis Testing: Variance</span>"
    ]
  },
  {
    "objectID": "mod-8-3-variance.html#difference-example",
    "href": "mod-8-3-variance.html#difference-example",
    "title": "38  Hypothesis Testing: Variance",
    "section": "38.2 Difference Example",
    "text": "38.2 Difference Example\nWe know that \\(\\sigma_1 = 3\\) and \\(\\sigma_2 = 6\\)\nAssume we sample the populations independently and randomly by taking 200 observations from each population.\nWe can construct the point estimate of the difference of the population means.\nWe can also construct the interval estimate (or margin of error) around this point estimate.\nWe specify that \\(\\alpha = 0.05\\).\n\nrm(list = ls())\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(kableExtra)\n\npop_1 &lt;- rnorm(10000, mean = 50.5, sd = 3)\npop_2 &lt;- rnorm(5000, mean = 47.5, sd = 6)\n\nxbar_1 &lt;- mean(sample(pop_1, 500))\nxbar_2 &lt;- mean(sample(pop_2, 200))\n\ndiff &lt;- xbar_1 - xbar_2\n\nstd_err &lt;- sqrt((9/500) + (36/200))\n\ninterval = xbar_1 - xbar_2\nmoe = qnorm(.975,0,1)*std_err\n\nint_lower = interval - qnorm(.975,0,1)*std_err\nint_upper = interval + qnorm(.975,0,1)*std_err\n\nmeans &lt;- data.frame(xbar_1, xbar_2, diff, std_err,\n                    moe, int_lower, int_upper)\n\nkable(means,\n      align     = \"ccccccc\",\n      caption   = \"Interval Estimation\",\n      col.names = c(\"Xbar(1)\", \"Xbar(2)\", \n                    \"Xbar(1)-Xbar(2)\",\n                    \"Std. Error\", \"MOE\", \n                    \"Lower Bound\", \"Upper Bound\")) %&gt;%\n  kable_styling(font_size = 11)\n\n\nInterval Estimation\n\n\nXbar(1)\nXbar(2)\nXbar(1)-Xbar(2)\nStd. Error\nMOE\nLower Bound\nUpper Bound\n\n\n\n\n50.29403\n46.79043\n3.503592\n0.4449719\n0.8721289\n2.631464\n4.375721",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Hypothesis Testing: Variance</span>"
    ]
  },
  {
    "objectID": "mod-8-3-variance.html#difference-in-means-1",
    "href": "mod-8-3-variance.html#difference-in-means-1",
    "title": "38  Hypothesis Testing: Variance",
    "section": "38.3 Difference in Means",
    "text": "38.3 Difference in Means\nLet \\(n_1\\) and \\(n_2\\) be samples used to estimate \\(\\bar{x}_1\\) and \\(\\bar{x}_2\\) and \\(\\sigma_1\\) and \\(\\sigma_2\\) are the population standard deviations for the two independent random samples.\nLet \\(D_0 = \\mu_1 - \\mu_2\\) be the hypothesized difference between the population means under \\(H_0\\).\nThe test-statistic for testing hypotheses regarding the difference between two population means when \\(\\sigma_1\\) and \\(\\sigma_2\\) are known is:\n\\[z = \\frac{(\\bar{x}_1 - \\bar{x}_2) - D_0}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\\]\nGiven a level of significance, \\(\\alpha\\), we can obtain the critical value(s) of the test statistic.\nIf it is a one-tailed test, then we use \\(\\alpha\\) to obtain the test statistic.\nIf it is a two-tailed test, then we use \\(\\alpha/2\\) to obtain the test statistic.",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Hypothesis Testing: Variance</span>"
    ]
  },
  {
    "objectID": "mod-8-3-variance.html#difference-example-1",
    "href": "mod-8-3-variance.html#difference-example-1",
    "title": "38  Hypothesis Testing: Variance",
    "section": "38.4 Difference Example",
    "text": "38.4 Difference Example\nConsider the following hypothesis test with \\(\\alpha = 0.05\\).\n\\[H_0: \\mu_1 - \\mu_2 = 0\\]\n\\[H_a: \\mu_1 - \\mu_2 \\ne 0\\]\nLet \\(n_1 = 50\\), \\(\\bar{x}_1 = 19.4\\), \\(\\sigma_1 = 2\\) and \\(n_2 = 25\\), \\(\\bar{x}_2 = 20.5\\), \\(\\sigma_2 = 4\\).\n\\[z = \\frac{(\\bar{x}_1 - \\bar{x}_2) - D_0}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} = \\frac{(19.4 - 20.5) - 0}{\\sqrt{\\frac{2^2}{50} + \\frac{4^2}{25}}} = \\frac{-1.1}{0.8485} = -1.2964\\]\nFor a two-tailed test, the critical value of the test statistic is \\(z_c =\\) -1.959964.\nWe fail to reject \\(H_0\\) at the 5% level of statistical significance as\n\\[z = -1.2964 &gt; -1.96 = z_c\\]\nWe can use pnorm as \\(P(z \\le -1.2964) = 0.0974\\) and fail to reject \\(H_0\\) since:\n\\[P(z \\le -1.2964) = 0.0974 &gt; 0.025 = P(z &lt; -1.96)\\]",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Hypothesis Testing: Variance</span>"
    ]
  },
  {
    "objectID": "mod-8-3-variance.html#difference-example-2",
    "href": "mod-8-3-variance.html#difference-example-2",
    "title": "38  Hypothesis Testing: Variance",
    "section": "38.5 Difference Example",
    "text": "38.5 Difference Example\nConsider the following hypothesis test with \\(\\alpha = 0.01\\).\n\\[H_0: \\mu_1 - \\mu_2 \\ge 0\\]\n\\[H_a: \\mu_1 - \\mu_2 \\le 0\\]\nLet \\(n_1 = 25\\), \\(\\bar{x}_1 = 18.4\\), \\(\\sigma_1 = 4\\) and \\(n_2 = 50\\), \\(\\bar{x}_2 = 20.8\\), \\(\\sigma_2 = 3\\).\n\\[z = \\frac{(\\bar{x}_1 - \\bar{x}_2) - D_0}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} = \\frac{(18.4 - 20.8) - 0}{\\sqrt{\\frac{4^2}{25} + \\frac{3^2}{50}}} = \\frac{-2.4}{0.9055} = -2.6504\\]\nFor a one-tailed test, the critical value of the test statistic is -2.3264.\nWe reject \\(H_0\\) at the 1% level of statistical significance as\n\\[z = -2.6504 &lt; -2.3264 = z_c\\]\nWe can use pnorm as \\(P(z \\le -2.6504)\\) and reject \\(H_0\\) since:\n\\[P(z \\le -2.6504) = 0.004 &lt; 0.01 = P(z &lt; -2.3264)\\]",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Hypothesis Testing: Variance</span>"
    ]
  },
  {
    "objectID": "mod-8-3-variance.html#unknown-variance",
    "href": "mod-8-3-variance.html#unknown-variance",
    "title": "38  Hypothesis Testing: Variance",
    "section": "38.6 Unknown Variance",
    "text": "38.6 Unknown Variance\nLet \\(n_1\\) and \\(n_2\\) be samples used to estimate \\(\\bar{x}_1\\) and \\(\\bar{x}_2\\). The population standard deviations for the two independent random samples are unknown.\nWe assume that \\(s_1\\) and \\(s_2\\) are unbiased estimators of the population standard deviations.\nLet \\(D_0 = \\mu_1 - \\mu_2\\) be the hypothesized difference between the population means under \\(H_o\\).\nThe test-statistic for testing hypotheses regarding the difference between two population means when \\(\\sigma\\) is unknown is:\n\\[t = \\frac{(\\bar{x}_1 - \\bar{x}_2) - D_0}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\\]\nGiven a level of significance, \\(\\alpha\\), we can obtain the critical value of the test statistic.\nIf it is a one-tailed test, then we use \\(\\alpha\\) to obtain the test statistic.\nIf it is a two-tailed test, then we use \\(\\alpha/2\\) to obtain the test statistic.",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Hypothesis Testing: Variance</span>"
    ]
  },
  {
    "objectID": "mod-8-3-variance.html#hypothesis-test-in-r",
    "href": "mod-8-3-variance.html#hypothesis-test-in-r",
    "title": "38  Hypothesis Testing: Variance",
    "section": "38.7 Hypothesis Test in R",
    "text": "38.7 Hypothesis Test in R\nAssume that we observe pressure readings for two pipes which are independent of each other.\nWe want to know whether the mean pressure readings for pipes are different.\nWe do not observe \\(\\sigma_1\\) or \\(\\sigma_2\\).\nWe can use the t.test function in R.\nWe find that \\(t = -0.14751\\).\nWe fail to reject the null hypothesis that the population means are equal.\n\nrm(list = ls())\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(kableExtra)\n\npressure_1 &lt;- c(110, 132, 138, 98, 122, 190, 92, 105, 104, 148, \n              83, 193, 132, 114, 192, 183, 100, 99, 111, 123, \n              130, 194, 141, 138, 98, 94, 97, 128, 105, 108, \n              193, 85, 80, 132, 134, 148, 192)\n\npressure_2 &lt;- c(140, 112, 98, 98, 192, 190, 93, 125, 144, 146, \n              87, 195, 112, 124, 132, 185, 102, 139, 101, 125, \n              123, 144, 123, 108, 128, 114, 107, 134, 115, 88, \n              143, 124, 120, 135, 121, 168, 172)\n\n# Manual T-Test, H0: mu(1)-mu(2) = 0\n\nxbar_1 &lt;- mean(pressure_1)\nxbar_2 &lt;- mean(pressure_2)\n\ndiff &lt;- xbar_1 - xbar_2 - 0\n\ndenom &lt;- sqrt((var(pressure_1)/length(pressure_1)) + \n                (var(pressure_2)/length(pressure_2)))\n\nt_man &lt;- diff/denom\n\n# Generated T-Test\nt_calc &lt;- t.test(pressure_1, pressure_2, mu = 0)\n\nmeans &lt;- data.frame(xbar_1, xbar_2, diff, \n                    t_man, t_calc$statistic, \n                    t_calc$parameter, t_calc$p.value)\n\nkable(means,\n      align     = \"ccccccc\",\n      caption   = \"Hypothesis Testing\",\n      col.names = c(\"Xbar(1)\", \"Xbar(2)\", \n                    \"Xbar(1)-Xbar(2)\",\n                    \"T-Manual\", \"T-Calculated\", \n                    \"Degrees Freedom\", \"P-Value\")) %&gt;%\n  kable_styling(font_size = 11)\n\n\nHypothesis Testing\n\n\n\nXbar(1)\nXbar(2)\nXbar(1)-Xbar(2)\nT-Manual\nT-Calculated\nDegrees Freedom\nP-Value\n\n\n\n\nt\n128.8108\n129.9189\n-1.108108\n-0.1475103\n-0.1475103\n69.28732\n0.8831577",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Hypothesis Testing: Variance</span>"
    ]
  },
  {
    "objectID": "mod-8-3-variance.html#hypothesis-test-in-r-1",
    "href": "mod-8-3-variance.html#hypothesis-test-in-r-1",
    "title": "38  Hypothesis Testing: Variance",
    "section": "38.8 Hypothesis Test in R",
    "text": "38.8 Hypothesis Test in R\nAssume that we observe pressure readings for two pipes which are independent of each other.\nWe want to know whether \\(\\mu_1 - \\mu_2 &gt; 0\\).\nWe do not observe \\(\\sigma_1\\) or \\(\\sigma_2\\).\nWe can use the t.test function in R.\nWe find that \\(t = 3.75\\) and \\(P(t &lt; T_c) = 0.0003\\).\nWe reject the null hypothesis that \\(\\mu_1 - \\mu_2 \\le 0\\) at the 1% level of statistical significance.\n\nrm(list = ls())\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(kableExtra)\n\npressure_3 &lt;- c(210, 32, 238, 198, 322, 290, 192, 105, 204, 248, \n              83, 103, 232, 114, 192, 283, 200, 99, 111, 193, \n              150, 94, 241, 238, 198, 194, 197, 128, 215, 108, \n              133, 185, 180, 141, 234, 48, 92, 281, 47, 290)\n\npressure_4 &lt;- c(140, 112, 98, 98, 192, 190, 93, 125, 144, 146, \n              87, 195, 112, 124, 132, 185, 102, 139, 101, 125, \n              123, 144, 123, 108, 128, 114, 107, 134, 115, 88, \n              143, 124, 120, 135, 121, 168, 172, 129, 103)\n\n# Manual T-Test, H0: mu(1)-mu(2) &gt; 0\n\nxbar_3 &lt;- mean(pressure_3)\nxbar_4 &lt;- mean(pressure_4)\n\ndiff &lt;- xbar_3 - xbar_4 - 0\n\ndenom &lt;- sqrt((var(pressure_3)/length(pressure_3)) + \n                (var(pressure_4)/length(pressure_4)))\n\nt_man &lt;- diff/denom\n\n# Generated T-Test\nt_calc &lt;- t.test(pressure_3, pressure_4, \n                 alternative = \"greater\", mu = 0)\n\nmeans &lt;- data.frame(xbar_3, xbar_4, diff, \n                    t_man, t_calc$statistic, \n                    t_calc$parameter, t_calc$p.value)\n\nkable(means,\n      align     = \"ccccccc\",\n      caption   = \"Hypothesis Testing\",\n      col.names = c(\"Xbar(1)\", \"Xbar(2)\", \n                    \"Xbar(1)-Xbar(2)\",\n                    \"T-Manual\", \"T-Calculated\", \n                    \"Degrees Freedom\", \"P-Value\")) %&gt;%\n  kable_styling(font_size = 11)\n\n\nHypothesis Testing\n\n\n\nXbar(1)\nXbar(2)\nXbar(1)-Xbar(2)\nT-Manual\nT-Calculated\nDegrees Freedom\nP-Value\n\n\n\n\nt\n176.075\n129.2051\n46.86987\n3.749521\n3.749521\n50.68183\n0.0002278",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Hypothesis Testing: Variance</span>"
    ]
  },
  {
    "objectID": "mod-8-3-variance.html#chi-squared-distribution",
    "href": "mod-8-3-variance.html#chi-squared-distribution",
    "title": "38  Hypothesis Testing: Variance",
    "section": "38.9 Chi-Squared Distribution",
    "text": "38.9 Chi-Squared Distribution\nLet \\(x_1, x_2, \\dots, x_n\\) be \\(n\\) identically and independently distributed random variables such that \\(x_i \\sim N(\\mu,\\sigma)\\).\nLet \\(\\sigma_1, \\sigma_2, \\dots, \\sigma_n\\) be the standard deviations of \\(x_1, x_2, \\dots, x_n\\).\nConsider the following two measures about the sample data and its relationship with the population parameters:\n\\[R = \\sum_{i=1}^n \\frac{(x_i - \\mu)}{\\sigma_i} \\sim N(0,1)\\]\n\\[V = \\sum_{i=1}^n \\frac{(x_i - \\mu)^2}{\\sigma_i^2} \\sim \\chi^2(k)\\]\nR is the sum of \\(n\\) standard normal variables and is distributed normally.\nV is the sum of \\(n\\) squared standard normal variables and is distributed \\(\\chi^2\\) with \\(k\\) degrees of freedom.",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Hypothesis Testing: Variance</span>"
    ]
  },
  {
    "objectID": "mod-8-3-variance.html#chi-square-distribution",
    "href": "mod-8-3-variance.html#chi-square-distribution",
    "title": "38  Hypothesis Testing: Variance",
    "section": "38.10 Chi-Square Distribution",
    "text": "38.10 Chi-Square Distribution\n\nrm(list = ls())\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(kableExtra)\n\nlibrary(ggplot2)\n\nggplot(data.frame(x = c(-10, 10)), \n       aes(x = x)) +\nstat_function(fun = dchisq, \n              args = list(df = 1),\n              linewidth = 1.2) +\nstat_function(fun = dnorm, \n              args = list(mean = 0, sd = 1),\n              linewidth = 1.2,\n              color = \"red\") +\ntheme_minimal() +\nlabs(x = \"x\",\n     y = \"P(x)\",\n     title = \"Standard Normal and Chi-Squared Distribution\",\n     subtitle = \"1 degree of freedom\")\n\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\n\nggplot(data.frame(x = c(-30, 30)), \n       aes(x = x)) +\nstat_function(fun = dchisq, \n              args = list(df = 10),\n              linewidth = 1.2) +\nstat_function(fun = dnorm, \n              args = list(mean = 0, sd = 1),\n              linewidth = 1.2,\n              color = \"red\") +\ntheme_minimal() +\nlabs(x = \"x\",\n     y = \"P(x)\",\n     title = \"Chi-Squared Distribution\",\n     subtitle = \"10 degrees of freedom\")",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Hypothesis Testing: Variance</span>"
    ]
  },
  {
    "objectID": "mod-8-3-variance.html#chi-squared-distributions",
    "href": "mod-8-3-variance.html#chi-squared-distributions",
    "title": "38  Hypothesis Testing: Variance",
    "section": "38.11 Chi-Squared Distributions",
    "text": "38.11 Chi-Squared Distributions\nLet \\(x\\) be the value of V for which we seek the probability or:\n\\[V = \\sum_{i=1}^n \\frac{(x_i - \\mu)^2}{\\sigma_i^2} \\sim \\chi^2\\]\nThe probability density function of the \\(\\chi^2\\) distribution is:\n\\[f(x;k) = \\frac{1}{2^{(k/2)} \\, \\Gamma(k/2)} x^{(k/2)-1} e^{-x/2}, \\quad 0 \\le \\chi^2 \\le 1\\]\nWhere \\(\\Gamma\\) is the Gamma function and\n\\[\\Gamma(z) = \\int_{0}^{\\infty} x^{z-1} \\, e^{-x} dx, \\quad x &gt; 0\\]\n\\[\\Gamma(n) = (n-1)! \\quad \\text{for any positive integer} \\, n\\]",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Hypothesis Testing: Variance</span>"
    ]
  },
  {
    "objectID": "mod-8-3-variance.html#chi-squared-properties",
    "href": "mod-8-3-variance.html#chi-squared-properties",
    "title": "38  Hypothesis Testing: Variance",
    "section": "38.12 Chi-Squared Properties",
    "text": "38.12 Chi-Squared Properties\nGiven \\(x_1, x_2, \\dots, x_n\\) random variables with \\(x_i \\sim N(0,\\sigma^2)\\), then\n\\[V = \\sum_{i=1}^n \\frac{(x_i - \\mu)^2}{\\sigma_i^2} \\sim \\chi^2(k)\\]\nGiven \\(n\\) and \\(k = n-1\\), then the mean of \\(\\chi^2(k)\\) is:\n\\[\\mu(\\chi^2) = k\\]\nThe variance and standard deviation of \\(\\chi^2(k)\\) is:\n\\[var(\\chi^2) = 2 \\times k\\]\n\\[sd(\\chi^2) = \\sqrt{2 \\times df}\\]\nThe \\(\\chi^2\\) distribution is non-symmetrical and skewed right.\nThere is a different \\(\\chi^2\\) curve for each degree of freedom \\(k\\).",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Hypothesis Testing: Variance</span>"
    ]
  },
  {
    "objectID": "mod-8-3-variance.html#chi-squared-examples",
    "href": "mod-8-3-variance.html#chi-squared-examples",
    "title": "38  Hypothesis Testing: Variance",
    "section": "38.13 Chi-Squared Examples",
    "text": "38.13 Chi-Squared Examples\nLet \\(Y\\) have a chi-square distribution with 15 degrees of freedom.\nWhat is \\(P(Y \\le y_0) = 0.025\\)?\nWhat is \\(P(Y \\ge y_0) = 0.05\\)?\nWhat is \\(P(Y \\le 15.23)\\)?\nWhat is \\(P(Y \\ge 22.307)\\)?\nWe can use the \\(\\chi^2\\) tables.\npchisq is the CDF for \\(\\chi^2\\) in R\nqchisq is the inverse CDF in R\n\nrm(list = ls())\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(kableExtra)\n\nk = 15\n\n#P(Y &lt; y0) = 0.025\n#Lower.tail = TRUE since area to left of y_0\n\nq_1 &lt;- qchisq(0.025, 15, lower.tail = TRUE)\n\n#P(Y &gt; y0) = 0.05\n#Lower.tail = FALSE since area to left of y_0\n\nq_2 &lt;- qchisq(0.05, 15, lower.tail = FALSE)\n\n#P(Y &lt; 15.23) = p\n#Lower.tail since area of the left of 15.23\n\nq_3 &lt;- pchisq(15.23, 15, lower.tail = TRUE)\n\n#P(Y &gt; 22.307) = p\n\nq_4 &lt;- pchisq(23.307, 15, lower.tail = FALSE)\n\nqchi &lt;- data.frame(k, q_1, q_2, q_3, q_4)\n\nkable(qchi,\n      align     = \"ccccc\",\n      caption   = \"Chi-Squared Examples\",\n      col.names = c(\"k\", \"P(y &lt; y0) = 0.025\", \n                    \"P(y &gt; y0) = 0.05\",\n                    \"P(y &lt; 15.23)\", \"P(y &gt; 23.307)\")) %&gt;%\n  kable_styling(font_size = 11)\n\n\nChi-Squared Examples\n\n\nk\nP(y &lt; y0) = 0.025\nP(y &gt; y0) = 0.05\nP(y &lt; 15.23)\nP(y &gt; 23.307)\n\n\n\n\n15\n6.262138\n24.99579\n0.5650197\n0.0778356",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Hypothesis Testing: Variance</span>"
    ]
  },
  {
    "objectID": "mod-8-3-variance.html#chi-squared-examples-1",
    "href": "mod-8-3-variance.html#chi-squared-examples-1",
    "title": "38  Hypothesis Testing: Variance",
    "section": "38.14 Chi-Squared Examples",
    "text": "38.14 Chi-Squared Examples\nTypically \\(\\chi^2_{\\alpha}\\) denotes the value for the \\(\\chi^2\\) distribution that provides an area or probability of \\(\\alpha\\) to the right of the \\(\\chi^2_\\alpha\\) value.\nLet \\(X\\) have a chi-square distribution with 19 degrees of freedom.\nWhat is \\(P(X \\ge x_0) = 0.025\\)?\nIn other words, what \\(\\chi^2\\) value is such that 2.5% of the observations lie to the right of it?\nWhat is \\(P(X \\ge x_0) = 0.95\\)?\nIn other words, what \\(\\chi^2\\) value is such that 95% of the observations lie to the right of it?\n\nrm(list = ls())\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(kableExtra)\n\nalpha_1 = 0.025\nalpha_2 = 0.95\ndf_1 = 19\n\n#Obtain chi-squared value for which 2.5% of obs lie to the right\n#Use alpha and lower.tail = FALSE \n#Use (1-alpha) and lower.tail = TRUE\n\nq_5 &lt;- qchisq(alpha_1, df_1, lower.tail = FALSE)\nq_6 &lt;- qchisq(1-alpha_1, df_1, lower.tail = TRUE)\n\n#Obtain chi-squared value for which 95% of obs lie to the right\n#Use alpha and lower.tail = FALSE \n#Use (1-alpha) and lower.tail = TRUE\n\nq_7 &lt;- qchisq(alpha_2, df_1, lower.tail = FALSE)\nq_8 &lt;- qchisq(1-alpha_2, df_1, lower.tail = TRUE)\n\nqchi2 &lt;- data.frame(df_1, q_5, q_6, q_7, q_8)\n\nkable(qchi2,\n      align     = \"ccccc\",\n      caption   = \"Chi-Squared Examples\",\n      col.names = c(\"k\", \"P(X &gt; x0) = 0.025\", \n                    \"P(X &lt; x0) = 0.975\",\n                    \"P(X &gt; x0) = 0.95\", \"P(X &lt; x0) = 0.05\")) %&gt;%\n  kable_styling(font_size = 11)\n\n\nChi-Squared Examples\n\n\nk\nP(X &gt; x0) = 0.025\nP(X &lt; x0) = 0.975\nP(X &gt; x0) = 0.95\nP(X &lt; x0) = 0.05\n\n\n\n\n19\n32.85233\n32.85233\n10.11701\n10.11701",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Hypothesis Testing: Variance</span>"
    ]
  },
  {
    "objectID": "mod-8-3-variance.html#variance-inferences",
    "href": "mod-8-3-variance.html#variance-inferences",
    "title": "38  Hypothesis Testing: Variance",
    "section": "38.15 Variance Inferences",
    "text": "38.15 Variance Inferences\nRecall that the sample variance \\(s^2\\) is the point estimator of the population variance \\(\\sigma^2\\).\nRecall that the sample variance is equal to:\n\\[s^2 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n-1}\\]\nTo make inferences about the population variance, we need to know the sampling distribution of:\n\\[\\frac{(n-1)s^2}{\\sigma^2}\\]\nWe will find that sampling distribution is \\(\\chi^2\\) with \\(k\\) degrees of freedom.\nWe can use the \\(\\chi^2\\) distribution to develop interval estimates and conduct hypothesis tests about a population variance.",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Hypothesis Testing: Variance</span>"
    ]
  },
  {
    "objectID": "mod-8-3-variance.html#sampling-distribution",
    "href": "mod-8-3-variance.html#sampling-distribution",
    "title": "38  Hypothesis Testing: Variance",
    "section": "38.16 Sampling Distribution",
    "text": "38.16 Sampling Distribution\nTo begin, let \\(y_i = x_i - \\mu\\) which implies that \\(\\bar{y} = \\bar{x} - \\mu\\) since \\(\\bar{u} = \\mu\\) and:\n\\[(y_i - \\bar{y}) = (x_i - \\mu) - (\\bar{x} - \\mu) = x_i - \\mu - \\bar{x} + \\mu = (x_i - \\bar{x})\\]\nRearranging \\(s^2\\) yields:\n\\[s^2 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n-1}\\]\n\\[(n-1) s^2 = \\sum_{i=1}^n (x_i - \\bar{x})^2 = \\sum_{i = 1}^n (y_i - \\mu)^2\\]\n\\[(n-1) s^2 = \\sum_{i = 1}^n(y_i^2 - 2 y_i \\bar{y} + \\bar{y}^2) = \\sum_{i=1}^n y_i^2 - 2\\bar{y}\\sum_{i=1}^n Y_i + \\sum_{i=1}^n \\bar{y}^2\\]\nRecall that \\(\\sum y_i = n \\bar{y}\\) and \\(\\sum \\bar{y} = n\\bar{y}\\) so\n\\[(n-1) s^2 = \\sum_{i=1}^n y_i^2 - (2n \\bar{y}  + n \\bar{y}^2) = \\sum_{i=1}^n y_i^2 - n\\bar{y}^2\\]\nSubstituting back \\(y_i = x_i - \\mu\\) yields:\n\\[(n-1) s^2 = \\sum_{i=1}^n (x_i - \\mu)^2 - n(\\bar{x} - \\mu)^2\\]\nDividing through by \\(\\sigma^2\\) and rearranging\n\\[\\frac{(n-1) s^2}{\\sigma^2} = \\sum_{i=1}^n \\frac{(x_i - \\mu)^2}{\\sigma^2} - \\frac{n(\\bar{x} - \\mu)^2}{\\sigma^2}\\]\n\\[\\sum_{i=1}^n \\bigg[\\frac{(x_i - \\mu)}{\\sigma}\\bigg]^2 = \\frac{(n-1) s^2}{\\sigma^2} + \\bigg[\\frac{(\\bar{x} - \\mu)}{\\sigma/\\sqrt{n}}\\bigg]^2\\]\nThe left hand side is the sum of the squares of \\(n\\) independent standard normal variables.\nThe second term on the right hand side is the square of a single standard normal variable.\nIf \\(s^2\\) and \\(\\bar{x}\\) are independent, then the first term on the right hand side must have the same distribution as the left hand side.\nEach term thus has the same distribution: \\(\\chi^2\\) with \\(k\\) degrees of freedom.",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Hypothesis Testing: Variance</span>"
    ]
  },
  {
    "objectID": "mod-8-3-variance.html#interval-estimation---variance",
    "href": "mod-8-3-variance.html#interval-estimation---variance",
    "title": "38  Hypothesis Testing: Variance",
    "section": "38.17 Interval Estimation - Variance",
    "text": "38.17 Interval Estimation - Variance\nGiven that\n\\[\\frac{(n-1) s^2}{\\sigma^2} \\sim \\chi^2(k)\\]\nGiven \\(\\alpha\\), we can find the interval in which \\(\\alpha\\) of \\((n-1)s^2/ \\sigma^2\\) lie or:\n\\[\\chi_{\\alpha/2}^2 \\le \\frac{(n-1) s^2}{\\sigma^2} \\le \\chi_{(1-\\alpha/2)}^2\\]\nRearranging, we obtain the confidence interval estimate for \\(\\sigma^2\\) or:\n\\[\\frac{(n-1)s^2}{\\chi_{\\alpha/2}^2} \\le \\sigma^2 \\le \\frac{(n-1)s^2}{\\chi_{(1-\\alpha/2)}^2}\\]\nThe \\(\\chi^2\\) values are based on a \\(\\chi^2\\) distribution with \\(k\\) degrees of freedom.\n\n\\((1-\\alpha)\\) is the confidence coefficient for the interval estimate.",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Hypothesis Testing: Variance</span>"
    ]
  },
  {
    "objectID": "mod-8-3-variance.html#example",
    "href": "mod-8-3-variance.html#example",
    "title": "38  Hypothesis Testing: Variance",
    "section": "38.18 Example",
    "text": "38.18 Example\nAssume that we collect a sample of 100 individuals from a population.\nThe sample standard deviation of individual income is $200.\nWe want to construct a 90% confidence interval for \\(\\sigma^2\\) so \\(\\alpha = 0.1\\)\nThere are \\((n-1) = k = 99\\) degrees of freedom for the \\(\\chi^2\\) statistic.\n\\(\\chi^2_{0.05}(99) =\\) 123.2252215\n\\(\\chi^2_{0.95}(99) =\\) 77.0463319\nThe 90% confidence interval is:\n\\[\\frac{(n-1)s^2}{\\chi_{\\alpha/2}^2} \\le \\sigma^2 \\le \\frac{(n-1)s^2}{\\chi_{(1-\\alpha/2)}^2}\\]\n\\[\\frac{99*40000}{123.23} \\le \\sigma^2 \\le \\frac{99*4000}{77.046}\\]\n\\[51397.64 \\le \\sigma^2 \\le 32136.28\\]",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Hypothesis Testing: Variance</span>"
    ]
  },
  {
    "objectID": "mod-8-3-variance.html#r-example",
    "href": "mod-8-3-variance.html#r-example",
    "title": "38  Hypothesis Testing: Variance",
    "section": "38.19 R Example",
    "text": "38.19 R Example\nWe collect salaries (in millions) on 10 managers.\nThe average salary is 1.47 million.\nThe sample variance is 0.767 million\nLet \\(\\alpha = 0.05\\)\nUse qchisq to find the \\(\\chi^2\\) value for \\((n-1)\\) degrees of freedom at \\(\\alpha/2\\) and \\((1-\\alpha/2)\\)\n95% confidence interval for \\(\\sigma^2\\) is [0.363, 2.556]\n\nsalaries &lt;- c(2.2, .5, 2.4, 2.7, 2.0, 1.5, 1.3, 1.5, .3, .3)\n\navg_sal &lt;- mean(salaries)\nvar_sal &lt;- var(salaries)\nsd_sal &lt;- sd(salaries)\nn = length(salaries)\n\nalpha = 0.05\n\nlower &lt;- ((n-1)*sd_sal^2) / qchisq(1-(alpha/2), n-1)\nupper &lt;- ((n-1)*sd_sal^2) / qchisq((alpha/2), n-1)\n\nvars &lt;- data.frame(n, avg_sal, var_sal, alpha, lower, upper)\n\nkable(vars,\n      align     = \"cccccc\",\n      caption   = \"Variance Interval\",\n      col.names = c(\"n\", \"Xbar\", \n                    \"s^2\",\n                    \"alpha\", \"Lower Bound\", \n                    \"Upper Bound\")) %&gt;%\n  kable_styling(font_size = 11)\n\n\nVariance Interval\n\n\nn\nXbar\ns^2\nalpha\nLower Bound\nUpper Bound\n\n\n\n\n10\n1.47\n0.7667778\n0.05\n0.3627758\n2.555557",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Hypothesis Testing: Variance</span>"
    ]
  },
  {
    "objectID": "mod-8-3-variance.html#hypothesis-testing",
    "href": "mod-8-3-variance.html#hypothesis-testing",
    "title": "38  Hypothesis Testing: Variance",
    "section": "38.20 Hypothesis Testing",
    "text": "38.20 Hypothesis Testing\nWe obtain a random sample of 29 bus arrivals at a station and \\(s^2 = 4.2\\).\nManagement argues the variance in arrival times is less than 4 minutes or:\n\\[Ho: \\sigma^2 \\ge 4\\]\n\\[Ha: \\sigma^2 &lt; 4\\]\nThe test statistic for a hypothesis tests about a population variance is:\n\\[\\chi^2(k) = \\frac{(n-1) \\, s^2}{\\sigma_0^2}\\]\n\\[\\chi^2(28) = \\frac{(29 - 1) \\, 4.2}{4.0} = 29.4\\]\nWe can use pchisq to find \\(P(X \\ge 29.4) =\\) 0.3924554\nSince \\(P(X \\ge 29.4) = 0.392\\) we fail to reject the null hypothesis that \\(\\sigma \\le 4\\).\nThere is insufficient empirical evidence to support the argument that the variance of arrival times is less than 4 minutes.",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Hypothesis Testing: Variance</span>"
    ]
  },
  {
    "objectID": "mod-8-3-variance.html#r-example-1",
    "href": "mod-8-3-variance.html#r-example-1",
    "title": "38  Hypothesis Testing: Variance",
    "section": "38.21 R Example",
    "text": "38.21 R Example\nUsing data on manager salaries.\nThe average salary is 1.65 million.\nThe sample variance is 1.35 million\nFind the 90% confidence interval.\nTest the hypothesis that \\(\\sigma^2 = 1.45\\)\n90% confidence interval for \\(\\sigma^2\\) is [0.85,2.53]\nWe fail to reject the null hypothesis that \\(\\sigma^2 = 1.45\\)\n\nsalaries &lt;- c(2.2, .5, 2.4, 2.7, 2.0, 1.5, 1.3, 1.5, .3, .3,\n              3.5, 1.2, 4.5, 0.2, 1.9, 1.3, 0.9, 0.4, 1.2, 3.1)\n\navg_sal &lt;- mean(salaries)\nvar_sal &lt;- var(salaries)\nsd_sal &lt;- sd(salaries)\nn = length(salaries)\nsigma2_null = 1.45\n\nalpha = 0.1\n\nlower &lt;- ((n-1)*sd_sal^2) / qchisq(1-(alpha/2), n-1)\nupper &lt;- ((n-1)*sd_sal^2) / qchisq((alpha/2), n-1)\n\nchi_test &lt;- ((n-1)*sd_sal^2)/sigma2_null\nchi_p &lt;- pchisq(chi_test, n-1, lower.tail = FALSE)\n\nvars &lt;- data.frame(n, avg_sal, var_sal, alpha, lower, upper,\n                   chi_test, chi_p)\n\nkable(vars,\n      align     = \"cccccccc\",\n      caption   = \"Variance Interval and Test\",\n      col.names = c(\"n\", \"Xbar\", \n                    \"s^2\",\n                    \"alpha\", \"Lower Bound\", \n                    \"Upper Bound\", \"Chi Test\", \"P(X &gt; x)\")) %&gt;%\n  kable_styling(font_size = 11)\n\n\nVariance Interval and Test\n\n\nn\nXbar\ns^2\nalpha\nLower Bound\nUpper Bound\nChi Test\nP(X &gt; x)\n\n\n\n\n20\n1.645\n1.352079\n0.1\n0.8522393\n2.539238\n17.7169\n0.5414171",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Hypothesis Testing: Variance</span>"
    ]
  },
  {
    "objectID": "mod-8-3-variance.html#two-population-variances",
    "href": "mod-8-3-variance.html#two-population-variances",
    "title": "38  Hypothesis Testing: Variance",
    "section": "38.22 Two Population Variances",
    "text": "38.22 Two Population Variances\nWhenever independent simple random samples of sizes \\(n_1\\) and \\(n_2\\) are selected for two normal populations with equal variances, the sampling distribution is an F distribution with \\((n_1 - 1)\\) degrees of freedom in the numerator and \\((n_2 - 1)\\) degrees of freedom in the denominator.\nGiven \\(s_1^2\\) and \\(s_2^2\\) the sampling distribution is:\n\\[\\frac{s_1^2}{s_2^2} \\sim F(n_1 - 1, n_2 - 1)\\]\nLet \\(s_1^2\\) be the population with the larger sample variance, then the test statistic for the equality of population variances is:\n\\[F(n_1-1,n_2-1) = \\frac{s_1^2}{s_2^2}\\]",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Hypothesis Testing: Variance</span>"
    ]
  },
  {
    "objectID": "mod-8-3-variance.html#f-test-example",
    "href": "mod-8-3-variance.html#f-test-example",
    "title": "38  Hypothesis Testing: Variance",
    "section": "38.23 F Test Example",
    "text": "38.23 F Test Example\nAssume that \\(n_1 = 28\\) and \\(s_1^2 = 48\\)\nAssume that \\(n_2 = 16\\) and \\(s_2^2 = 20\\)\nTest whether \\(\\sigma_1^2 = \\sigma_2^2\\)\nLet \\(\\alpha = .10\\) so \\(\\alpha/2 = 0.05\\)\nThe calculated F-statistic is 2.4.\nThe critical value at 5% is 2.28.\nWe reject the null hypothesis that the variances are equal at the 5% level of statistical significance.\n\nn_1 &lt;- 26\ns_1 &lt;- 48\n\nn_2 &lt;- 16\ns_2 &lt;- 20\n\nalpha = .1\n\nf_crit &lt;- qf(alpha/2, n_1-1, n_2-1, lower.tail = FALSE)\n\nf_stat &lt;- s_1/s_2\n\nf_pvalue &lt;- pf(f_stat, n_1 - 1, n_2-1, lower.tail = FALSE)\n\nvars &lt;- data.frame(n_1, s_1, n_2, s_2, alpha, \n                   f_stat, f_crit, f_pvalue)\n\nkable(vars,\n      align     = \"cccccccc\",\n      caption   = \"F Test\",\n      col.names = c(\"n1\", \"s1\", \"n2\", \"s2\", \n                    \"alpha\", \"F Statistic\", \n                    \"F Critical\", \"P(F &gt; F)\")) %&gt;%\n  kable_styling(font_size = 11)\n\n\nF Test\n\n\nn1\ns1\nn2\ns2\nalpha\nF Statistic\nF Critical\nP(F &gt; F)\n\n\n\n\n26\n48\n16\n20\n0.1\n2.4\n2.279729\n0.0405991",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Hypothesis Testing: Variance</span>"
    ]
  },
  {
    "objectID": "mod-8-3-variance.html#wrapping-up",
    "href": "mod-8-3-variance.html#wrapping-up",
    "title": "38  Hypothesis Testing: Variance",
    "section": "38.24 Wrapping Up",
    "text": "38.24 Wrapping Up\nIn this module, we explored hypothesis testing. We noted the differences between Type I and Type II errors and examined how to form one-tail and two-tailed hypothesis tests. We estimated critical statistics when the population variance is known and unknown. We then explored examples of different types of hypothesis tests in R.\nWe then discussed how to conduct hypothesis tests for means, differences in mean, and variances. We introduced the Chi-Squared distribution and how it is used to make inferences regarding population variances. We also investigated how to use R to test hypotheses regarding two population variances and introduced the F-test.",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Hypothesis Testing: Variance</span>"
    ]
  }
]