---
title: "Correlation"
format:
  html:
    toc: true
    toc-depth: 3
    toc-location: right
    self-contained: false     # must be false when using webr
urlcolor: blue
filters:
  - webr
execute:
  webR: true
---

```{r setup, echo = FALSE, message = FALSE}

library(dplyr, quietly = TRUE)
library(ggplot2, quietly = TRUE)
library(kableExtra, quietly = TRUE)
library(tidyquant, quietly = TRUE)

```

## Introduction

To this point, we have mostly examined methods of describing one variable, that is, **univariate** analysis.

However, we may wish to do more than describe the properties of one variable.

We may want to explore a potential relationship (or absence of one) between two variables. Moving from univariate to **bivariate** analysis starts with a discussion of measures of association between two variables.

To start our discussion, we need to understand the **expectations operator**.

## The Expectations Operator

Given a function $f(x)$ in the variable $x$, the expectation value is denoted as $E[X]$.

For a single, discrete variable $X$ with probability density function $P(X)$, the expectation of $X$ is defined as:

$$
E[X] = \sum_{X} f(x) P(x)
$$

If $X$ is a continuous variable, then $E[X]$ is defined as:

$$
E[X] = \int f(x) P(X) dx
$$

The expected value of a random variable is the arithmetic mean of that variable. This term has been retained in mathematical statistics to mean the long-run average for any random variable over an indefinite number of trials or samples. In other words,

$$
E[X] = \mu_x
$$

The variance of a random variable $X$ is defined as the expected (average) squared deviation of the values of this random variable about their mean. That is,

$$var(x) = E[(X - \mu)^2] = E[X^2] - \mu^2 = \sigma_X^2$$

The expectations operator adheres to the following rules.

Let $X$ and $Y$ be random variables and let $k$ and $j$ be constants.

The first rule is that the expectation of a constant is a constant.

In other words, if $k=4$, then every observation of $k$ is 4 and the mean of $k$ is $E[k] = \mu_k = 4$.

$$
E[k] = k
$$

The second rule is that the expectation of the product of a constant and a random variable is equal to the product of a constant and the expectation of a random variable. In other words, assume that we multiply every observation of a random variable by 3 and then find the long-run mean of the random variable. This is the same as finding the long-run mean and multiplying the mean by 3.

$$
E[kX] = kE[X] = k \mu_x
$$

The third rule is that the expectation of the sum of two random variables is equal to the addition of the expectiations of the two random variables. In other words, if we add two random variables, one with the mean of 5 and the other with the mean of 10, this is the same as adding the two means together.

$$
E[X+Y] = E[X] + E[Y]
$$

The fourth rule combines the second and third rules in that if you have the sum of the two random variables where each random variable is multiplied by a constant, then you can restate this as the product of the constant and the expectation of the first random variable plus the product of the constant and the expectation of the second random variable.

$$
E[kX + jY] = kE[X] + jE[Y]
$$

There are a number of other expectation rules which we will cover later on in the course.

## Covariance

Given two random variables, $X$ and $Y$, the covariance between these two variables can be defined as:

$$
cov(X,Y) = E[(X - E[X])(Y - E[Y])]
$$

$$
cov(X,Y) = E[(X - \mu_x)(Y - \mu_Y)] = E[XY] - E[X]E[Y]
$$ The population covariance $\sigma_{xy}$ between two random variables is defined as:

$$
cov(X,Y) = \sigma_{XY} = \sum_{i=1}^{N} \frac{(x_i - \mu_x)(y_i - \mu_y)}{N}
$$

The sample covariance $s_{xy}$ between two random variables is defined as:

$$
cov(X,Y) = s_{XY} = \sum_{i=1}^{n} \frac{(x_i - \bar{x})(y_i - \bar{y})}{n-1}
$$

One point to note is that the covariance between a variable and itself is the variance of the variable.

We can easily see this for the population covariance by substituting $X$ for $Y$ or

$$
cov(X,X) = \sigma_{XX} = \sigma_X^2 = \sum_{i=1}^{N} \frac{(x_i - \mu_x)(x_i - \mu_x)}{N} = \sum_{i=1}^{N} \frac{(x_i - \mu_x)^2}{N}
$$

## Scatterplots

In the following code chunks, we create scatterplots of car weight and miles per gallon from the *mtcars* data set.

The scatterplot of the two variables suggests that as car weight increases, miles per gallon declines.

In the first scatterplot, we have weight on the x-axis and miles per gallon on the y-axis.

```{webr-r}

rm(list = ls())

data(mtcars)

ggplot(data = mtcars,
       aes(x = wt, 
           y = mpg)) +
geom_point() +
theme_minimal() +
labs(x = "Weight in Thousands of Pounds",
     y = "Miles Per Gallon",
     title = "Scatterplot of Weight and Miles Per Gallon")
```

What happens if we have miles per gallon on the x-axis and weight on the y-axis?

The same negative association appears to be present between the two variables, however, the important point is that that association is not causation, that is, we do not know if variation in one variable causes variation in another variable.

Another way to think about this is that we are investigating whether variables 'move' together in a systematic fashion.

```{webr-r}

rm(list = ls())

data(mtcars)

ggplot(data = mtcars,
       aes(x = mpg, 
           y = wt)) +
geom_point() +
theme_minimal() +
labs(x = "Miles Per Gallon",
     y = "Weight in Thousands of Pounds",
     title = "Scatterplot of Weight and Miles Per Gallon")
```

## Covariance Example

Let's explore the covariance between two variables, $X$, and $Y$ in the following code.

We first create a tibble with $X$ and $Y$.

We then estimate the covariance between $X$ and $Y$ manually:

-   We estimate the mean deviations of $X$

-   We estimate the mean deviations of $Y$

-   We estimate the product of the mean deviations of $X$ and $Y$

-   We estimate the product of $1/(n-1)$ and the product of the mean deviations of $X$ and $Y$

The result is an estimate of the sample covariance between $X$ and $Y$.

We then use the **cov** function to estimate the sample covariance between $X$ and $Y$.

This also illustrate that R uses the sample covariance function to estimate the covariance between two variables.

```{webr-r}

rm(list = ls())

df_1 <- tibble(x = c(8, 10 , 23 , 59, 60, 72, 100),
               y = c(-2, -10, -20, -43, -75, -90, -120))

cov_x_y <- df_1 %>%
           mutate(x_dev = x - mean(x),
                  y_dev = y - mean(y),
                  x_y   = x_dev * y_dev,
                  x_y_n = x_y/(nrow(df_1)-1)) %>%
           summarize(cov_x_y = sum(x_y_n))

df_2 <- tibble(cov_manual = cov_x_y$cov_x_y,
               cov_auto   = cov(df_1$x, df_1$y))

kable(df_2,
      align  = 'c',
      digits = 3,
      col.names = c('Manual Covariance', 'Estimated Covariance'))

```

## FRED Covariance

In the following code chunk, we want to estimate the covariance between the headline unemployment rate and the headline inflation rate.

Using the **tidyquant** package, we obtain the data from **FRED**.

However, the data from FRED are in 'long' format, that is, one column contains symbols that identify the series and another column contains the values. We want to translate the data into 'wide' format, where each row (date) contains several columns of data.

We use the **pivot_wider** command to translate the data from long to wide format and then estimate the monthly inflation rate as the year-over-year change in the Consumer Price Index. Since the unemployment rate is not in decimal form, we divide the unemployment series by 100.

We then create a scatterplot before estimating the mean of each series and the covariance between the two series.

```{r, message = FALSE, warnings = FALSE, result = 'hold'}

rm(list = ls())

library(dplyr)
library(ggplot2)
library(tidyverse)
library(tidyquant)
library(scales)

df_3 <- tq_get(c('UNRATE', 'CPIAUCSL'),
               get  = 'economic.data',
               from = '1969-01-01') %>%
  pivot_wider(names_from = symbol, values_from = price) %>%
  mutate(inflation = (CPIAUCSL - lag(CPIAUCSL, 12)) / lag(CPIAUCSL, 12),
         UNRATE    = UNRATE/100) %>%
  filter(!is.na(inflation))

ggplot(data = df_3, 
       aes(x = inflation, 
           y = UNRATE)) +
geom_point() +
scale_x_continuous(labels = scales::percent) +
scale_y_continuous(labels = scales::percent) +
theme_classic() +
labs(x = "Inflation (YoY %)",
     y = "Unemployment Rate (%)",
     title = "Scatterplot of Inflation and Unemployment")

df_4 <- df_3 %>%
        summarize(mean_inf = mean(inflation),
                  mean_unr = mean(UNRATE),
                  cov_i_u  = cov(inflation, UNRATE))

kable(df_4,
      col.names = c('Mean Inflation',
                    'Mean Unemploymnet',
                    'Covariance Inflation and Unemployment'),
      align     = 'c') %>%
kable_classic()


```

## MTCARS Covariance

In our last example of covariance, we return to the *mtcars* dataset and the variables for the weight of each car (in thousands of pounds) and the miles per gallon of each car.

```{webr-r}

rm(list = ls())

data(mtcars)

car_stats <- mtcars %>%
             summarize(mean_mpg = mean(mpg),
                       mean_wt  = mean(wt),
                       cov_m_w  = cov(mpg, wt))

kable(car_stats,
      align   = 'c',
      digits  = 3,
      col.names = c('Mean MPG', 'Mean Weight',
                    'Covariance between MPG and Weight'))

```

## Interpreting the Covariance

Given that the sample covariance in the previous example is equal to approximately -5.12, what does this mean about the potential relationship between the two variables?

If $\sigma_{XY} > 0$ or $s_{XY} > 0$, this implies that $X$ and $Y$ are positively linearly related, that is, as $X$ increases, $Y$ increases in a linear fashion.

Likewise, If $\sigma_{XY} < 0$ or $s_{XY} < 0$, this implies that $X$ and $Y$ are negatively linearly related, that is, as $X$ increases, $Y$ declines in a linear fashion.

If $\sigma_{XY} \approx 0$ or $s_{XY} \approx 0$, this implies that $X$ and $Y$ are not linearly related, that is, an increase in $X$ does not appear to influence $Y$.

If $X$ and $Y$ are independent, then $\sigma_{XY} = 0$ and $s_{XY} = 0$.

However, we typically do not rely on covariance to measure the association between two variables.

## Scaling and Covariance

The choice of units can alter the covariance measure. In other words, you can increase or decrease covariance between two variables by changing the units of measures of one or both variables.

We can easily see this in the following example. We add two new variables, one that converts miles per gallon into feet per gallon **mpg_ft** and one that converts weight in pounds into weight in ounces **wt_oz**.

The covariance is dependent upon how each variable is measured.

```{webr-r}

rm(list = ls())

data(mtcars)

cov_alt <- mtcars %>%
  select(mpg, wt) %>%
  mutate(mpg_ft = mpg*5280,
         wt_oz  = wt*16) %>%
  summarize(cov_mpg_wt   = cov(mpg,wt),
            cov_ampg_wt  = cov(mpg_ft, wt),
            cov_mpg_awt  = cov(mpg, wt_oz),
            cov_ampg_awt = cov(mpg_ft, wt_oz))

kable(cov_alt,
      align  = 'c',
      digits = 3,
      col.names = c('Cov(MPG,WT)', 'Cov(MPG_FT, WT)',
                    'Cov(MPG, WT_OZ', 'COV(MPG_FT, WT_OZ'))

```

## Correlation Coefficient

Given that the covariance measure is affected by the choice of units of measurement, we can use the **correlation coefficient** to examine the relationship between two random variables.

The **Pearson correlation coefficient** is defined as the ratio of the covariance between $X$ and $Y$ to the product of the standard deviations of $X$ and $Y$ and is denoted as $\rho_{XY}$ in the population and $r_{XY}$ in the sample.

In expectations, the Pearson correlation coefficient is equal to:

\begin{equation}
\rho_{XY} = \frac{E[(X - \mu_x)(Y - \mu_Y)]}{\sigma_X \sigma_Y}
\end{equation}

In the population, the Pearson correlation coefficient, $\rho_{XY}$, is equal to:

\begin{equation}
\rho_{XY} = \frac{\sigma_{XY}}{\sigma_X \sigma_Y}
\end{equation}

In the sample, the Pearson correlation coefficient, $r_{XY}$, is equal to:

\begin{equation}
r_{XY} = \frac{s_{XY}}{s_X s_Y}
\end{equation}

**The Pearson correlation coefficient is bounded between -1 and 1.**

A negative sign implies a negative association between the two variables of interest, while a positive sign implies a positive association. The closer the correlation coefficient is to -1 or 1, the stronger the association between the two variables. If the correlation coefficient is equal to -1 or 1, the two variables are **perfectly correlated**. If the correlation coefficient is equal to 0, the two variables are **uncorrelated**.

Returning to the previous example, we first manually calculate the sample covariance between car weight and miles per gallon and the sample standard deviations for these variables. We then calculate the sample correlation coefficient manually and compare it the results of the correlation function.

## Correlation Example

Let's explore the correlation between two variables, $X$, and $Y$ in the following code.

We return first to our previous example.

-   We estimate the standard deviations of $X$ and $Y$

-   We estimate the covariance between $X$ and $Y$

-   We manually estimate the correlation by dividing the covariance by the product of the standard deviations.

-   We use the **cor** function to estimate the correlation between the two variables

```{webr-r}

rm(list = ls())  

df_4 <- tibble(x = c(8, 10 , 23 , 59, 60, 72, 100),                
               y = c(-2, -10, -20, -43, -75, -90, -120))  

df_stats <- df_4 %>%
            summarize(sd_x    = sd(x),
                      sd_y    = sd(y), 
                      cov_x_y = cov(x,y),
                      cor_man = cov(x,y)/(sd(x)*sd(y)),
                      cor_x_y = cor(x,y))

kable(df_stats,
      align  = 'c',
      digits = 3,
      col.names = c('SD(X)', 'SD(Y)', 'COV(X,Y)',
                    'Manual COR(X,Y)', 'COR(X,Y)'))


```

## FRED Correlation

In the code chunk below, we again retrieve the headline unemployment rate and headline inflation series from FRED.

This time, however, we create two new variables.

-   Inflation_2 is equal to the inflation series times 1000

-   Unemployment_2 is equal to the unemployment series time 100

We then estimate the covariance between

-   Inflation and unemployment

-   Inflation_2 and unemployment

-   Inflation and unemployment_2

The results demonstrate that the covariance measure is dependent on the scale or units of measurement and that we can inflate (or deflate) the covariance measure by scaling one or both variables.

We then estimate four correlation coefficients:

-   Inflation and unemployment

-   Inflation_2 and unemployment

-   Inflation and unemployment_2

-   Inflation_2 and unemployment_2

The correlation coefficients are equal to each other. In other words, the correlation coefficient is **invariant** to the scale of the variables.

```{r, message = FALSE, warnings = FALSE, result = 'hold'}

rm(list = ls())

library(dplyr)
library(ggplot2)
library(tidyverse)
library(tidyquant)
library(scales)

df_5 <- tq_get(c('UNRATE', 'CPIAUCSL'),
               get  = 'economic.data',
               from = '1969-01-01') %>%
  pivot_wider(names_from = symbol, values_from = price) %>%
  mutate(inflation = (CPIAUCSL - lag(CPIAUCSL, 12)) / lag(CPIAUCSL, 12),
         unrate    = UNRATE/100) %>%
  filter(!is.na(inflation)) %>%
  mutate(unrate_2 = unrate *100,
         inflation_2 = inflation * 1000) %>%
  summarize(cov_i_u     = cov(inflation, unrate),
            cov_i2_u    = cov(inflation_2, unrate),
            cov_i_u2    = cov(inflation, unrate_2),
            cor_i_u     = cor(inflation, unrate),
            cor_i2_u    = cor(inflation_2, unrate),
            cor_i_u2    = cor(inflation, unrate_2),
            cor_i2_u2   = cor(inflation_2, unrate_2))

kable(df_5,
      align = 'c',
      digits = 3,
      col.names = c('COV(Inf, Unemp)', 'COV(Inf2, Unemp)',
                    'COV(Inf, Unemp2)', 'COR(Inf,Unemp)',
                    'COR(Inf2, Unemp)', 'COR(Inf, Unemp2)',
                    'COR(Inf2, Unemp2)'))


```

## MTCARS Correlation

We return to the *mtcars* dataset and the variables for the weight of each car (in thousands of pounds) and the miles per gallon of each car.

```{webr-r}

rm(list = ls())

data(mtcars)

car_stats <- mtcars %>%
             summarize(mean_mpg = mean(mpg),
                       mean_wt  = mean(wt),
                       cov_m_w  = cov(mpg, wt),
                       cor_m_w  = cor(mpg, wt))

kable(car_stats,
      align   = 'c',
      digits  = 3,
      col.names = c('Mean MPG', 'Mean Weight',
                    'Covariance between MPG and Weight',
                    'Correlation between MPG and Weight'))

```

## Correlation and Causation

To this point, we have examined **measures of association** rather than **measures of causation**.

We must be careful to understand that while association may imply causation, association, by itself, is not sufficient to conclude a causal relationship exists.

**Association is necessary, but not sufficient, for causation**

Correlation between two variables may be the product of one or more confounding variables. Observational studies may fall victim to confounding variables, leading one to conclude causation exists. For example, assume that 100 patients were sick with a disease with 5% mortality after two weeks. All the patients were given an experimental drug and at the end of two weeks, only 1 patient has died. Obviously the drug has reduced mortality, or has it? What if 10,000 patients were treated and only 1% died?

While the drug is associated with decreased mortality, we have not controlled for other potential confounding variables. The general health of the patients may have been different than the general population. The patients may have received other drugs that are known to reduce mortality. The age of the patients may be lower (higher) than the general population. This is why scientists use double-blind, randomized, placebo-controlled trials to examine the efficacy of new drugs. These trials reduce (but do not eliminate) the possibility of confounding so investigators can examine whether the outcomes of the control (receive placebo) and treatment (receive drug) groups are statistically different.

A strong association between two variables is **suggestive** of a causal relationship but we must be cautious. How were the data gathered? Was there a mechanism to reduce the presence of confounding variables? Are the data experimental, that is, random assignment into control and treatment groups? As we move from observational studies to experimental studies, the ability to suggest that association implies causation increases, but, in the end, we must find other methods to explore whether two (or more) variables are causally related.

## Module Summary

We use descriptive statistics to develop an understanding of the properties of the data.

-   We have discussed two broad properties of the data:

    -   The center of the data

    -   The spread of the data

-   We developed two measures of association (co-movement)

    -   Covariance

    -   Correlation

-   In the coming modules, we will discuss randomness, the concept of probability, and probability distributions.
