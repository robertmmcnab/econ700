---
title: "Discrete Random Variables"
format:
  html:
    toc: true
    toc-depth: 3
    toc-location: right
    self-contained: false     # must be false when using webr
urlcolor: blue
filters:
  - webr
execute:
  webR: true
---

```{r setup, echo = FALSE, message = FALSE}

library(dplyr, quietly = TRUE)
library(ggplot2, quietly = TRUE)
library(kableExtra, quietly = TRUE)
library(tidyquant, quietly = TRUE)

```

## Discrete Random Variables

A **random variable** is a numerical description of an outcome of an experiment.

A **discrete random variable** may assume a **finite** or **infinite** number of values.

A fair coin flip or roll of a six-sided die is a random experiment with a finite number of outcomes.

For the coin flip, the random variable is the face of the coin, the values of which are: heads (1) and tails (0).

For the six-sided die, the random variable is the number of dots showing on the die, the values of which are: 1,2,3,4,5,6.

If the random experiment is operating a website for a 24-hour period, the random variable is the number of unique visitors to the website.

The values of the random variable are: 0, 1, 2, 3 ......

## Continuous Random Variables

A **continuous random variable** may assume any value in an interval or collection of intervals.

If we select 1,000 students at random who come to ODU today, the driving distance is bounded below at 0.

The driving distance is practically bounded above (maybe 200 miles?).

Driving distance is a continuous random variable as it can take on any value from 0 to infinity (and beyond!).

If the random experiment is a visit to a webpage by a customer, the random variable is the time spent on the page, with values of the random variable being $x \ge 0$.

If the random experiment is to fill a kiloliter container, the random variable is the amount of liquid in the container, with values of the random variable being $0 \le x \le 1\, kl$.

## Discrete Probability Distributions

The **probability distribution** for a random variable describes how probabilities are distributed over the values of the random variable.

Given a discrete random variable, $x$, a **probability function**, $f(x)$, provides the probability for each value of the random variable.

The **classical** method of assigning probabilities of the values of a random variable is used when the experimental outcomes generate values of the random variable that are equally likely.

Consider the experiment of rolling a six-sided die.

-   We know the sample space is $s = \{1, 2, 3, 4, 5, 6\}$

-   Let $x$ be the number obtained on one throw of the die

-   Let $f(x)$ be the probability of $x$

-   We can represent the probability of $x$ with a frequency table or graph.

In the code below, we run a simulated six-sided die 10,000 times.

-   The **sample** function takes a sample of the sequence of integers from 1 to 6

-   The **size** option sets the number of samples to be taken

-   The **replace = TRUE** replaces the number sampled

-   The **group_by** identifies the bins

-   The **summarize** command estimates the frequencies and relative frequencies

```{webr-r}

rm(list = ls())

throws = 10000

rolls <- tibble(throw = sample(1:6, 
                               size = throws, 
                               replace = TRUE)) %>%
  group_by(throw) %>%
  summarize(freq     = n(),
            rel_freq = n()/throws)

ggplot(data = rolls,
        aes(x = throw,
            y = rel_freq,
            fill = throw)) +
geom_bar(stat = 'identity') +
geom_text(data  = rolls,
          aes(x = throw,
              y = rel_freq,
              label = scales::label_number(a=0.01)(rel_freq)),
          nudge_y   = 0.01) +
theme_minimal() +
theme(legend.position = " ") +
labs(title    = "Throws of a Six Sided Die",
     x        = " ",
     y        = "Relative Frequency")
```

## Frequency Tables

The relative frequency method is applicable when the data are "reasonably" large.

The discrete probability distribution that is developed using the relative frequency method is known as the **empirical discrete distribution**.

Two conditions are required for a discrete probability distribution.

$$f(x) \ge 0$$

$$\sum f(x) = 1$$ In the code below, we replicate the experiment of throwing a six-sided die 10,000 times. We estimate the relative frequency for each of the outcomes of the experiment and create a relative frequency table that mirrors the graph.

```{webr-r}

rm(list = ls())

throws = 10000

rolls <- tibble(throw = sample(1:6, 
                               size = throws, 
                               replace = TRUE)) %>%
  group_by(throw) %>%
  summarize(freq     = n(),
            rel_freq = n()/throws)

kable(rolls,
      align       = "lcc", 
      format.args = list(big.mark = ",", 
                         scientific = FALSE),
      caption     = "Probability Distribution for Number\n
                     Obtained on One Roll of Six Sided Die",
      col.names   = c("Number (x)", "Frequency of X", 
                      "Probability of X - f(x)"))
```

## Discrete Probability Distribution

A formula that gives the probability function, $f(x)$ for every value of $x$ is known as a **discrete probability distribution**.

Using data, we developed the empirical discrete probability function for a fair six-sided die.

Let $n$ be equal to the number of values of that the random variable can take.

The **discrete uniform probability distribution** is then defined as:

$$f(x) = \frac{1}{n}$$

Note that we can apply the discrete uniform probability distribution function to several of our previous examples.

Flipping a fair coin: $$f(x) = \frac{1}{2}$$

Rolling a fair six-sided die: $$f(x) = \frac{1}{6}$$

## Expected Value

The **expected value** of a random variable is a measure of the central tendency of the random variable.

The expected value of $x$ is defined as

$$E(x) = \mu = \sum x \, f(x)$$

The expected value of a random variable is essentially a **weighted average**.

Each value of the random variable, $x$, is multiplied by the probability that the random variable takes that value or $p(x_i)$. The $x_i \times p(x_i)$ are then summed over all possible values.

The expected value of a random variable can also be interpreted as the **long-run value** of the random variable. If we repeat the random experiment 'many' times and take the average of all the outcomes, we would obtain the expected value of the random variable.

The expected value, as it is the mean of the random variable, is also the **measure of central tendency** of the random variable and, as such, represents the **center of mass** of the probability mass function.

## Expected Value Example

Assume that you have a game where you can win various prizes with the following probabilities:

-   \$0 with $P(X = 0) = 0.1$

-   \$1 with $P(X = 1) = 0.15$

-   \$2 with $P(X = 2) = 0.4$

-   \$3 with $P(X = 3) = 0.25$

-   \$4 with $P(X = 4) = 0.1$

What is the expected value of the game? In other words, if the game were played 'many' times, what would be the average expected gross winnings?

$$
E(X) = \sum_{i=1}^{5} x_i f(x_i)
$$

$$
E(X) = (0 \times 0.1) + (1 \times 0.15) + (2 \times 0.4) + (3 \times 0.25) + (4 \times 0.1) 
$$

$$
E(X) = 2.1
$$

The expected value of the game is \$2.1. On average, over repeated trials, one would expect to win \$2.1 from this game. If the game cost less than \$2.1 per play, it would make sense to play the game.

## Expected Value Example

Let's return to our experiment where we simulate rolling a fair six-sided die.

We increase the number of simulated rolls in this experiment to 1,000,000.

We estimate the expected value as:

$$
EV(X) = \sum_{i=1}^6 x_i f(x_i)
$$

We produce the table of frequencies and relative frequencies as well as a histogram.

The expected value of our experiment mirrors that obtained manually.

-   1 with $P(X = 1) = 0.167$

-   2 with $P(X = 2) = 0.167$

-   3 with $P(X = 3) = 0.167$

-   4 with $P(X = 4) = 0.167$

-   5 with $P(X = 5) = 0.167$

-   6 with $P(X = 6) = 0.167$

$$
E(X) = (1 \times 0.167) + (2 \times 0.167) + (3 \times 0.167) \\+ (4 \times 0.167) + (5 \times 0.167) + (6 \times 0.167) 
$$

$$
EV = 3.5
$$

```{r, message = FALSE, warnings = FALSE, results = 'hold'}

rm(list = ls())

set.seed(1234)

library(dplyr)
library(ggplot2)
library(janitor)
library(kableExtra)
library(tidyverse)

throws = 1000000

rolls <- tibble(throw = sample(1:6, 
                               size = throws, 
                               replace = TRUE)) %>%
  group_by(throw) %>%
  summarize(freq     = n(),
            rel_freq = n()/throws) %>%
  mutate(xfx = throw * rel_freq) 


kable(rolls %>%  
      adorn_totals(where = "row", name = "Sums"),
      align       = "lcccc", 
      digits      = 3,
      format.args = list(big.mark = ",", 
                         scientific = FALSE),
      caption     = "Rolls of Six-Sided Die",
      col.names   = c("Number (x)", "Frequency of X", 
                      "Probability of X - f(x)", "x * f(x)")) %>%
kable_classic(full_width = TRUE)

ggplot(data = rolls,
        aes(x = throw,
            y = rel_freq,
            fill = throw)) +
geom_bar(stat = 'identity') +
geom_text(data  = rolls,
          aes(x = throw,
              y = rel_freq,
              label = scales::label_number(a=0.01)(rel_freq)),
          nudge_y   = 0.01) +
theme_minimal() +
theme(legend.position = " ") +
labs(title    = "Throws of a Six Sided Die",
     x        = " ",
     y        = "Relative Frequency")
```

## Variance

The expected value, $\mu$, provides a measure of central tendency of a random variable.

**The variance is a measure of the dispersion of the values of the random variable.**

The variance of a discrete random variable is defined as:

$$var(x) = \sigma^2 = \sum (x - \mu)^2 \, f(x)$$

One can think about the variance as the weighted average of the squared mean deviations, where the weights are the probabilities of the values of $x$.

The standard deviation of a discrete random variable is defined as:

$$sd(x) = \sqrt{\sigma^2} = \sqrt{\sum (x - \mu)^2 \, f(x)}$$

The standard deviation is preferred in describing the dispersion of a random variable as it is in the same units of the random variable as opposed to the variance which is measured in squared units and is more difficult to interpret.

```{r, message = FALSE, warnings = FALSE, results = 'hold'}

rm(list = ls())

set.seed(1234)

library(dplyr)
library(ggplot2)
library(janitor)
library(kableExtra)
library(tidyverse)

throws = 1000000

rolls <- tibble(throw = sample(1:6, 
                               size = throws, 
                               replace = TRUE)) %>%
  group_by(throw) %>%
  summarize(freq     = n(),
            rel_freq = n()/throws) %>%
  mutate(xfx = throw * rel_freq,
         deviations    = as.numeric(throw) - sum(xfx),
         sq_deviations = deviations^2,
         sqdev_fx      = sq_deviations * rel_freq) %>%
  adorn_totals(where = "row", name = "Sums")

kable(rolls,
      align       = "lcccccc", 
      format.args = list(big.mark = ",", 
                         scientific = FALSE),
      caption     = "Rolls of Six-Sided Die",
      col.names   = c("Number (x)", "Frequency of X", 
                      "Probability of X - f(x)", "x f(x)",
                      "Mean Deviations", "Squared Deviations",
                      "Squared Deviations * f(x)")) %>%
  kable_styling(font_size = 10)

vars <- rolls %>% 
       filter(throw != "Sums") %>% 
       summarize(var_x = sum(sqdev_fx),
                 sd_x  = sqrt(sum(sqdev_fx)))

kable(vars,
      col.names = c("Variance (X)", "Standard Deviation (X)"),
      align     = 'c',
      digits    = 3) %>%
kable_styling()

```
